<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.40">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2025-05-27">

<title>Building Real-time RecSys Chapter 4 - Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation – DvQ</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../../avatar.png" rel="icon" type="image/png">
<script src="../../../site_libs/cookie-consent/cookie-consent.js"></script>
<link href="../../../site_libs/cookie-consent/cookie-consent.css" rel="stylesheet">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting-86daaaaad7353f9cc0c554efc1dd6d94.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap-c1d7b6365050eb3642bec814578358de.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-NH6GYY9FZV"></script>

<script type="text/plain" cookie-consent="tracking">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-NH6GYY9FZV', { 'anonymize_ip': true});
</script>

<script type="text/javascript" charset="UTF-8">
document.addEventListener('DOMContentLoaded', function () {
cookieconsent.run({
  "notice_banner_type":"simple",
  "consent_type":"implied",
  "palette":"light",
  "language":"en",
  "page_load_consent_levels":["strictly-necessary","functionality","tracking","targeting"],
  "notice_banner_reject_button_hide":false,
  "preferences_center_close_button_hide":false,
  "website_name":""
  ,
"language":"en"
  });
});
</script> 
  
<style>html{ scroll-behavior: smooth; }</style>
<!-- Inter -->
<link rel="preconnect" href="https://rsms.me/">
<link rel="stylesheet" href="https://rsms.me/inter/inter.css">
<!-- Roboto Mono -->
<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin="">
<link href="https://fonts.googleapis.com/css2?family=Roboto+Mono:ital,wght@0,100..700;1,100..700&amp;display=swap" rel="stylesheet">


<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top quarto-banner">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">DvQ</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" role="menu" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html"> 
<span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../projects.html"> 
<span class="menu-text">Projects</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.linkedin.com/in/dvquy/"> <i class="bi bi-linkedin" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/dvquy13"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://x.com/dvquys"> <i class="bi bi-twitter-x" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="../../../index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
            <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Building Real-time RecSys Chapter 4 - Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation</h1>
            <p class="subtitle lead">Establishing the evaluation foundation and implementing your first recommendation model</p>
                                <div class="quarto-categories">
                <div class="quarto-category">recsys</div>
                <div class="quarto-category">recsys-real-time-series</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">May 27, 2025</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#the-challenges-of-evaluation-in-recommendation-systems" id="toc-the-challenges-of-evaluation-in-recommendation-systems" class="nav-link" data-scroll-target="#the-challenges-of-evaluation-in-recommendation-systems">The Challenges of Evaluation in Recommendation Systems</a></li>
  <li><a href="#online-vs-offline-evaluation-the-ultimate-goal-vs-the-development-tool" id="toc-online-vs-offline-evaluation-the-ultimate-goal-vs-the-development-tool" class="nav-link" data-scroll-target="#online-vs-offline-evaluation-the-ultimate-goal-vs-the-development-tool">Online vs Offline Evaluation: The Ultimate Goal vs The Development Tool</a></li>
  <li><a href="#evaluation-metrics-for-recommendation-systems" id="toc-evaluation-metrics-for-recommendation-systems" class="nav-link" data-scroll-target="#evaluation-metrics-for-recommendation-systems">Evaluation Metrics for Recommendation Systems</a>
  <ul class="collapse">
  <li><a href="#ranking-metrics" id="toc-ranking-metrics" class="nav-link" data-scroll-target="#ranking-metrics">Ranking Metrics</a></li>
  <li><a href="#diversity-metrics" id="toc-diversity-metrics" class="nav-link" data-scroll-target="#diversity-metrics">Diversity Metrics</a></li>
  <li><a href="#classification-metrics" id="toc-classification-metrics" class="nav-link" data-scroll-target="#classification-metrics">Classification Metrics</a></li>
  </ul></li>
  <li><a href="#how-we-set-up-evaluation-for-our-project" id="toc-how-we-set-up-evaluation-for-our-project" class="nav-link" data-scroll-target="#how-we-set-up-evaluation-for-our-project">How We Set Up Evaluation for our Project</a>
  <ul class="collapse">
  <li><a href="#core-evaluation-utilities" id="toc-core-evaluation-utilities" class="nav-link" data-scroll-target="#core-evaluation-utilities">Core Evaluation Utilities</a></li>
  <li><a href="#metric-logging-with-evidently" id="toc-metric-logging-with-evidently" class="nav-link" data-scroll-target="#metric-logging-with-evidently">Metric Logging with Evidently</a></li>
  </ul></li>
  <li><a href="#mlflow-integration-configuration-driven-experiment-tracking" id="toc-mlflow-integration-configuration-driven-experiment-tracking" class="nav-link" data-scroll-target="#mlflow-integration-configuration-driven-experiment-tracking">MLflow Integration: Configuration-Driven Experiment Tracking</a>
  <ul class="collapse">
  <li><a href="#configuration-based-setup" id="toc-configuration-based-setup" class="nav-link" data-scroll-target="#configuration-based-setup">Configuration-Based Setup</a></li>
  <li><a href="#automatic-parameter-logging" id="toc-automatic-parameter-logging" class="nav-link" data-scroll-target="#automatic-parameter-logging">Automatic Parameter Logging</a></li>
  </ul></li>
  <li><a href="#implementing-the-popularity-baseline" id="toc-implementing-the-popularity-baseline" class="nav-link" data-scroll-target="#implementing-the-popularity-baseline">Implementing the Popularity Baseline</a>
  <ul class="collapse">
  <li><a href="#popularity-score-calculation" id="toc-popularity-score-calculation" class="nav-link" data-scroll-target="#popularity-score-calculation">Popularity Score Calculation</a></li>
  <li><a href="#generating-recommendations-for-all-users" id="toc-generating-recommendations-for-all-users" class="nav-link" data-scroll-target="#generating-recommendations-for-all-users">Generating Recommendations for All Users</a></li>
  </ul></li>
  <li><a href="#complete-evaluation-workflow" id="toc-complete-evaluation-workflow" class="nav-link" data-scroll-target="#complete-evaluation-workflow">Complete Evaluation Workflow</a>
  <ul class="collapse">
  <li><a href="#setup-and-configuration" id="toc-setup-and-configuration" class="nav-link" data-scroll-target="#setup-and-configuration">1. Setup and Configuration</a></li>
  <li><a href="#data-preparation" id="toc-data-preparation" class="nav-link" data-scroll-target="#data-preparation">2. Data Preparation</a></li>
  <li><a href="#model-implementation-and-evaluation" id="toc-model-implementation-and-evaluation" class="nav-link" data-scroll-target="#model-implementation-and-evaluation">3. Model Implementation and Evaluation</a></li>
  <li><a href="#classification-metrics-1" id="toc-classification-metrics-1" class="nav-link" data-scroll-target="#classification-metrics-1">4. Classification Metrics</a></li>
  <li><a href="#experiment-cleanup" id="toc-experiment-cleanup" class="nav-link" data-scroll-target="#experiment-cleanup">5. Experiment Cleanup</a></li>
  </ul></li>
  <li><a href="#understanding-the-results" id="toc-understanding-the-results" class="nav-link" data-scroll-target="#understanding-the-results">Understanding the Results</a>
  <ul class="collapse">
  <li><a href="#mlflow-experiment-tracking" id="toc-mlflow-experiment-tracking" class="nav-link" data-scroll-target="#mlflow-experiment-tracking">MLflow Experiment Tracking</a></li>
  <li><a href="#evidently-reports" id="toc-evidently-reports" class="nav-link" data-scroll-target="#evidently-reports">Evidently Reports</a></li>
  <li><a href="#baseline-performance-expectations" id="toc-baseline-performance-expectations" class="nav-link" data-scroll-target="#baseline-performance-expectations">Baseline Performance Expectations</a></li>
  </ul></li>
  <li><a href="#why-this-foundation-matters" id="toc-why-this-foundation-matters" class="nav-link" data-scroll-target="#why-this-foundation-matters">Why This Foundation Matters</a>
  <ul class="collapse">
  <li><a href="#reproducible-experiments" id="toc-reproducible-experiments" class="nav-link" data-scroll-target="#reproducible-experiments">1. <strong>Reproducible Experiments</strong></a></li>
  <li><a href="#systematic-model-development" id="toc-systematic-model-development" class="nav-link" data-scroll-target="#systematic-model-development">2. <strong>Systematic Model Development</strong></a></li>
  <li><a href="#production-readiness" id="toc-production-readiness" class="nav-link" data-scroll-target="#production-readiness">3. <strong>Production Readiness</strong></a></li>
  <li><a href="#debugging-and-validation" id="toc-debugging-and-validation" class="nav-link" data-scroll-target="#debugging-and-validation">4. <strong>Debugging and Validation</strong></a></li>
  </ul></li>
  <li><a href="#configuration-management-best-practices" id="toc-configuration-management-best-practices" class="nav-link" data-scroll-target="#configuration-management-best-practices">Configuration Management Best Practices</a>
  <ul class="collapse">
  <li><a href="#centralized-configuration" id="toc-centralized-configuration" class="nav-link" data-scroll-target="#centralized-configuration">Centralized Configuration</a></li>
  <li><a href="#easy-experimentation" id="toc-easy-experimentation" class="nav-link" data-scroll-target="#easy-experimentation">Easy Experimentation</a></li>
  <li><a href="#automatic-documentation" id="toc-automatic-documentation" class="nav-link" data-scroll-target="#automatic-documentation">Automatic Documentation</a></li>
  </ul></li>
  <li><a href="#whats-next" id="toc-whats-next" class="nav-link" data-scroll-target="#whats-next">What’s Next</a></li>
  <li><a href="#recap" id="toc-recap" class="nav-link" data-scroll-target="#recap">Recap</a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">





<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>In <a href="../../../projects/real-time-seq-recsys/c3/index.html">Chapter 3</a>, inspired by how Word2Vec cleverly samples out-of-context words to train its language model, we implemented our own popularity-based sampling strategy to generate negative samples for our training dataset. Now we’re ready to tackle one of the most critical aspects of any ML project: <strong>evaluation</strong>.</p>
<p>We will discuss building a comprehensive evaluation framework, setting up MLflow for experiment tracking, and implementing a popularity-based baseline model to illustrate the end-to-end process. A solid measuring foundation provides the groundwork for systematic model development—you’ll use these tools throughout your different attempts at improving the model performance.</p>
<div class="callout callout-style-simple callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Code
</div>
</div>
<div class="callout-body-container callout-body">
<p>All code for this chapter is available in the <code>notebooks/010-baseline-popular.ipynb</code> file and the <code>src/eval/</code> directory in the <a href="https://github.com/dvquy13/recsys-seq-model">project repository</a>.</p>
</div>
</div>
</section>
<section id="the-challenges-of-evaluation-in-recommendation-systems" class="level2">
<h2 class="anchored" data-anchor-id="the-challenges-of-evaluation-in-recommendation-systems">The Challenges of Evaluation in Recommendation Systems</h2>
<p>Recommendation systems are notoriously tricky to evaluate. Unlike classification problems where accuracy is straightforward, recommendations involve ranking, relevance, and user satisfaction—all of which are nuanced concepts.</p>
<p>RecSys present unique evaluation challenges that go far beyond traditional machine learning problems. The most fundamental issue is the <strong>counterfactual problem</strong>: we only observe user interactions with items they actually encountered, but we have no ground truth for the vast majority of items they never saw. If a user interacted with 10 items in their history, we can reasonably infer labels for those 10, but what about the remaining thousands or millions of items in our catalog? Did the user avoid them because they’re irrelevant, or simply because they never discovered them? This missing data problem makes offline evaluation inherently problematic—we’re essentially trying to measure recommendation quality using an incomplete and biased sample of user preferences.</p>
<p>Beyond this counterfactual challenge, recommendation systems must also struggle with implicit feedback where users rarely provide explicit ratings, ranking quality where the order of recommendations matters as much as the items themselves, personalization where a perfect recommendation for one user might be terrible for another, and temporal dynamics where user preferences evolve over time. These complexities mean that unlike classification problems where accuracy is straightforward, recommendations involve nuanced concepts of relevance, satisfaction, and utility that resist simple measurement.</p>
<p>To address these challenges, we need a comprehensive evaluation framework that can measure multiple aspects of recommendation quality. For the scope of this tutorial, we would be focusing on measuring both the ranking quality and the classification quality of the recommendations, while leaving the problem of counterfactual evaluation for a future post perhaps<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>.</p>
</section>
<section id="online-vs-offline-evaluation-the-ultimate-goal-vs-the-development-tool" class="level2">
<h2 class="anchored" data-anchor-id="online-vs-offline-evaluation-the-ultimate-goal-vs-the-development-tool">Online vs Offline Evaluation: The Ultimate Goal vs The Development Tool</h2>
<p>Before diving into the technical details, let me remind you of the what ultimately matters to the business: delivering relevant recommendations measured via business metrics like click-through rates and conversion rates.</p>
<p>These metrics can only be evaluated at production environment with the help of online experiments. But they are typically expensive and slow. A/B testing and its alike require real users, real traffic, and real time to see if your changes work. So to iterate quickly on your backlog of a hundred different ideas, we often resort to offline evaluation as our development tool. The assumption is that improvements made over historical observational data can guide our decisions.</p>
<p>Though more often than not, offline improvements don’t always translate to online wins. You might build a model that crushes your offline metrics, deploy it to production, and watch your click-through rates stay flat or even drop. The gap between offline and online performance is one of the most challenging aspects of recommendation systems.</p>
<p>Of course there have already been research efforts to address this, and googling around would actually lead us to some interesting ideas<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a>. IMHO though, the approaches are often too complicated so I normally refrain from fancy techniques in the early phases of the project. In principle we should nonetheless design our offline evaluation to match online results as closely as possible, while maintaining a mindset that offline evaluation is a guiding tool and a guardrail, not the definitive answer.</p>
<p>I usually think like this: if your offline metrics show clear improvements, that’s a good sign to move forward. If they show no change but you have strong reasons to believe your improvements are effective, don’t let that stop you from running an online experiment. Sometimes the best ideas don’t show up in offline metrics until they meet real users. Just make sure you can roll back quickly if you see significant drops in the early days.</p>
<p>For this tutorial series, we’ll focus on offline evaluation because it lets us iterate fast and learn the fundamentals. But important message worths repeating—the real test happens when your recommendations meet actual users.</p>
</section>
<section id="evaluation-metrics-for-recommendation-systems" class="level2">
<h2 class="anchored" data-anchor-id="evaluation-metrics-for-recommendation-systems">Evaluation Metrics for Recommendation Systems</h2>
<p>There are plenty of great posts diving into the details of these metrics so I would just briefly mention them here. Our evaluation framework implements several key metrics:</p>
<section id="ranking-metrics" class="level3">
<h3 class="anchored" data-anchor-id="ranking-metrics">Ranking Metrics</h3>
<ul>
<li><strong>Precision@K</strong>: What fraction of top-K recommendations are relevant?</li>
<li><strong>Recall@K</strong>: What fraction of relevant items appear in top-K recommendations?</li>
<li><strong>NDCG@K</strong>: Normalized Discounted Cumulative Gain—rewards relevant items appearing higher in the ranking</li>
</ul>
</section>
<section id="diversity-metrics" class="level3">
<h3 class="anchored" data-anchor-id="diversity-metrics">Diversity Metrics</h3>
<ul>
<li><strong>Personalization</strong>: Measures how different recommendations are across users (higher is better)</li>
</ul>
</section>
<section id="classification-metrics" class="level3">
<h3 class="anchored" data-anchor-id="classification-metrics">Classification Metrics</h3>
<ul>
<li><strong>ROC-AUC</strong>: Area under the ROC curve for binary relevance prediction</li>
<li><strong>Precision-Recall curves</strong>: Trade-offs between precision and recall at different thresholds</li>
</ul>
<p>If you wish to learn more, I suggest checking out <a href="https://amitness.com/posts/information-retrieval-evaluation">this post by Amit Chaudhary</a> and the <a href="https://docs.evidentlyai.com/metrics/all_metrics#ranking">Evidently documentation on Ranking metrics</a>.</p>
</section>
</section>
<section id="how-we-set-up-evaluation-for-our-project" class="level2">
<h2 class="anchored" data-anchor-id="how-we-set-up-evaluation-for-our-project">How We Set Up Evaluation for our Project</h2>
<p>Our evaluation framework is built around three core modules in <code>src/eval/</code>:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.<span class="bu">eval</span> <span class="im">import</span> (</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>    create_label_df,           <span class="co"># Create ground truth labels</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    merge_recs_with_target,    <span class="co"># Merge recommendations with labels  </span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    log_ranking_metrics,       <span class="co"># Log ranking-based metrics</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    log_classification_metrics <span class="co"># Log classification-based metrics</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="core-evaluation-utilities" class="level3">
<h3 class="anchored" data-anchor-id="core-evaluation-utilities">Core Evaluation Utilities</h3>
<p>The <code>src/eval/utils.py</code> module provides essential functions for evaluation setup:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> create_label_df(df, user_col, item_col, rating_col, timestamp_col):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Create ground truth labels from validation data.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Ranks items by rating and timestamp for each user.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>    label_cols <span class="op">=</span> [user_col, item_col, rating_col, <span class="st">"rating_rank"</span>]</span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>    label_df <span class="op">=</span> (</span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a>        df.sort_values([timestamp_col], ascending<span class="op">=</span>[<span class="va">False</span>])</span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a>        .assign(</span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a>            rating_rank<span class="op">=</span><span class="kw">lambda</span> df: df.groupby(user_col)[rating_col].rank(</span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>                method<span class="op">=</span><span class="st">"first"</span>, ascending<span class="op">=</span><span class="va">False</span></span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>        .sort_values([<span class="st">"rating_rank"</span>], ascending<span class="op">=</span>[<span class="va">True</span>])[label_cols]</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> label_df</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function creates a ranking of items for each user based on their ratings and interaction timestamps, providing the ground truth for evaluation.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> merge_recs_with_target(recs_df, label_df, k<span class="op">=</span><span class="dv">10</span>, user_col, item_col, rating_col):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Merge recommendation rankings with ground truth labels.</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Essential for computing ranking metrics.</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>        recs_df.pipe(</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>            <span class="kw">lambda</span> df: pd.merge(</span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>                df, label_df[[user_col, item_col, rating_col, <span class="st">"rating_rank"</span>]],</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>                on<span class="op">=</span>[user_col, item_col], how<span class="op">=</span><span class="st">"outer"</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>        .assign(</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>            rating<span class="op">=</span><span class="kw">lambda</span> df: df[rating_col].fillna(<span class="dv">0</span>).astype(<span class="bu">int</span>),</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>            rec_ranking<span class="op">=</span><span class="kw">lambda</span> df: df[<span class="st">"rec_ranking"</span>].fillna(k <span class="op">+</span> <span class="dv">1</span>).astype(<span class="bu">int</span>),</span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        .sort_values([user_col, <span class="st">"rec_ranking"</span>])</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a>    )</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This function aligns your model’s recommendations with the ground truth, handling cases where recommended items don’t appear in the validation set.</p>
</section>
<section id="metric-logging-with-evidently" class="level3">
<h3 class="anchored" data-anchor-id="metric-logging-with-evidently">Metric Logging with Evidently</h3>
<p>The <code>src/eval/log_metrics.py</code> module uses the <a href="https://www.evidentlyai.com/">Evidently</a> library to compute and log comprehensive metrics:</p>
<div class="sourceCode" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_ranking_metrics(cfg: Config, eval_df):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Compute and log ranking metrics using Evidently.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Automatically integrates with MLflow for experiment tracking.</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    column_mapping <span class="op">=</span> ColumnMapping(</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        recommendations_type<span class="op">=</span><span class="st">"rank"</span>,</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        target<span class="op">=</span>cfg.data.rating_col,</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        prediction<span class="op">=</span><span class="st">"rec_ranking"</span>, </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        item_id<span class="op">=</span>cfg.data.item_col,</span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>        user_id<span class="op">=</span>cfg.data.user_col,</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    report <span class="op">=</span> Report(</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        metrics<span class="op">=</span>[</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            NDCGKMetric(k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_rerank),</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>            RecallTopKMetric(k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_retrieve),</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>            PrecisionTopKMetric(k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_rerank),</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a>            FBetaTopKMetric(k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_rerank),</span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            PersonalizationMetric(k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_rerank),</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>        ]</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>    report.run(reference_data<span class="op">=</span><span class="va">None</span>, current_data<span class="op">=</span>eval_df, column_mapping<span class="op">=</span>column_mapping)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Automatically log to MLflow if configured</span></span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> cfg.run.log_to_mlflow:</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        mlflow.log_artifact(evidently_report_fp)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Log individual metrics for easy comparison</span></span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> metric_result <span class="kw">in</span> report.as_dict()[<span class="st">"metrics"</span>]:</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>            <span class="co"># ... metric logging logic</span></span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> report</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The beauty of this approach is that Evidently handles the complex metric calculations while our framework automatically logs everything to MLflow for tracking and comparison.</p>
</section>
</section>
<section id="mlflow-integration-configuration-driven-experiment-tracking" class="level2">
<h2 class="anchored" data-anchor-id="mlflow-integration-configuration-driven-experiment-tracking">MLflow Integration: Configuration-Driven Experiment Tracking</h2>
<p>One of the standout features of our framework is the seamless MLflow integration through the configuration system. Instead of manually managing MLflow runs, everything is handled through the <code>ConfigLoader</code>:</p>
<section id="configuration-based-setup" class="level3">
<h3 class="anchored" data-anchor-id="configuration-based-setup">Configuration-Based Setup</h3>
<div class="sourceCode" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.cfg <span class="im">import</span> ConfigLoader</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a><span class="co"># Load configuration and initialize MLflow</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> ConfigLoader(<span class="st">"../cfg/common.yaml"</span>)</span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>cfg.run.run_name <span class="op">=</span> <span class="st">"001-baseline-popular"</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>cfg.run.experiment_name <span class="op">=</span> <span class="st">"Retrieve - Binary"</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a>cfg.init()  <span class="co"># This automatically sets up MLflow!</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>The <code>cfg.init()</code> method handles all the MLflow setup:</p>
<div class="sourceCode" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init(<span class="va">self</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Initialize MLflow experiment tracking automatically."""</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> <span class="va">self</span>.run.log_to_mlflow:</span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        logger.info(<span class="ss">f"Setting up MLflow experiment </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>run<span class="sc">.</span>experiment_name<span class="sc">}</span><span class="ss">..."</span>)</span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        mlflow.set_experiment(<span class="va">self</span>.run.experiment_name)</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>        mlflow.start_run(run_name<span class="op">=</span><span class="va">self</span>.run.run_name)</span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>._mlf_logger <span class="op">=</span> MLFlowLogger(</span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a>            experiment_name<span class="op">=</span><span class="va">self</span>.run.experiment_name,</span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a>            run_id<span class="op">=</span>mlflow.active_run().info.run_id,</span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a>            tracking_uri<span class="op">=</span>mlflow_uri,</span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a>            log_model<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> <span class="va">self</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="automatic-parameter-logging" class="level3">
<h3 class="anchored" data-anchor-id="automatic-parameter-logging">Automatic Parameter Logging</h3>
<p>The configuration system automatically logs all parameters to MLflow:</p>
<div class="sourceCode" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> log_config_to_mlflow(<span class="va">self</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Log all configuration parameters to MLflow with dot notation."""</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    flat_config <span class="op">=</span> flatten_dict(<span class="va">self</span>.config.model_dump())</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> key, value <span class="kw">in</span> flat_config.items():</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        mlflow.log_param(key, value)  <span class="co"># e.g., "train.learning_rate": 0.01</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This means every experiment automatically captures the complete configuration, making results fully reproducible.</p>
</section>
</section>
<section id="implementing-the-popularity-baseline" class="level2">
<h2 class="anchored" data-anchor-id="implementing-the-popularity-baseline">Implementing the Popularity Baseline</h2>
<p>Now let’s implement our first recommendation model: a popularity-based recommender. While simple, this baseline serves several important purposes:</p>
<ol type="1">
<li><strong>Sanity check</strong>: Ensures our evaluation pipeline works correctly</li>
<li><strong>Performance benchmark</strong>: Provides a baseline for more complex models to beat</li>
<li><strong>Production fallback</strong>: Often used as a fallback when personalized models fail</li>
</ol>
<section id="popularity-score-calculation" class="level3">
<h3 class="anchored" data-anchor-id="popularity-score-calculation">Popularity Score Calculation</h3>
<p>The implementation is straightforward—we rank items by their interaction frequency in the training data:</p>
<div class="sourceCode" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate popularity scores from training data</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>popular_items_df <span class="op">=</span> (</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    train_df.groupby(cfg.data.item_col, as_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    .size()</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    .assign(</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        score<span class="op">=</span><span class="kw">lambda</span> df: df[<span class="st">"size"</span>] <span class="op">/</span> df[<span class="st">"size"</span>].<span class="bu">max</span>(),  <span class="co"># Normalize to [0,1]</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>        rec_ranking<span class="op">=</span><span class="kw">lambda</span> df: df[<span class="st">"score"</span>]</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>        .rank(method<span class="op">=</span><span class="st">"first"</span>, ascending<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        .astype(<span class="bu">int</span>),</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>    .sort_values([<span class="st">"rec_ranking"</span>], ascending<span class="op">=</span>[<span class="va">True</span>])</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a><span class="co"># Get top-K popular items for evaluation</span></span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>top_popular_items_df <span class="op">=</span> popular_items_df.head(cfg.<span class="bu">eval</span>.top_k_retrieve)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This creates a ranking where the most frequently interacted items get the highest scores.</p>
</section>
<section id="generating-recommendations-for-all-users" class="level3">
<h3 class="anchored" data-anchor-id="generating-recommendations-for-all-users">Generating Recommendations for All Users</h3>
<p>For evaluation, we need to generate recommendations for every user in the validation set:</p>
<div class="sourceCode" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Create recommendations: same popular items for every user</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>unique_users <span class="op">=</span> val_df[cfg.data.user_col].unique()</span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a>recommendations_df <span class="op">=</span> pd.concat([</span>
<span id="cb9-4"><a href="#cb9-4" aria-hidden="true" tabindex="-1"></a>    top_popular_items_df.assign(<span class="op">**</span>{cfg.data.user_col: user})</span>
<span id="cb9-5"><a href="#cb9-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> user <span class="kw">in</span> unique_users</span>
<span id="cb9-6"><a href="#cb9-6" aria-hidden="true" tabindex="-1"></a>], ignore_index<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>This gives every user the same set of popular items, which is exactly what a popularity-based recommender should do.</p>
</section>
</section>
<section id="complete-evaluation-workflow" class="level2">
<h2 class="anchored" data-anchor-id="complete-evaluation-workflow">Complete Evaluation Workflow</h2>
<p>Here’s how everything comes together in the <code>010-baseline-popular.ipynb</code> notebook:</p>
<section id="setup-and-configuration" class="level3">
<h3 class="anchored" data-anchor-id="setup-and-configuration">1. Setup and Configuration</h3>
<div class="sourceCode" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.cfg <span class="im">import</span> ConfigLoader</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> src.<span class="bu">eval</span> <span class="im">import</span> (</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>    create_label_df, log_classification_metrics, </span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a>    log_ranking_metrics, merge_recs_with_target,</span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize configuration with MLflow</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>cfg <span class="op">=</span> ConfigLoader(<span class="st">"../cfg/common.yaml"</span>)</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>cfg.run.run_name <span class="op">=</span> <span class="st">"001-baseline-popular"</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>cfg.run.experiment_name <span class="op">=</span> <span class="st">"Retrieve - Binary"</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>cfg.init()  <span class="co"># Automatic MLflow setup</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="data-preparation" class="level3">
<h3 class="anchored" data-anchor-id="data-preparation">2. Data Preparation</h3>
<div class="sourceCode" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load preprocessed data with negative samples</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> pd.read_parquet(cfg.data.train_features_neg_fp)</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>val_df <span class="op">=</span> pd.read_parquet(cfg.data.val_features_neg_fp)</span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Create ground truth labels for evaluation</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>label_df <span class="op">=</span> create_label_df(</span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    val_df,</span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    user_col<span class="op">=</span>cfg.data.user_col,</span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a>    item_col<span class="op">=</span>cfg.data.item_col,</span>
<span id="cb11-10"><a href="#cb11-10" aria-hidden="true" tabindex="-1"></a>    rating_col<span class="op">=</span>cfg.data.rating_col,</span>
<span id="cb11-11"><a href="#cb11-11" aria-hidden="true" tabindex="-1"></a>    timestamp_col<span class="op">=</span>cfg.data.timestamp_col,</span>
<span id="cb11-12"><a href="#cb11-12" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="model-implementation-and-evaluation" class="level3">
<h3 class="anchored" data-anchor-id="model-implementation-and-evaluation">3. Model Implementation and Evaluation</h3>
<div class="sourceCode" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Calculate popularity scores and generate recommendations</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>popular_items_df <span class="op">=</span> (</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>    train_df.groupby(cfg.data.item_col, as_index<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>    .size()</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    .assign(</span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>        score<span class="op">=</span><span class="kw">lambda</span> df: df[<span class="st">"size"</span>] <span class="op">/</span> df[<span class="st">"size"</span>].<span class="bu">max</span>(),</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        rec_ranking<span class="op">=</span><span class="kw">lambda</span> df: df[<span class="st">"score"</span>].rank(method<span class="op">=</span><span class="st">"first"</span>, ascending<span class="op">=</span><span class="va">False</span>).astype(<span class="bu">int</span>),</span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    .sort_values([<span class="st">"rec_ranking"</span>], ascending<span class="op">=</span>[<span class="va">True</span>])</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a><span class="co"># Generate recommendations for all users</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>unique_users <span class="op">=</span> val_df[cfg.data.user_col].unique()</span>
<span id="cb12-14"><a href="#cb12-14" aria-hidden="true" tabindex="-1"></a>recommendations_df <span class="op">=</span> pd.concat([</span>
<span id="cb12-15"><a href="#cb12-15" aria-hidden="true" tabindex="-1"></a>    popular_items_df.head(cfg.<span class="bu">eval</span>.top_k_retrieve).assign(<span class="op">**</span>{cfg.data.user_col: user})</span>
<span id="cb12-16"><a href="#cb12-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> user <span class="kw">in</span> unique_users</span>
<span id="cb12-17"><a href="#cb12-17" aria-hidden="true" tabindex="-1"></a>], ignore_index<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb12-18"><a href="#cb12-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-19"><a href="#cb12-19" aria-hidden="true" tabindex="-1"></a><span class="co"># Merge recommendations with ground truth</span></span>
<span id="cb12-20"><a href="#cb12-20" aria-hidden="true" tabindex="-1"></a>eval_df <span class="op">=</span> merge_recs_with_target(</span>
<span id="cb12-21"><a href="#cb12-21" aria-hidden="true" tabindex="-1"></a>    recommendations_df, label_df,</span>
<span id="cb12-22"><a href="#cb12-22" aria-hidden="true" tabindex="-1"></a>    k<span class="op">=</span>cfg.<span class="bu">eval</span>.top_k_retrieve,</span>
<span id="cb12-23"><a href="#cb12-23" aria-hidden="true" tabindex="-1"></a>    user_col<span class="op">=</span>cfg.data.user_col,</span>
<span id="cb12-24"><a href="#cb12-24" aria-hidden="true" tabindex="-1"></a>    item_col<span class="op">=</span>cfg.data.item_col,</span>
<span id="cb12-25"><a href="#cb12-25" aria-hidden="true" tabindex="-1"></a>    rating_col<span class="op">=</span>cfg.data.rating_col,</span>
<span id="cb12-26"><a href="#cb12-26" aria-hidden="true" tabindex="-1"></a>)</span>
<span id="cb12-27"><a href="#cb12-27" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-28"><a href="#cb12-28" aria-hidden="true" tabindex="-1"></a><span class="co"># Log ranking metrics</span></span>
<span id="cb12-29"><a href="#cb12-29" aria-hidden="true" tabindex="-1"></a>ranking_report <span class="op">=</span> log_ranking_metrics(cfg, eval_df)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="classification-metrics-1" class="level3">
<h3 class="anchored" data-anchor-id="classification-metrics-1">4. Classification Metrics</h3>
<p>We also evaluate the model’s ability to predict binary relevance:</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Prepare data for classification evaluation</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a>eval_classification_df <span class="op">=</span> pd.merge(</span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a>    val_df,</span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a>    popular_items_df[[cfg.data.item_col, <span class="st">"score"</span>]],</span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a>    on<span class="op">=</span>[cfg.data.item_col],</span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    how<span class="op">=</span><span class="st">"left"</span>,</span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    validate<span class="op">=</span><span class="st">"m:1"</span>,</span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>).assign(label<span class="op">=</span><span class="kw">lambda</span> df: df[cfg.data.rating_col].gt(<span class="dv">0</span>).astype(<span class="bu">int</span>))</span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="co"># Log classification metrics</span></span>
<span id="cb13-11"><a href="#cb13-11" aria-hidden="true" tabindex="-1"></a>classification_report <span class="op">=</span> log_classification_metrics(</span>
<span id="cb13-12"><a href="#cb13-12" aria-hidden="true" tabindex="-1"></a>    cfg, eval_classification_df, </span>
<span id="cb13-13"><a href="#cb13-13" aria-hidden="true" tabindex="-1"></a>    target_col<span class="op">=</span><span class="st">"label"</span>, </span>
<span id="cb13-14"><a href="#cb13-14" aria-hidden="true" tabindex="-1"></a>    prediction_col<span class="op">=</span><span class="st">"score"</span></span>
<span id="cb13-15"><a href="#cb13-15" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="experiment-cleanup" class="level3">
<h3 class="anchored" data-anchor-id="experiment-cleanup">5. Experiment Cleanup</h3>
<div class="sourceCode" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Log all configuration parameters and end the MLflow run</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a><span class="cf">if</span> cfg.run.log_to_mlflow:</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>    cfg.log_config_to_mlflow()</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>    mlflow.end_run()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
</section>
<section id="understanding-the-results" class="level2">
<h2 class="anchored" data-anchor-id="understanding-the-results">Understanding the Results</h2>
<p>After running the baseline, you’ll see several types of outputs:</p>
<section id="mlflow-experiment-tracking" class="level3">
<h3 class="anchored" data-anchor-id="mlflow-experiment-tracking">MLflow Experiment Tracking</h3>
<ul>
<li><strong>Parameters</strong>: All configuration values automatically logged</li>
<li><strong>Metrics</strong>: Ranking and classification metrics with step-wise tracking</li>
<li><strong>Artifacts</strong>: Evidently HTML reports for detailed analysis</li>
</ul>
</section>
<section id="evidently-reports" class="level3">
<h3 class="anchored" data-anchor-id="evidently-reports">Evidently Reports</h3>
<p>The framework generates comprehensive HTML reports showing: - Precision@K and Recall@K curves - NDCG scores across different K values - Personalization metrics - ROC curves and precision-recall curves</p>
</section>
<section id="baseline-performance-expectations" class="level3">
<h3 class="anchored" data-anchor-id="baseline-performance-expectations">Baseline Performance Expectations</h3>
<p>For a popularity-based recommender, expect: - <strong>Low personalization scores</strong>: Everyone gets the same recommendations - <strong>Moderate precision@K</strong>: Popular items have broad appeal - <strong>Variable recall</strong>: Depends on how well popular items match user preferences - <strong>Decent ROC-AUC</strong>: Popular items are often relevant</p>
</section>
</section>
<section id="why-this-foundation-matters" class="level2">
<h2 class="anchored" data-anchor-id="why-this-foundation-matters">Why This Foundation Matters</h2>
<p>This evaluation framework and baseline implementation provide several critical benefits:</p>
<section id="reproducible-experiments" class="level3">
<h3 class="anchored" data-anchor-id="reproducible-experiments">1. <strong>Reproducible Experiments</strong></h3>
<p>Every experiment is fully tracked with: - Complete configuration parameters - Evaluation metrics across multiple dimensions - Generated artifacts for detailed analysis</p>
</section>
<section id="systematic-model-development" class="level3">
<h3 class="anchored" data-anchor-id="systematic-model-development">2. <strong>Systematic Model Development</strong></h3>
<p>The framework enables you to: - Compare models objectively using consistent metrics - Track improvements over time - Identify which changes actually help</p>
</section>
<section id="production-readiness" class="level3">
<h3 class="anchored" data-anchor-id="production-readiness">3. <strong>Production Readiness</strong></h3>
<p>The evaluation patterns you establish here will: - Scale to more complex models - Integrate with production monitoring - Support A/B testing frameworks</p>
</section>
<section id="debugging-and-validation" class="level3">
<h3 class="anchored" data-anchor-id="debugging-and-validation">4. <strong>Debugging and Validation</strong></h3>
<p>A working baseline helps you: - Validate that your evaluation pipeline works correctly - Identify data quality issues early - Establish reasonable performance expectations</p>
</section>
</section>
<section id="configuration-management-best-practices" class="level2">
<h2 class="anchored" data-anchor-id="configuration-management-best-practices">Configuration Management Best Practices</h2>
<p>The configuration-driven approach offers several advantages:</p>
<section id="centralized-configuration" class="level3">
<h3 class="anchored" data-anchor-id="centralized-configuration">Centralized Configuration</h3>
<div class="sourceCode" id="cb15"><pre class="sourceCode yaml code-with-copy"><code class="sourceCode yaml"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="co"># cfg/common.yaml</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="fu">eval</span><span class="kw">:</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">top_k_retrieve</span><span class="kw">:</span><span class="at"> </span><span class="dv">100</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">top_k_rerank</span><span class="kw">:</span><span class="at"> </span><span class="dv">10</span></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">min_roc_auc</span><span class="kw">:</span><span class="at"> </span><span class="fl">0.5</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a><span class="fu">run</span><span class="kw">:</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">log_to_mlflow</span><span class="kw">:</span><span class="at"> </span><span class="ch">true</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a><span class="at">  </span><span class="fu">experiment_name</span><span class="kw">:</span><span class="at"> </span><span class="st">"Retrieve - Binary"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="easy-experimentation" class="level3">
<h3 class="anchored" data-anchor-id="easy-experimentation">Easy Experimentation</h3>
<div class="sourceCode" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Try different evaluation settings</span></span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>cfg.<span class="bu">eval</span>.top_k_retrieve <span class="op">=</span> <span class="dv">50</span></span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>cfg.<span class="bu">eval</span>.top_k_rerank <span class="op">=</span> <span class="dv">5</span></span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Or different experiment tracking</span></span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>cfg.run.experiment_name <span class="op">=</span> <span class="st">"Retrieve - Rating"</span></span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a>cfg.train.label_format <span class="op">=</span> <span class="st">"rating"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</section>
<section id="automatic-documentation" class="level3">
<h3 class="anchored" data-anchor-id="automatic-documentation">Automatic Documentation</h3>
<p>Every MLflow run captures the complete configuration, making it easy to reproduce results or understand what changed between experiments.</p>
</section>
</section>
<section id="whats-next" class="level2">
<h2 class="anchored" data-anchor-id="whats-next">What’s Next</h2>
<p>With your evaluation framework and baseline model in place, you have:</p>
<ul>
<li><strong>A robust evaluation pipeline</strong> that can assess recommendation quality across multiple dimensions</li>
<li><strong>MLflow integration</strong> for systematic experiment tracking and comparison</li>
<li><strong>A working baseline model</strong> that establishes performance expectations</li>
<li><strong>The foundation</strong> for building and evaluating more sophisticated models</li>
</ul>
<p>In <a href="../c5/index.qmd">Chapter 5</a>, we’ll build on this foundation to implement sequence-based recommendation models that can capture temporal patterns in user behavior. The evaluation framework you’ve built here will be essential for measuring whether these more complex models actually improve upon our simple baseline.</p>
</section>
<section id="recap" class="level2">
<h2 class="anchored" data-anchor-id="recap">Recap</h2>
<p>This chapter established the critical evaluation infrastructure for your recommendation system:</p>
<ul>
<li><strong>Comprehensive metrics</strong>: Ranking, classification, and diversity metrics using Evidently</li>
<li><strong>Seamless MLflow integration</strong>: Configuration-driven experiment tracking</li>
<li><strong>Baseline implementation</strong>: Popularity-based recommender as a performance benchmark</li>
<li><strong>Complete workflow</strong>: From data preparation to metric logging and experiment cleanup</li>
</ul>
<p>The evaluation framework you’ve built is production-ready and will scale with you as you develop more sophisticated models. Most importantly, you now have a systematic way to measure progress and make data-driven decisions about model improvements.</p>
<hr>
<p><br> If you find this tutorial helpful, please cite this writeup as:</p>
<blockquote class="blockquote">
<p>Quy, Dinh. (May 2025). Building Real-time RecSys Chapter 4 - Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation. dvquys.com. https://dvquys.com/projects/real-time-seq-recsys/c4/.</p>
</blockquote>


</section>


<div id="quarto-appendix" class="default"><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>You may take a look at this writing <a href="https://eugeneyan.com/writing/counterfactual-evaluation/">Counterfactual Evaluation for Recommendation Systems</a> to learn more.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>You may take a look at this writing <a href="https://eugeneyan.com/writing/counterfactual-evaluation/">Counterfactual Evaluation for Recommendation Systems</a> to learn more.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/dvquys\.com");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      const annoteTargets = window.document.querySelectorAll('.code-annotation-anchor');
      for (let i=0; i<annoteTargets.length; i++) {
        const annoteTarget = annoteTargets[i];
        const targetCell = annoteTarget.getAttribute("data-target-cell");
        const targetAnnotation = annoteTarget.getAttribute("data-target-annotation");
        const contentFn = () => {
          const content = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
          if (content) {
            const tipContent = content.cloneNode(true);
            tipContent.classList.add("code-annotation-tip-content");
            return tipContent.outerHTML;
          }
        }
        const config = {
          allowHTML: true,
          content: contentFn,
          onShow: (instance) => {
            selectCodeLines(instance.reference);
            instance.reference.classList.add('code-annotation-active');
            window.tippy.hideAll();
          },
          onHide: (instance) => {
            unselectCodeLines();
            instance.reference.classList.remove('code-annotation-active');
          },
          maxWidth: 300,
          delay: [50, 0],
          duration: [200, 0],
          offset: [5, 10],
          arrow: true,
          appendTo: function(el) {
            return el.parentElement.parentElement.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto',
          placement: 'right',
          popperOptions: {
            modifiers: [
            {
              name: 'flip',
              options: {
                flipVariations: false, // true by default
                allowedAutoPlacements: ['right'],
                fallbackPlacements: ['right', 'top', 'top-start', 'top-end', 'bottom', 'bottom-start', 'bottom-end', 'left'],
              },
            },
            {
              name: 'preventOverflow',
              options: {
                mainAxis: false,
                altAxis: false
              }
            }
            ]        
          }      
        };
        window.tippy(annoteTarget, config); 
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://giscus.app/client.js" data-repo="dvquy13/icy-touch-comments" data-repo-id="R_kgDONEpzPA" data-category="General" data-category-id="DIC_kwDONEpzPM4Cjn8n" data-mapping="title" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="top" data-theme="light" data-lang="en" crossorigin="anonymous" data-loading="lazy" async="">
</script>
<input type="hidden" id="giscus-base-theme" value="light">
<input type="hidden" id="giscus-alt-theme" value="dark">
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
      &nbsp;
    </div>   
    <div class="nav-footer-center">

<div class="cookie-consent-footer"><a href="#" id="open_preferences_center">Cookie Preferences</a></div></div>
    <div class="nav-footer-right">
      &nbsp;
    </div>
  </div>
</footer>




<script src="../../../site_libs/quarto-html/zenscroll-min.js"></script>
</body></html>