[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Quy—an Applied Data Scientist. I’m from Hanoi, Vietnam 🇻🇳. Currently living in Singapore 🇸🇬.\nI blog about RecSys, ML, Productivity and other random things. You can find some of my side projects here.\nI teach a RecSys course for practitioners at FullStackDataScience."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Building Real-time RecSys Chapter 3 - Negative Sampling\n\n\nWhy your model needs to see what users don’t want—and how to generate those examples\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 25, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Real-time RecSys Chapter 1 - Introduction and Project Overview\n\n\nUnderstanding session-based recommendations and setting up your development environment\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n9 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering\n\n\nExploring Amazon product dataset and building features for sequence models\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Conversational Assistant for Restaurant Discovery and Booking\n\n\nLessons from Creating a Chatbot Using RAG, LLMs, and Advanced Retrieval Techniques\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nLet’s build an ML system\n\n\nEnd-to-end set up and deploy on Kubernetes\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/review-rec-bot/index.html",
    "href": "projects/review-rec-bot/index.html",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "",
    "text": "Project Details"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#demo",
    "href": "projects/review-rec-bot/index.html#demo",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#problem-statement",
    "href": "projects/review-rec-bot/index.html#problem-statement",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Problem Statement",
    "text": "Problem Statement\nListing/reservation businesses like Yelp offer value to users by providing useful information to make them find out where to go next. Good search and recommendation systems go a long way, but they are still far from delivering the ultimate experience where users can interact naturally with the system for complex queries or have a conversation to drill down their needs."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#approach",
    "href": "projects/review-rec-bot/index.html#approach",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Approach",
    "text": "Approach\nBuild a chatbot assistant to assist users in discovering places to go and booking.\nWorkflow:\n\nDownload Yelp reviews data. Sample 5,240 reviews from 100 businesses.\nSet up development environment including experimentation tracking via MLflow, observability with Arize Phoenix\nBuild MVP version using LlamaIndex, Qdrant\nBuild synthetic evaluation datasets with 30 questions for retrieval and response. Manual response dataset are gradually built up and added based on error analysis\nConduct error analysis and look at the model’s output to come up with new iteration ideas. Run a total of 10 experiments to improve the RAG, with notable attempts including: Replacing Llama-8.1B with GPT-4o-mini, Fine-tuning Embedding Model, Hybrid Retrievers, Semantic Chunking, BGE Reranker, Query Expansion\nBuild RAG Agent based on OpenAI API with Query Engine tool and Reservation Service tool. Chatbot UI is built with Chainlit."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#evaluation",
    "href": "projects/review-rec-bot/index.html#evaluation",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Evaluation",
    "text": "Evaluation\nEvaluation results can be found here.\nTwo proposed key metrics are Retrieval Hit Rate and Response Correctness.\nRetrieval is a critical component in any RAG system. Along with data prep, retrieval sits at the top of the pipeline so any improvements in these fronts is more likely to improve the overall system. Hit rate is chosen as a key metric because since we can employ rerank as a subsequent step, we have room to optimize for the ranking issues. \nFor response, Correctness measures both how relevant the answer is with respect to the query and how correct it is compared to the referenced answer. It’s therefore a better indicator than pure relevance, which is just based on the query and hence easier to get right.\nFor reference, Response Correctness on synthetic dataset has improved +166% from 1.75 / 5.00 from MVP version to 4.67 / 5.00 on the current version. The current Retrieval Hit Rate @ 50 reaches 73%, not directly comparable but at MVP version Retrieval Hit Rate @ 10 was 20%.\nAs next steps, while there is not much room to improve Response Correctness, we ought to increase Retrieval Hit Rate to 90% which should be doable since this dataset only contains a small amount of data."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#learningsremarks",
    "href": "projects/review-rec-bot/index.html#learningsremarks",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Learnings/Remarks",
    "text": "Learnings/Remarks\n\nUsing question-style query leads to 5-20% uplift in retrieval hit rate compared to using keyword search\nBM25 Retriever alone results in 200% increase in retrieval effectiveness including hit rate, average precision, MRR and NDCG\nFine-tuning small embedding model like Snowflake/snowflake-arctic-embed-m-v1.5 yield +80% retrieval effectiveness, especially rankings of the retrieved nodes\nUsing GPT-4o-mini as response synthesizer significantly improve the quality of response in all aspects (especially correctness from 2.6 to 3.8) compared to Llama 3.1-8B-Instruct\nUsing TreeSummarize with custom prompt yields a +10% uplift on response correctness evaluation, from 3.97 to 4.37. Based on eyeballing we also see a way better response that is recommendation-like"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#challenges",
    "href": "projects/review-rec-bot/index.html#challenges",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Challenges",
    "text": "Challenges\n\nChallenge 1: Auto-retrieval not reliable\nWhile theoretically both precision and recall should be greatly improved if we are able to apply the right filters for User questions instead of relying on embedding/keyword matching, my first attempt to apply auto-retrieval with ChromaDB did not yield promising results. There were at least two syntactic issues which broke the agentic workflow. Even after fixing those two the unreliable nature of this approach is still there and I also witnessed a -10% degradation in Retrieval Hit Rate.\nIn the end I forfeited the feature but nevertheless look forward to a way to re-applying this technique.\n\n\nChallenges 2: Indexing pipeline takes too long\nIndexing 70K nodes from 30K reviews for 400 businesses takes more than 6 hours!"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#future-improvements",
    "href": "projects/review-rec-bot/index.html#future-improvements",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Future Improvements",
    "text": "Future Improvements\n\nGuardrail system inputs and outputs\nExperiment with Contextual Compression and Filters\nFine tune LLM Re-ranker (FlagEmbedding BGE Reranker)\nTry ColBERT as a new retriever (may be add to the list of retrievers)\nTry different loss function in training embeddings\nImprove the diversity by implement custom re-ranker that weight downs the reviews from the already seen biz_id\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Sep 2024). Building a Conversational Assistant for Restaurant Discovery and Booking. dvquys.com. https://dvquys.com/projects/review-rec-bot/."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html",
    "href": "posts/site-search-elasticsearch/index.html",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "",
    "text": "This article is originally published in Towards Data Science"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#intro",
    "href": "posts/site-search-elasticsearch/index.html#intro",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Intro",
    "text": "Intro\nOver the last 22 months I have been working as a site-search engineer who uses Elasticsearch to help improve relevance in our restaurant platform. I have deployed in total 83 releases including 3 major versions.\nWith roughly one release per week, I can say that not only our search engine is much better than it was 2 years ago, but I have also learned quite a lot. Though still far from a great search engine, here are some things worth sharing in my opinion. More importantly, I want to get feedback about them.\nThis blog post is to provide an approach to design the Elasticsearch query template to deal with common site-search problems including searching for matches across different fields, boosting results and testing. Together we will identify issues with the default approach and then gradually come up with a new one to address the issues altogether.\nThis Github repo contains the examples and code discussed in this post."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#main",
    "href": "posts/site-search-elasticsearch/index.html#main",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Main",
    "text": "Main\nWe now play the role of a search engineer for a restaurant platform, which allows diners to discover and make reservation for their next meals. We haven’t had much experience, but luckily the app does not require accuracy level of Google from the start. The key is to make gradual visible progresses!\nAlright, let’s dive into it. First off, we make sure user can search for restaurant by name. Here we can rely on the simple default query-match to get the job done.\n\n\nCode: Index our first two restaurants\n\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"001sabichuong\" } }\n{ \"restaurant_name\": \"Sa Bi Chuong\", \"cuisine\": \"Vietnamese\", \"rating\": 5.0 }\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"002vietnamesephonoodle\" } }\n{ \"restaurant_name\": \"Vietnamese Pho Noodle\", \"cuisine\": \"Vietnamese\", \"rating\": 4.0 }\n\n# Test searching for one\n# Should return Vietnamese Pho Noodle\nGET restaurant/_search\n{\n  \"query\" : {\n    \"match\" : { \"restaurant_name\": \"vietnamese\" }\n  }\n}\n\nThe above snippet can be run at Kibana’s Dev Tools &gt; Console, which will be available at your localhost:5601 if you follow the repo.\nThe code is self-explained. We ask Elasticsearch to return restaurants whose name contains vietnamese. And we get back one result for Vietnamese Pho Noodle. No problems.\nBut we quickly find out that name is not the only place we want to look for when user submit a query. Given keywordvietnamese we should also return the restaurant Sa Bi Chuong, because it’s a Vietnamese restaurant as tagged in the cuisine. A multi_match query allows us to do exactly that.\n\n\nCode: Matching multiple fields\n\n# Should return all 2 Vietnamese restaurant with the Vietnamese Pho Noodle on top\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  }\n}\n\n\n\nResult\n\n\"hits\": {\n    ...\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.6931471,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.18232156,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      }\n    ]\n  }"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#problems-with-the-default-tfidf",
    "href": "posts/site-search-elasticsearch/index.html#problems-with-the-default-tfidf",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Problems with the default TFIDF",
    "text": "Problems with the default TFIDF\nNotice the above scores. The first one is like 4 times higher than the second, indicating that it’s much more relevant given query vietnamese. One might have an assumption that because matching at multiple fields will make the score higher.\nWhenever we have doubts, we can use Elasticsearch explain to get a detailed breakdown of its scoring components.\n\n\nCode: Let’s use explain=true to see what happens under the hood\n\n# Vietnamese Pho Noodle is on top because of the default implementation of TFIDF that penalizes the matching at cuisine field because there are multiple restaurants with cuisine=Vietnamese while there are only one restaurant with name=Vietnamese\n# Question: But why having the name Vietnamese in its name makes it more Vietnamese than other restaurants?\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  },\n  \"explain\": true\n}\n# Result 👇\n\"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.6931471,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        },\n        \"_explanation\": {\n          \"value\": 0.6931471,\n          \"description\": \"max of:\",\n          \"details\": [\n            # Matching in field `cuisine` yields score=0.18\n            # Note that by default the score is calculated by TFIDF\n            # More info about Elasticsearch TFIDF: https://www.elastic.co/guide/en/elasticsearch/reference/8.6/index-modules-similarity.html#bm25\n            {\n              \"value\": 0.18232156,\n              \"description\": \"weight(cuisine:vietnamese in 1) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            },\n            # Matching in field `restaurant_name` yields score=0.69\n            {\n              \"value\": 0.6931471,\n              \"description\": \"weight(restaurant_name:vietnamese in 1) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            }\n            # Because the final score is \"max of:\" those two above scores,\n            # it is equal to the matching score with `restaurant_name`\n          ]\n        }\n      },\n      {\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.18232156,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        },\n        # Similarly since there's no matching with `restaurant_name`,\n        # here the final score is equal to the matching score of `cuisine`\n        \"_explanation\": {\n          \"value\": 0.18232156,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.18232156,\n              \"description\": \"weight(cuisine:vietnamese in 0) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            }\n          ]\n        }\n      }\n    ]\n  }\n\nAbove we can see that Vietnamese Pho Noodle is on top because of the default implementation of TFIDF that penalizes the matching at cuisine field because there are multiple restaurants with cuisine=Vietnamese while there are only one restaurant with name=Vietnamese.\nDiving into the _explanation block, we realize that score difference originates from the TFIDF matching output for restaurant_name. This is expected as the algorithm assumes that a keyword is a better signal if it is not common and usually found in a lot of documents (sort of a solution to automatically handle stopwords). In our examples, both the restaurants have cuisine Vietnamese so according to TFIDF, that match does not say much about the relevance of the documents.\nWhether we should encourage this behavior is a question. Is it true that having Vietnamese in the name make one restaurant more “Vietnamese” than the other?\nAnother problem with TFIDF is that it takes into account the length of the field.\n\n\nCode: Let’s add another restaurant\n\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"003vietnamesepho\" } }\n{ \"restaurant_name\": \"Vietnamese Pho\", \"cuisine\": \"Vietnamese\", \"rating\": 3.0 }\n\n# In the below example we see that the new Vietnamese Pho restaurant is ranked higher...\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese pho\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  },\n  \"explain\": true\n}\n\nYou can find the detailed and lengthy result in Appendix 1. In short, we see that the result ranks restaurant Vietnamese Pho first and then Vietnamese Pho Noodle. Analyzing the component scores indicates that the key difference is that Vietnamese Pho has length=2 (words) while Vietnamese Pho Noodle has length=3. It feels unintuitive since we know that the second restaurant has higher rating, given that both, in practice, are equally matching to user’s keyword."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#reranking-boosting-with-function_score",
    "href": "posts/site-search-elasticsearch/index.html#reranking-boosting-with-function_score",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Reranking (boosting) with function_score",
    "text": "Reranking (boosting) with function_score\nAs we talk about rating, we can wrap our query with function_score to incorporate that information to modify our matching scores, hence have a better control over our ranking.\n\n\nCode: Function score\n\nGET restaurant/_search\n{\n  \"query\": {\n    \"function_score\": {\n      # Our main query is wrapped in a function_score clause\n      \"query\": {\n        \"multi_match\" : {\n          \"query\": \"vietnamese\",\n          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n        }\n      },\n      # We define the functions that will be applied on top of the matching scores\n      # returned by our main query\n      \"functions\": [\n        {\n          \"field_value_factor\": {\n            \"field\": \"rating\",\n            \"modifier\": \"none\",\n            \"missing\": 1\n          }\n        }\n      ],\n      # Retrieve the max boosting defined inside `functions`\n      # Above there is only one boosting so it's applied by default\n      \"score_mode\": \"max\",\n      # Multiply the matching score with the boosting calculated from functions\n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 1.7885544,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 1.5706451,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.66765696,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      }\n    ]\n  }\n}\n\nThe higher rating restaurant is on top now. But how about restaurant Sa Bi Chuong with rating=5? It being the last result seems like we haven’t boosted “enough”.\nWe might start tinkering a bit more with function_score to make that happen. Here is one of the implementation which models the boosting in a non-linear manner to effectively apply a strong boost on documents with rating=5.\n\n\nCode: Non-linear function score boosting\n\nGET restaurant/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"multi_match\" : {\n          \"query\": \"vietnamese\",\n          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n        }\n      },\n      \"functions\": [\n        # Apply a non-linear function to model that\n        # a rating of 5 has much more weight than rating of 4 (not just 25% more)\n        {\n          \"filter\": {\n            \"range\": {\n              \"rating\": {\n                \"gte\": 5,\n                \"lte\": 5\n              }\n            }\n          },\n          \"weight\": 10\n        },\n        {\n          \"filter\": {\n            \"range\": {\n              \"rating\": {\n                \"gte\": 4,\n                \"lt\": 5\n              }\n            }\n          },\n          \"weight\": 2\n        }\n      ],\n      \"score_mode\": \"max\",\n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 1.3353139,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.8942772,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 0.52354836,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        }\n      }\n    ]\n  }\n}\n\nYou may wonder that: “Isn’t the function boosting now looking too arbitrary? Will it work for other cases?”. Indeed, that’s the question we should ask ourselves. Overtime, with more and more requirements, our query template will grow in complexity, leading to conflicts between the modifications we make.\nLet’s move to the next example to illustrate what I mean by “conflict”."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#the-complexity-comes-with-fuzzy-matching",
    "href": "posts/site-search-elasticsearch/index.html#the-complexity-comes-with-fuzzy-matching",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "The complexity comes with fuzzy matching",
    "text": "The complexity comes with fuzzy matching\nWhile not vital, the ability to handle user’s typo is always a nice-to-have feature, especially when they are now familiar with smart search engine like Google’s. Elasticsearch has a built-in mechanism called fuzzy matching, which is configurable with the option fuzziness.\n\n\nCode: fuzzy matching\n\n# The use of `bool` query below is to implement the logic: At least one condition should match\nPUT _scripts/01-default-fuzzy-search-template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"function_score\": {\n          \"query\": {\n            \"bool\": {\n              \"must\": [\n                {\n                  \"bool\": {\n                    \"should\": [\n                      {\n                        \"multi_match\" : {\n                          \"query\": \"{{query_string}}\",\n                          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n                        }\n                      },\n                      {\n                        \"multi_match\" : {\n                          \"query\": \"{{query_string}}\",\n                          \"fields\": [ \"restaurant_name\", \"cuisine\" ],\n                          # For the purpose of this demo, default behavior works well enough\n                          \"fuzziness\": \"AUTO\"\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          },\n          \"functions\": [\n            {\n              \"filter\": {\n                \"range\": {\n                  \"rating\": {\n                    \"gte\": 5,\n                    \"lte\": 5\n                  }\n                }\n              },\n              \"weight\": 10\n            },\n            {\n              \"filter\": {\n                \"range\": {\n                  \"rating\": {\n                    \"gte\": 4,\n                    \"lt\": 5\n                  }\n                }\n              },\n              \"weight\": 2\n            }\n          ],\n          \"score_mode\": \"max\",\n          \"boost_mode\": \"multiply\"\n        }\n      }\n    },\n    \"params\": {\n      \"query_string\": \"My query string\"\n    }\n  }\n}\n\nNotice that we just created a query template instead of running a query. We can now invoke the query with paramaters, which is a nice feature Elasticsearch introduces to make our code look less overwhelming. Like this:\nGET /_search/template\n{\n  \"id\": \"01-default-fuzzy-search-template\",\n  \"params\": {\n    \"query_string\": \"vietnames\"\n  }\n}\nThe above query returns our expected Vietnamese restaurant given a typo keyword vietnames. Under the hood, fuzzy matching uses Levenshtein edit distance, which measures similarity between strings by the number of modifications one make to make one become another. In our example, we just need to add one letter e at the end to make vietnames become vietnamese. Quite an easy task for the algorithm. One might also argue that it’s quite easy for our developers as well. 2 lines of code and a new beautiful feature.\nWell, the interesting bit lies elsewhere. One day, our sales team suddenly comes to us with a complaint that search result is wrong. People are getting Japanese BBQ restaurants over Korean ones even when they explicitly search for kbbq (which is a common acronym for korean bbq).\nHere are the restaurants:\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"004parkhangseokbbq\" } }\n{ \"restaurant_name\": \"Park Hang-seo's KBBQ\", \"cuisine\": \"Korean\", \"rating\": 2.0 }\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"005bestbbqintown\" } }\n{ \"restaurant_name\": \"Best BBQ in town\", \"cuisine\": \"Japanese\", \"rating\": 5.0 }\nQuery:\n{\n  \"id\": \"01-default-fuzzy-search-template\",\n  \"params\": {\n    \"query_string\": \"kbbq\"\n  }\n}\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"005bestbbqintown\",\n        \"_score\": 8.384459,\n        \"_source\": {\n          \"restaurant_name\": \"Best BBQ in town\",\n          \"cuisine\": \"Japanese\",\n          \"rating\": 5\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"004parkhangseokbbq\",\n        \"_score\": 2.5153382,\n        \"_source\": {\n          \"restaurant_name\": \"Park Hang-seo's KBBQ\",\n          \"cuisine\": \"Korean\",\n          \"rating\": 2\n        }\n      }\n    ]\n  }\n}\n\nTo understand what is happening, we need to enable explain=true to see what contributes to the final scores. As this time the output is too verbose, here are the findings:\n\nThe keyword matching score (before boosting) for the Best BBQ in town restaurant is 0.8, less than the 1.2 of Park Hang-seo's KBBQ\nSo if no boosting applied, we will see Park Hang-seo's KBBQ restaurant ranks at the first position\nBut then the boosting from rating modifies the score, leading to the ordering as we can see\n\nOne way to frame the issue is that we have imperfect boosting. Say we have a better formula that strikes the right balances, then the problem should be solved. But it’s close to impossible to guarantee that the new formula will not cause any other issues. We don’t want these kinds of issue creep into the system without any notice and then some day being flagged out by stakeholders. We want to be the first to be aware of those issues, especially whenever we make any changes. Therefore, before discussing potential solutions, I hope we all agree that the very next important thing we should do is (yes, you are probably thinking about the same thing as I am) setting up a testing/evaluation mechanism."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#how-should-we-create-test-cases-for-this-search-application",
    "href": "posts/site-search-elasticsearch/index.html#how-should-we-create-test-cases-for-this-search-application",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "How should we create test cases for this search application?",
    "text": "How should we create test cases for this search application?\nIMHO, the first challenge is about moving data. The queries and the documents can both grow over time, so a static mock dataset might not be a very good representative of the search relevance anymore after a month. The next bit is related to our mindset. Sometimes we might need to think about whether we need 100% passed test cases in order to fix this new very urgent issue. For example, there are cases where if you fix some issues then the search result orderings of the other test cases might alter a bit. If we hard-code the rankings, then we might sweat ourselves trying to tweak our query template. But in practice a lot of the times we neither don’t need the ranking to be exactly pre-defined nor we are perfectly sure about which ordering is actually optimal. We should consider using a soft mechanism where we quantify the relevance of the system and using threshold instead.\nHere we look at how we can use Elasticsearch Ranking Evaluation API to implement such evaluation scheme:\n\n\nCode: Evaluation API\n\nGET restaurant/_rank_eval\n{\n  # Query template comes in really handy when used in conjunction with _rank_eval\n  \"templates\": [\n     {\n        \"id\": \"01-default-fuzzy-search-template\",\n        \"template\": {\n            \"id\": \"01-default-fuzzy-search-template\"\n        }\n     }\n  ],\n  \"requests\": [\n    {\n     \"id\": \"kbbq_query\",\n     # Here we manually define the true positives with rating &gt;= 1.0\n     # The actual rating number helps when using metrics that takes into account\n     # the ranking of the search results\n     \"ratings\": [\n        { \"_index\": \"restaurant\", \"_id\": \"004parkhangseokbbq\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"005bestbbqintown\", \"rating\": 1 }\n      ],\n     \"template_id\": \"01-default-fuzzy-search-template\",\n     \"params\": {\n        \"query_string\": \"kbbq\"\n      }\n    },\n    {\n     \"id\": \"vietnamese_query\",\n     \"ratings\": [\n        { \"_index\": \"restaurant\", \"_id\": \"001sabichuong\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"002vietnamesephonoodle\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"003vietnamesepho\", \"rating\": 3 }\n      ],\n     \"template_id\": \"01-default-fuzzy-search-template\",\n     \"params\": {\n        \"query_string\": \"vietnamese\"\n      }\n    }\n  ],\n  \"metric\": {\n    \"dcg\": {\n      \"k\": 5,\n      \"normalize\": true\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"metric_score\": 0.8549048706984328,  # This is the overall metric score, best is 1.0, worst is 0.0\n  \"details\": {\n    \"kbbq_query\": {\n      # This kbbq_query has a imperfect score because it ranks the more relevant result lower\n      \"metric_score\": 0.7098097413968655,\n      \"unrated_docs\": [],\n      \"hits\": [\n        {\n          \"hit\": {\n            \"_index\": \"restaurant\",\n            \"_id\": \"005bestbbqintown\",\n            \"_score\": 8.384459\n          },\n          \"rating\": 1\n        },\n        {\n          \"hit\": {\n            \"_index\": \"restaurant\",\n            \"_id\": \"004parkhangseokbbq\",\n            \"_score\": 2.5153382\n          },\n          \"rating\": 3\n        }\n      ],\n      \"metric_details\": {\n        ...\n      }\n    },\n    \"vietnamese_query\": {\n      \"metric_score\": 1,\n      \"unrated_docs\": [],\n      \"hits\": [\n        ...\n      ],\n      \"metric_details\": {\n        ...\n      }\n    }\n  },\n  \"failures\": {}\n}\n\nLet’s try to better our search by introducing changes that move the evaluation score closer to the perfect 1.0."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#our-revised-search-model",
    "href": "posts/site-search-elasticsearch/index.html#our-revised-search-model",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Our revised search model",
    "text": "Our revised search model\nBefore start designing a new query template, we can take a step back and really think about how we should model the search engine. Below are the essentials:\n\nExact matching will always surface on top of not-exact ones like fuzzy matching;\nExact matches does not take into account field length or word/document frequencies. If two documents have the same exact match in a field, they should have the same keyword matching score;\nWithin the same level of matching (whether exact or fuzzy), while the initial keyword matching scores should be the same, they can be reranked by certain modifiers such as distance, popularity, … However, the modified scores should not make the final score to exceed the base score of the upper level, e.g. modifed fuzzy score should not be greater than exact base score. This is to ensure the essential #1.\n\nIf you watch football, this is similar to how the leagues such as Premiere League rank their teams. No matter how much more goals the team L has scored compared to team M’s or their head-to-head results, if team M has more points than team M has a higher ranking. The other measures are for tie-breaker only.\nThis understanding can be then transferred to how we use Elasticsearch to express our model.\nOne approach is to use dis_max query combined with constant_score query. The idea is to categorize each type of matching into different levels of score where one level will have twice the score of the below level. The documents fall into one level of matching (tie) will be reranked by modifiers but eventually the new scores will not exceed the upper base score. Here is the new query template:\n\n\nCode: New query template\n\nPUT _scripts/02-constant-score-search-template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"function_score\": {\n          \"query\": {\n            \"bool\": {\n              \"must\": [\n                {\n                  \"bool\": {\n                    \"should\": [\n                      {\n                        # `dis_max` query gets the max score of an array of clauses\n                        \"dis_max\": {\n                          \"queries\": [\n                            {\n                              # `constant_score` says that if matches, return a constant score\n                              \"constant_score\": {\n                                \"filter\": {\n                                  \"multi_match\" : {\n                                    \"query\": \"{{query_string}}\",\n                                    \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n                                  }\n                                },\n                                # This is the constant that is returned as score\n                                # Note that the exact number is chosen intentionally\n                                # Here the upper level will be twice the lower level\n                                # and we will restrict the modifiers to be only\n                                # able to boost by at most 100% the base score\n                                # so that the lower level can not exceed the upper\n                                \"boost\": 2\n                              }\n                            },\n                            {\n                              \"constant_score\": {\n                                \"filter\": {\n                                  \"multi_match\" : {\n                                    \"query\": \"{{query_string}}\",\n                                    \"fields\": [ \"restaurant_name\", \"cuisine\" ],\n                                    \"fuzziness\": \"AUTO\"\n                                  }\n                                },\n                                \"boost\": 1\n                              }\n                            }\n                          ]\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          },\n          \"functions\": [\n            # Design the modifiers to be multiplier of maximum 1.9999 the base score\n            {\n              \"weight\": 1\n            },\n            {\n              \"field_value_factor\": {\n                \"field\": \"rating\",\n                \"modifier\": \"ln\",\n                \"missing\": 1\n              },\n              \"weight\": 0.1\n            }\n          ],\n          \"score_mode\": \"sum\",\n          \"boost_mode\": \"multiply\"\n        }\n      }\n    },\n    \"params\": {\n      \"query_string\": \"My query string\"\n    }\n  }\n}\n\nWhen we re-run the evaluation, we can observe that the normalized DCG metric now has score equal to 1.0, denoting a perfect accuracy!"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#summary",
    "href": "posts/site-search-elasticsearch/index.html#summary",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Summary",
    "text": "Summary\nThis blog post focuses on putting you in the shoe of an Elasticsearch engineer who has to derive query templates that fit the needs of a site-search enginer. We have briefly coverred the following topics:\n\nKeyword matching with multiple fields\nUnderstanding default Elasticsearch scoring\nProblems with the default TFIDF\nBoosting search results by attributes\nFuzzy matching\nElasticsearch query templateEvaluation with Rank Evaluation API\nConstructing query with dis_maxand constant_score\n\nThough definitely not optimal, I hope that parts of the blog post help you come closer to utilize Elasticsearch to help solve your own problems.\nI also much appreciate any comments or feedbacks. If you want to discuss more, please comment on this post or open an issue in the Github repo.\nThanks all!"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#appendix",
    "href": "posts/site-search-elasticsearch/index.html#appendix",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Appendix",
    "text": "Appendix\n\nAppendix 1: Detailed breakdown of default TFIDF matching where length of the field value affect overall matching score\n\n\nResults\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 1.0470967,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        },\n        \"_explanation\": {\n          \"value\": 1.0470967,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.13353139,\n                  \"description\": \"weight(cuisine:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            },\n            {\n              \"value\": 1.0470967,\n              \"description\": \"sum of:\",\n              \"details\": [\n                # Matching score with \"vietnamese\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.52354836,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        {\n                          \"value\": 2.2,\n                          \"description\": \"boost\",\n                          \"details\": []\n                        },\n                        {\n                          \"value\": 0.47000363,\n                          \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\n                          ...\n                        },\n                        {\n                          \"value\": 0.50632906,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            {\n                              \"value\": 1,\n                              \"description\": \"freq, occurrences of term within document\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 1.2,\n                              \"description\": \"k1, term saturation parameter\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 0.75,\n                              \"description\": \"b, length normalization parameter\",\n                              \"details\": []\n                            },\n                            # Notice the length=2 here is in the denominator,\n                            # which means that the higher the length the less\n                            # the score\n                            {\n                              \"value\": 2,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 2.6666667,\n                              \"description\": \"avgdl, average length of field\",\n                              \"details\": []\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                # Matching score with \"pho\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:pho in 0) [PerFieldSimilarity], result of:\",\n                  # Details are exactly like above\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.8942772,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        },\n        \"_explanation\": {\n          \"value\": 0.8942772,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [...]\n            },\n            {\n              \"value\": 0.8942772,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:vietnamese in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.4471386,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        ...,\n                        {\n                          \"value\": 0.4324324,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            ...,\n                            # Here the length=3 (greater than length=2 of the\n                            # above restaurant)\n                            {\n                              \"value\": 3,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            ...\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:pho in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2023). A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration. dvquys.com. https://dvquys.com/posts/site-search-elasticsearch/."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html",
    "href": "posts/deploy-ml-gcp/index.html",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "",
    "text": "This article is originally published in VinID Engineering\nImagine a company named Rainbow imports boxes of flowers and need to classify them into species. For six months, they have some staff label the boxes manually. Now, they hire you to build a Machine Learning model to do the task.\nWith a small amount of labelled data as input and tons of experience working on Kaggle projects, you quickly develop a 95% accuracy using simple RandomForestClassifier from the popular scikit-learn library. Nice. Stakeholders approve and ask you when you could deploy that model to production.\nHmm, deploy a model from my laptop? …\nIn case you wonder, I hope this tutorial will help you understand one among some common and most simple approaches. The diagram below depicts how we will use Google Cloud Platform to do the job in a batch-processing manner.\nI choose the Iris data set as our input to help you see how our approach works with small-sized problems. All the codes are in this repo."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#introduction",
    "href": "posts/deploy-ml-gcp/index.html#introduction",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Introduction",
    "text": "Introduction\nLike many other self-taught data people, I am familiar with manipulating data and develop a model on my laptop.\nHowever, when you’re solving real-world problems, your duty does not stop after you deliver a presentation. You will have to think about how to bring that solution to the production environment.\nOver the last few months, I have tried to deploy multiple computing pipelines. They are different in their scopes and complexity, ranging from processing a dozen of MB to 400 GB data per run. In this article, I want to summarize and share what I learned."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#the-targeted-audience",
    "href": "posts/deploy-ml-gcp/index.html#the-targeted-audience",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "The targeted audience",
    "text": "The targeted audience\nThis post is for data analysts/scientists who want to deploy their local solution, especially those without a software engineering background.\nYou will need Cloud Dataproc to proceed. This product allows you to spin up a cluster of machines to run your computing job in a distributed manner. Please refer to this documentation if you don’t know what Dataproc is."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#agenda",
    "href": "posts/deploy-ml-gcp/index.html#agenda",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Agenda",
    "text": "Agenda\n\nDiscuss the approach\nStep-by-step instructions to create the infrastructure and run the pipeline\nExplain codebase\nIntroduce other extended components, including Big Data processing with Apache Spark, scheduler with Airflow, local development environment, unit testing\nSummary"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#approaches",
    "href": "posts/deploy-ml-gcp/index.html#approaches",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Approaches",
    "text": "Approaches\n\nAbout writing codes\nInstead of writing a long script to do everything, we break a pipeline into tasks and checkpoint interim data to disk. For example, after doing preprocess on train and test data, we dump both the data outputs and the transformer to Google Cloud Storage. We then load those objects as inputs for the next step.\nThis strategy has several purposes. First, for a long-running task, if a job fails at one of the last steps, we can re-run the pipeline from the nearest checkpoint rather than wasting time and resources restarting the whole pipeline. Second, it allows us to (1) debug more easily, (2) get alert when things break and (3) monitor interim outputs. Lastly, decoupled components can be understood more clearly, and easier to be replaced or extended later.\n\n\nAbout computing resources\nNormally for a small input size, we are fine with setting up a single virtual machine on the cloud. However, in some companies with mature cloud practice, the overhead of managing that VM is a type of cost that is difficult to justify. Especially when we have better options. For instance, Cloud Dataproc provides us with virtual machines that only live for the duration of one run, thereby free us from managing the machines. In this post, we explore Dataproc as our main engine for all the computing process."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#step-by-step-instructions",
    "href": "posts/deploy-ml-gcp/index.html#step-by-step-instructions",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\n\nCreate a GCP project and enable necessary components\n\n👉 Create a free GCP account with $300 credit by going to console.cloud.google.com. Beware that by following this tutorial, you might incur a cost of about $0.2–$0.5.\n\n\n\n\n\n👉 Click Billing at the left sidebar and initiate a billing account to be able to use the components used in this tutorial\n\n\n\n\n\n👉 Select Library, then search and enable the following API: Cloud Dataproc, Cloud Storage and Cloud Firestore.\n👉 Navigate to the Firestore either by scrolling the sidebar to the left or search from the top menu bar. When you arrive at the below screen, choose SELECT NATIVE MODE, then choose us-east1 as the location.\n\n\n\n\n\n\nEnvironment setup\n\nStep 1: Launch terminal window\n\n👉 At the home page of your GCP project, select the command button to the right of your menubar. The CloudShell window then appears as you can see below:\n\n\n\n\n\n👉 Launch Cloud Shell Editor:\n\n\n\n\n\n\nIt’s recommended to use Cloud Shell to follow this tutorial. However, if you’re using Linux and want to use terminal on your local machine, make sure you first install the Google Cloud SDK and firebase CLI.\n\n\nStep 2: Clone Github repo\n\n👉 In the Terminal window:\n\ngit clonehttps://github.com/dvquy13/gcp_ml_pipeline.git\ncd gcp_ml_pipeline\n\n👉 Select File then open the file gcp_ml_pipeline/configs/.project_env:\n\n\n\n\n\n👉 Replace the values enclosed by &lt;&gt;. For the GCP_PROJECT, you need to provide the id of your GCP project. For the remaining, feel free to choose some random names for the global variables that identify your resources. The final output looks like this:\n\nGCP_PROJECT='zinc-primer-230105'\nGCS_BUCKET=dvquys-tut-gcp-ml-pipeline\nDATA_LOCATION=us-east1\nBQ_DATASET=tut_iris\nBQ_ORG_TABLE=F_ORIGINAL\nCLUSTER_NAME=iris-pred\n\n👉 Grant execute permission to the folder scripts by running the command: chmod +x -R ./scripts. Then, run ./scripts/00_import_data_to_bigquery.sh. Link to the script.\n\n\n\nStep 3: Create Dataproc cluster and submit jobs\nWe use Makefile to orchestrate our actions. You can find it here.\n️Now, run the following commands in sequence:\n\nmake create-dataproc-cluster: This command creates a Dataproc cluster. The single-node flag indicates that this is a cluster containing only one machine. n1-standard-1 is the cheapest machine we can rent. To install Python packages, we supply the metadata and initialization-actions params.\nmake build: Package your code, including your source code and other 3rd party libraries that you can not pre-install when creating the cluster (PyYAML for example). To submit a job to the cluster, we will send these codes to those machines via the gcloud dataproc jobs submit pyspark command.\nmake submit-job ENV=dev MODULE=data_import TASK=query_train_pred: Submit job cloning input data for training and predicting. The submit-job make command allows you to use this interface to run on both local and development environments.\nmake submit-job ENV=dev MODULE=feature_engineer TASK=normalize: Prepare features. In this illustrative example, we choose to include only normalization in the pipeline. After learning the normalization parameters from the train data set, we save those configurations for later usage.\nmake submit-job ENV=dev MODULE=model TASK=fit: Train model. Here we build a pipeline consisting of 2 steps, Normalization and Logistic Regression. After that, we persist the fit pipeline.\nmake submit-job ENV=dev MODULE=predict TASK=batch_predict: Batch predict. This job demonstrates the process when you use your learned model to make predictions.\nmake submit-job ENV=dev MODULE=predict TASK=store_predictions: Store predictions. The reason we do not combine this with the above step is two-fold. First, writing to a database often takes time and requires several retries. Second, we write to a document database like Cloud Firestore because when other team uses, they typically retrieve one document per query. However, there are times when we want to inspect the whole batch of predictions (e.g. debugging, count number of documents scored more than 0.9). For this query pattern, we will better off using the persisted outputs from the previous step, stored as parquet files in Cloud Storage.\nmake delete-dataproc-cluster: Delete Dataproc cluster. After the process finishes, delete the cluster so no further cost incurs.\n\n\n\n\nSucceeded Dataproc jobs\nYou can see that your predictions are stored at Cloud Firestore by accessing its web console.\n\n\n\nFirestore populated with predictions\nAlong the way, you will see that the output data of each step is persisted in Cloud Storage. I use parquet rather than CSV as the serialization format because it can embed schema information (therefore you do not have to specify column types when reading) and reduce storage size. For more detail, please refer to this benchmark.\n\n\n\nClean up\n\n👉 Finally, when you’re done exploring the results, you can delete all resources by running these commands:\n\n./scripts/01_erase_resources.sh\n./scripts/02_disable_resources.sh\n./scripts/03_delete_project.sh"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#explain-codebase",
    "href": "posts/deploy-ml-gcp/index.html#explain-codebase",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Explain codebase",
    "text": "Explain codebase\nscripts/: This directory contains some initial scripts, which are the steps to help you set things up. In practice, I also favor using script rather than user interfaces such as web console because it is self-documented and easy for others to follow the exact steps.\nconfigs/: Store all the arguments that need to be set initially. .project_env is a file to store the global variables used to work with GCP. We also have the runtime.yaml, where we use Anchor, Alias and Extension in YAML to define runtime parameters for multiple environments. Both of these files serve as a centralized config store so that we can easily look up and make changes, instead of finding the configs scattered elsewhere in the code.\nMakefile: Originally Makefile is used to orchestrate the build process in C programming language. But it has done so well out of being just a shortcut so people start using it to facilitate ML model development. I have seen many tutorials using this tool, including the one that inspires me to design my Pyspark codebase.In this small project, we also use Makefile to save us a lot of time. As you can see above in Step 3, I put there our frequently used commands so that I can easily type make &lt;something&gt; to run a particular step.\niris_pred/: Source code.\nmain.py: is the interface to all tasks. This file parses the arguments to load config and get the job name, then call analyze function in entry_point.py from the appropriate module.\njobs/: contain tasks as modules. Inside jobs, we have one module corresponding to a step in our pipeline. All these modules expose an entry_point.py file where we unify the API to easily and consistently communicate with main.py.\n\n\nCode: train.py\n\n\n\niris_pred/jobs/model/train.py\n\nimport logging\nlogger = logging.getLogger(__name__)\nimport subprocess\n\nfrom shared.io_handler import IOHandler\nfrom jobs.feature_engineer.normalize import FeatureNormalizer\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nclass Trainer:\n    def __init__(self, params, load: bool):\n        self.params = params\n\n        self.io_handler = IOHandler(params)\n        self.interim_output_path, self.final_output_path = \\\n            self._get_fpath()\n\n        self.normalizer = None\n        self.learner = None\n        self.pipeline = None\n\n    def _get_fpath(self):\n        interim_output_path = \\\n            f'../{self.params.io.pipeline}/pipeline.joblib'\n        final_output_path = \\\n            f'{self.io_handler.fpath_dict.pipeline}/pipeline.joblib'\n        return interim_output_path, final_output_path\n\n    def _load_train_data(self):\n        X_train = self.io_handler.load('X_train')\n        y_train = self.io_handler.load('y_train')['species']\n        return X_train, y_train\n\n    def _load_transformer(self):\n        normalizer_wrapper = FeatureNormalizer(self.params, load=True)\n        self.normalizer = normalizer_wrapper.normalizer\n\n    def _initiate_learner(self):\n        self.learner = LogisticRegression()\n\n    def _make_pipeline(self):\n        self.pipeline = make_pipeline(\n            self.normalizer,\n            self.learner)\n\n    def _fit(self, X_train, y_train):\n        self.pipeline.fit(X_train, y_train)\n\n    def _persist_pipeline(self):\n        # Temporarily save model to disk\n        joblib.dump(self.pipeline, self.interim_output_path)\n\n        # Copy model to GCS\n        if self.params.env_name != 'local':\n            logger.info(f\"Persisting {self.final_output_path}...\")\n            subprocess.check_output([\n                'gsutil', '-m', 'cp', '-r',\n                self.interim_output_path,\n                self.final_output_path])\n\n    def run(self):\n        X_train, y_train = self._load_train_data()\n        self._load_transformer()\n        self._initiate_learner()\n        self._make_pipeline()\n        self._fit(X_train, y_train)\n        self._persist_pipeline()\n\n\nAs you can see in the snippet above, the class Trainer expose a function run. Each step in the process corresponds to a private function declared in the same class.\nshared/: functions and classes to be reused across modules\nIn io_handler.py, the class IOHandler applies the principle Composition Over Inheritance to ease the process of loading outputs from the previous step."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#further-discussion",
    "href": "posts/deploy-ml-gcp/index.html#further-discussion",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Further discussion",
    "text": "Further discussion\nTo completely build and operate a pipeline, there is still more to be considered.\n\nApache Spark for bigger data\nIn this tutorial, we rent one small machine from Dataproc and use pandas as our preprocessing engine, which perfectly handles the case of data fit into the memory of that machine. However, often data input in real-world situations will be much bigger, therefore require us to use a distributed computing framework for scalability. In that case, you can just switch to using Apache Spark. From version 1.3, Spark introduces its DataFrame API, which greatly bears resemblance to Pandas counterpart. After porting your code from Pandas to Spark, to be able to run jobs across multiple machines, you just need to create a bigger cluster with a master and multiple workers.\n\n\nApache Airflow for orchestration\nMost of the batch job is not ad hoc. If it is, we should not even think about putting effort to standardize the process in the first place. Apache Airflow can play the role of both a scheduler and a monitor. It keeps metadata of each run and can send you alerts when things fail.\n\n\n\n\nExample of Airflow DAG: Source\n\n\n\nAn alternative is Dataproc Workflows. This is a native solution offered by GCP, but I haven’t tried it myself so I will just leave the documentation here.\n\n\nLocal development\nBecause rarely our codes work the first time we write them, it’s very important to be able to quickly test without having to go through all the boilerplate steps from setting up variables to requesting cloud resources. My suggestion is that we should set up our local environment asap. We can install Apache Spark 2.4.3+ to act as our runner engine, and MongoDB to be our alternative for Cloud Firestore. Here in the code repo, you can still refer to some line containing what I call the “environment branching logic”, which enables you to switch between running the same code on both local and cloud environments.\n\n\nUnit testing\nMany people have already talked about unit testing, so I won’t go too detailed here. I also don’t do unit testing in this tutorial for the sake of simplicity. However, I strongly encourage you to add testing yourself. Whatever it takes, unit testing forces us to modularize our code and add a layer of alerting. This is very important because things in data science often break in silence."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#summary",
    "href": "posts/deploy-ml-gcp/index.html#summary",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Summary",
    "text": "Summary\nHere is a summary of what you have learned in this tutorial:\n\nHow to utilize different Google Cloud Platform components to build a batch job pipeline (whether it involves ML or not).\nA product named Google Cloud Dataproc, where you can both submit a light-weight job via single-node mode and easily scale to a cluster of computers.\nOne approach to structurize ML pipeline codebase: Link to the repo.\nSome convenient components in model development, e.g. Makefile, runtime config, parquet persistence. This mostly helps people with little or no software engineering background.\n\nAgain, one of my main goals in writing this article is to receive feedback from the community, so I can do my job better. Please feel free me leave me comments, and I hope you guys enjoy this tutorial."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#references",
    "href": "posts/deploy-ml-gcp/index.html#references",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "References",
    "text": "References\n\nRicky Kim. (Dec 2018). PySpark Sentiment Analysis on Google Dataproc. towardsdatascience.com. https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468.\nEvan Kampf. (Jan 2017). Best Practices Writing Production-Grade PySpark Jobs. developerzen.com. https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f.\nKing Chung Huang. (Oct 2017). Don’t Repeat Yourself with Anchors, Aliases and Extensions in Docker Compose Files. medium.com. https://medium.com/@kinghuang/docker-compose-anchors-aliases-extensions-a1e4105d70bd."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#credits",
    "href": "posts/deploy-ml-gcp/index.html#credits",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Credits",
    "text": "Credits\nKudos to Bido for reviewing my work; to anh Khanh, anh Linh, anh Tuan for providing me feedback.\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Feb 2020). From Model to Production: Deploying Your Machine Learning Solution on Google Cloud. dvquys.com. https://dvquys.com/posts/deploy-ml-gcp/."
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html",
    "href": "projects/reviews-parsing-mlsys/index.html",
    "title": "Let’s build an ML system",
    "section": "",
    "text": "This project focuses on applying engineering practices to build a Machine Learning System using Kubernetes and deploy on Google Cloud Platform."
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html#architecture",
    "href": "projects/reviews-parsing-mlsys/index.html#architecture",
    "title": "Let’s build an ML system",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html#demo",
    "href": "projects/reviews-parsing-mlsys/index.html#demo",
    "title": "Let’s build an ML system",
    "section": "Demo",
    "text": "Demo\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2024). Let’s build an ML system. dvquys.com. https://dvquys.com/projects/reviews-parsing-mlsys/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Building Real-time RecSys Chapter 3 - Negative Sampling\n\n\nWhy your model needs to see what users don’t want—and how to generate those examples\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 25, 2025\n\n\n10 min\n\n\n\n\n\n\n\nBuilding Real-time RecSys Chapter 1 - Introduction and Project Overview\n\n\nUnderstanding session-based recommendations and setting up your development environment\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n9 min\n\n\n\n\n\n\n\nBuilding Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering\n\n\nExploring Amazon product dataset and building features for sequence models\n\n\n\nrecsys\n\n\nrecsys-real-time-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n8 min\n\n\n\n\n\n\n\nBuilding a Conversational Assistant for Restaurant Discovery and Booking\n\n\nLessons from Creating a Chatbot Using RAG, LLMs, and Advanced Retrieval Techniques\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\nLet’s build an ML system\n\n\nEnd-to-end set up and deploy on Kubernetes\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\n\nA Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration\n\n\nA Practical Guide to Designing Query Templates, Boosting Relevance, and Addressing Site-Search Challenges\n\n\n\ntech\n\n\nsearch\n\n\n\n\n\n\nJun 1, 2023\n\n\n57 min\n\n\n\n\n\n\n\nFrom Model to Production: Deploying Your Machine Learning Solution on Google Cloud\n\n\nTransforming a Laptop-Trained ML Model into a Scalable Batch Processing Pipeline using GCP’s Dataproc\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nFeb 17, 2020\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html",
    "href": "projects/real-time-seq-recsys/c1/index.html",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "",
    "text": "Imagine you’re building an e-commerce site and your CEO asks: “Why don’t we show personalized recommendations that update in real-time as users browse?” You think to yourself—sounds simple enough, but then reality hits: How do you capture user intent from their session behavior? How do you serve recommendations fast enough to feel real-time? How do you handle the complexity of sequence modeling at scale?\nIf this sounds familiar, this tutorial series is for you.\nOver the next several chapters, I’ll walk you through building a complete real-time recommendation system that learns from user behavior sequences. We’ll go from raw data to a complete recommender system that personalizes recommendations as users interact with your platform.\n\n\n\n\n\n\nEnough Talking, Show Me the Code!\n\n\n\nIf you’re familiar with RecSys and just want to see the code, here it is: https://github.com/dvquy13/recsys-seq-model.\n\n\nFor those who aren’t well aware of what a recommender system is, here is a quick recap. Recommender systems are specialized tools designed to suggest items—whether products, content, or services—to users based on what we know about their preferences and behaviors. At their core, they comprise a set of technologies, algorithms, and processes that analyze user interaction signals (clicks, views, purchases, ratings, etc.) to predict what individual users might find relevant or engaging.\n\n\n\nFrom a business standpoint, the primary goals of a recommender system are to create a sense of “we understand you” for each user and to drive revenue through personalized cross-selling opportunities. By tailoring recommendations to each user, companies can boost engagement and average order value. From the user’s perspective, these systems help uncover content or products they might never have discovered on their own and save time by surfacing the most relevant options up front.\nIn practice, recommender systems power some of the world’s largest digital platforms. For example, Amazon’s “Frequently Bought Together” suggestions guide shoppers toward complementary products, Facebook uses recommendation algorithms to prioritize posts and ads in users’ feeds, and Netflix’s home screen is largely driven by personalized movie and show recommendations.\n\n\n\nThe impact of these systems is profound: According to this report from McKinsey, roughly 75 percent of Netflix viewing hours come from recommended titles, 35 percent of Amazon’s purchases are influenced by its recommendations, and Netflix alone saves around $1 billion per year through improved user retention and engagement. These metrics underscore why recommender systems have become a critical component of modern digital businesses."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#introduction",
    "href": "projects/real-time-seq-recsys/c1/index.html#introduction",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "",
    "text": "Imagine you’re building an e-commerce site and your CEO asks: “Why don’t we show personalized recommendations that update in real-time as users browse?” You think to yourself—sounds simple enough, but then reality hits: How do you capture user intent from their session behavior? How do you serve recommendations fast enough to feel real-time? How do you handle the complexity of sequence modeling at scale?\nIf this sounds familiar, this tutorial series is for you.\nOver the next several chapters, I’ll walk you through building a complete real-time recommendation system that learns from user behavior sequences. We’ll go from raw data to a complete recommender system that personalizes recommendations as users interact with your platform.\n\n\n\n\n\n\nEnough Talking, Show Me the Code!\n\n\n\nIf you’re familiar with RecSys and just want to see the code, here it is: https://github.com/dvquy13/recsys-seq-model.\n\n\nFor those who aren’t well aware of what a recommender system is, here is a quick recap. Recommender systems are specialized tools designed to suggest items—whether products, content, or services—to users based on what we know about their preferences and behaviors. At their core, they comprise a set of technologies, algorithms, and processes that analyze user interaction signals (clicks, views, purchases, ratings, etc.) to predict what individual users might find relevant or engaging.\n\n\n\nFrom a business standpoint, the primary goals of a recommender system are to create a sense of “we understand you” for each user and to drive revenue through personalized cross-selling opportunities. By tailoring recommendations to each user, companies can boost engagement and average order value. From the user’s perspective, these systems help uncover content or products they might never have discovered on their own and save time by surfacing the most relevant options up front.\nIn practice, recommender systems power some of the world’s largest digital platforms. For example, Amazon’s “Frequently Bought Together” suggestions guide shoppers toward complementary products, Facebook uses recommendation algorithms to prioritize posts and ads in users’ feeds, and Netflix’s home screen is largely driven by personalized movie and show recommendations.\n\n\n\nThe impact of these systems is profound: According to this report from McKinsey, roughly 75 percent of Netflix viewing hours come from recommended titles, 35 percent of Amazon’s purchases are influenced by its recommendations, and Netflix alone saves around $1 billion per year through improved user retention and engagement. These metrics underscore why recommender systems have become a critical component of modern digital businesses."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#what-youll-build",
    "href": "projects/real-time-seq-recsys/c1/index.html#what-youll-build",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "What You’ll Build",
    "text": "What You’ll Build\nBy the end of this series, you’ll have constructed an end-to-end recommendation system while know how to:\n\nDesign and Train Session-based RecSys models using PyTorch to personalizes recommendations in real-time based on users’ recent interaction sequences\nTrack experiments with MLflow for reproducible ML workflows\nServe predictions through a FastAPI service in conjunction with a vector database and caching\nPackage the project using Docker and modern ML infrastructure\nLaunch aninteractive demo with a frontend1\n\n\n\n\n\nThe Demo"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#target-audience",
    "href": "projects/real-time-seq-recsys/c1/index.html#target-audience",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Target Audience",
    "text": "Target Audience\nThis tutorial series is designed for:\n\nData Scientists looking beyond training models and into ML services\nML Engineers building scalable recommendation systems\n\nBackend Developers interested in ML/RecSys architecture\nAnyone curious about a modern and real-time end-to-end RecSys project"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#session-based-vs-traditional-recommendations",
    "href": "projects/real-time-seq-recsys/c1/index.html#session-based-vs-traditional-recommendations",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Session-Based vs Traditional Recommendations",
    "text": "Session-Based vs Traditional Recommendations\n\nTraditional Collaborative Filtering\nTraditional recommendation systems often rely on collaborative filtering, which uses historical user-item interaction matrices to find similar users or items. While effective, these approaches have limitations:\n\nStatic representations: User preferences are treated as fixed\nCold start problems: Difficulty with new users or items\nLimited temporal understanding: Doesn’t capture evolving user interests within a session\n\n\n\nSession-Based Recommendations\nSession-based recommendation systems address these limitations by:\n\nModeling sequences: Understanding the order and timing of user interactions\nCapturing short-term intent: Focusing on recent behaviors within a session\nHandling anonymity: Working even without persistent user identifiers\nReal-time adaptation: Continuously updating recommendations as users interact"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#why-sequence-modeling-matters",
    "href": "projects/real-time-seq-recsys/c1/index.html#why-sequence-modeling-matters",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Why Sequence Modeling Matters",
    "text": "Why Sequence Modeling Matters\nUser behavior is inherently sequential. Consider an e-commerce browsing session:\n\nUser searches for “wireless headphones”\nViews a Sony model\nChecks reviews for Audio-Technica alternatives\nCompares prices across brands\n\nEach step provides context for the next recommendation. Traditional systems might miss this sequential pattern, while sequence models capture the evolving intent throughout the session.\n\n\n\n\nData signals"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#real-time-architecture-a-simplified-payload-approach",
    "href": "projects/real-time-seq-recsys/c1/index.html#real-time-architecture-a-simplified-payload-approach",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Real-Time Architecture: A Simplified Payload Approach",
    "text": "Real-Time Architecture: A Simplified Payload Approach\nIn production, real-time recommendation systems typically leverage streaming architectures with event pipelines (Kafka, Kinesis, etc.) that process user interactions. The streaming system is normally built in a centralized manner by a platform team with the aim to provide real-time data processing capabilities to multiple downstream usages. This approach also ensures scalability and reliability.\nHowever, for the purpose of this tutorial, we’ll use the simplified request-payload approach to focus on the ML aspects rather than streaming infrastructure complexity.\nBasically we would include the real-time events in the payload of the requests coming in to RecSys APIs. In real-world, implementation can be done by frontend capturing user interactions locally (clicks, views, etc.) and includes recent session history in each API request.\nWhile this approach looks like a hacky-workaround ways, I have seen teams using this in production for early-stage real-time ML RecSys. In the end, the idea in and of itself has some merits such as minimal signal delay, users not needing to refresh to get updated recommendations and easier to implement and debug.\nIn the below screenshot you can see that in the POST request payload sending to a recommendation endpoint, we include the recent user interacted item IDs in item_seq_raw in the payload.\n\n\n\n\nExample request payload"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#our-technology-stack",
    "href": "projects/real-time-seq-recsys/c1/index.html#our-technology-stack",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Our Technology Stack",
    "text": "Our Technology Stack\nBelow is a list of the technology frameworks we use in this project. There is no need to understand all of them in detail, so do not worry too much if you are not familiar. However, I do encourage you to at least learn the basics of each of them. Wherever possible I would also explain the rationale and what roles they play in the system that we build together.\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nML Framework\nPyTorch\nDeep learning model training\n\n\nAPI Server\nFastAPI\nHigh-performance API endpoints\n\n\nExperiment Tracking\nMLflow\nModel versioning and experiments\n\n\nVector Database\nQdrant\nSimilarity search and retrieval\n\n\nCaching\nRedis\nReal-time data storage\n\n\nPackage Management\nuv\nFast Python dependency management\n\n\nContainerization\nDocker\nConsistent deployment environments"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#project-structure-overview",
    "href": "projects/real-time-seq-recsys/c1/index.html#project-structure-overview",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Project Structure Overview",
    "text": "Project Structure Overview\nThe codebase can be found here. Below is a quick glance into its structure.\nrecsys-seq-model/\n├── notebooks/           # Progressive tutorial notebooks\n│   ├── 000-prep-data.ipynb\n│   ├── 001-features.ipynb\n│   ├── 002-negative-sample.ipynb\n│   ├── 010-baseline-popular.ipynb\n│   ├── 011-sequence-modeling.ipynb\n│   ├── 020-ann-index.ipynb\n│   └── 021-redis-prep.ipynb\n├── src/                 # Core implementation modules\n│   ├── cfg.py          # Configuration management\n│   ├── dataset.py      # Data loading utilities  \n│   ├── id_mapper.py    # ID mapping functionality\n│   ├── negative_sampling.py  # Sampling strategies\n│   ├── sequence/       # Sequence model implementations\n│   ├── eval/          # Evaluation frameworks\n│   └── vectorstore.py # Qdrant integration\n├── api/                # FastAPI service\n│   ├── app.py         # Main application\n│   ├── services.py    # Business logic\n│   └── models.py      # Request/response schemas\n├── mlflow/            # MLflow configuration\n├── data/              # Dataset storage\n└── compose.yml        # Docker orchestration\n\nProgressive Notebook Approach\nNotice the numbered notebook sequence (000, 001, 002, etc.). This design ensures you build knowledge progressively:\n\n000-series: Data preparation and exploration\n010-series: Model training and evaluation\n\n020-series: Production deployment preparation\n\nEach notebook represents a complete milestone, allowing you to pause and resume at any point.\n\n\nA note on the commonly used commands\nYou may found the below commands useful, as I have myself running them regularly when developing the projects. Treating them as shortcuts, I have put them in the Makefile:\n# Environment Management\nmake ml-platform-up      # Start MLflow, Qdrant, Redis\nmake ml-platform-down    # Stop all services\nmake ml-platform-logs    # View service logs\n\n# Development\nuv sync --all-groups     # Install/update dependencies\nmake requirements-txt    # Export requirements for Docker\n\n# API Operations (covered in later chapters)\nmake api-up             # Start the FastAPI service\nmake api-test           # Run API tests\n\n# UI Demo (optional)\nmake ui-up              # Start the frontend demo"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#what-weve-covered",
    "href": "projects/real-time-seq-recsys/c1/index.html#what-weve-covered",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "What We’ve Covered",
    "text": "What We’ve Covered\nTo get a hands on the system that we will build together, you can clone the repo and follow the instructions in the README to get the project running.\nAfter that you should have:\n\nA clear picture of what session-based recommendations are and why they’re useful\nYour development environment ready to go with all services running\n\nFamiliarity with the project structure so you can navigate the codebase\n\nThe foundation knowledge to tackle the upcoming chapters"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#whats-next",
    "href": "projects/real-time-seq-recsys/c1/index.html#whats-next",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "What’s Next",
    "text": "What’s Next\nIn Chapter 2, we’ll get our hands dirty while setting up the input data in a format that’s designed for sequence modeling.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Building Real-time RecSys Chapter 1 - Introduction and Project Overview. dvquys.com. https://dvquys.com/projects/real-time-seq-recsys/c1/."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#footnotes",
    "href": "projects/real-time-seq-recsys/c1/index.html#footnotes",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThough not covered in this tutorial, there would be example UI code in the repo.↩︎"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html",
    "href": "projects/real-time-seq-recsys/c2/index.html",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "",
    "text": "In Chapter 1, we set up our development environment and explored the project architecture. Now it’s time to dive into the heart of any recommendation system: data. The quality of our recommendations depends entirely on how well we understand, process, and engineer features from our data.\nThis chapter focuses on the data pipeline from raw data to model-ready features. We’ll explore the Amazon product dataset, understand user-item interaction sequences, and build the feature engineering pipeline that will power our session-based recommender.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/000-prep-data.ipynb and notebooks/001-features.ipynb files in the project repository."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#introduction",
    "href": "projects/real-time-seq-recsys/c2/index.html#introduction",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "",
    "text": "In Chapter 1, we set up our development environment and explored the project architecture. Now it’s time to dive into the heart of any recommendation system: data. The quality of our recommendations depends entirely on how well we understand, process, and engineer features from our data.\nThis chapter focuses on the data pipeline from raw data to model-ready features. We’ll explore the Amazon product dataset, understand user-item interaction sequences, and build the feature engineering pipeline that will power our session-based recommender.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/000-prep-data.ipynb and notebooks/001-features.ipynb files in the project repository."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#dataset-overview-amazon-reviews-2023",
    "href": "projects/real-time-seq-recsys/c2/index.html#dataset-overview-amazon-reviews-2023",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Dataset Overview: Amazon Reviews 2023",
    "text": "Dataset Overview: Amazon Reviews 2023\nA good dataset for this kind of sequential recommendation project would typically have the following characteristics:\n\nTemporal richness: Each interaction has a timestamp, enabling sequence modeling\nScale: Millions of interactions across thousands of users and items\nReal-world patterns: Authentic user behavior with natural sparsity\n\nAmong the well-known public datasets, I chose the Amazon Reviews 2023 dataset from the McAuley Lab, specifically the “Books” category. Beyond the characteristics above, this dataset has other useful features—like reviews for both users and items—with plenty of observations to work with.\n\n\n\n\n\n\nThe main schema of the dataset is as simple as follows:\n# From cfg/common.yaml\ndata:\n1  user_col: \"user_id\"\n2  item_col: \"parent_asin\"\n3  rating_col: \"rating\"\n4  timestamp_col: \"timestamp\"\n\n1\n\nUnique user identifier\n\n2\n\nProduct identifier (in our case, the ASIN—Amazon Standard Identification Number)\n\n3\n\nUser rating (1-5 scale)\n\n4\n\nInteraction timestamp\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\n\nfrom src.cfg import ConfigLoader\n\n# Load configuration\ncfg = ConfigLoader(\"cfg/common.yaml\")\nThroughout this project, we store most configuration in cfg/common.yaml. This design does not only make it easier for notebooks and scripts to access their inputs and outputs but also help us try different configurations quickly."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#core-data-schema",
    "href": "projects/real-time-seq-recsys/c2/index.html#core-data-schema",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Core Data Schema",
    "text": "Core Data Schema\nOur processed dataset follows a clean schema optimized for recommendation systems:\n\n\n\nField\nDescription\nSource Column\n\n\n\n\nuser_id\nUnique user identifier\nuser_id\n\n\nparent_asin\nProduct identifier (our item_id)\nparent_asin\n\n\nrating\nUser rating (1-5 scale)\nrating\n\n\ntimestamp\nInteraction timestamp\ntimestamp\n\n\n\nThis maps to our configuration:\n# From cfg/common.yaml\ndata:\n  user_col: \"user_id\"\n  item_col: \"parent_asin\"  \n  rating_col: \"rating\"\n  timestamp_col: \"timestamp\""
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#data-preprocessing-pipeline",
    "href": "projects/real-time-seq-recsys/c2/index.html#data-preprocessing-pipeline",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Data Preprocessing Pipeline",
    "text": "Data Preprocessing Pipeline\nThe data flows through several key preprocessing steps:\n\n1. Data Sampling and Filtering\nAs ML is mostly about iterative development, the main rationale of sampling and filtering is to have a small but decent enough dataset to experiment different ideas fast. The more experiments we run, the more likely we would introduce improvements to our models.\nWhat does it mean for a decent RecSys dataset? One of the key criteria is sparsity—the ratio of observed interactions to all possible user-item pairs.\nTo understand why sparsity is problematic, consider the interaction matrix where each cell represents a potential user-item interaction:\n\nMatrix size: num_users × num_items (total possible interactions)\nActual interactions: Much smaller number of observed ratings/clicks\nSparsity: 1 - (actual_interactions / (num_users × num_items))\n\nThe sparsity problem gets quadratically worse as datasets grow:\n# Small dataset example\nusers = 1,000, items = 1,000 → possible interactions = 1M\nactual interactions = 50,000 → sparsity = 95%\n\n# Larger dataset \nusers = 10,000, items = 10,000 → possible interactions = 100M  \nactual interactions = 500,000 → sparsity = 99.5%\nEach new user adds an entire row of mostly empty interactions, and each new item adds an entire column of mostly empty interactions. Since users typically interact with only a tiny fraction of available items, the interaction matrix becomes increasingly sparse as the catalog grows.\nIn RecSys, the interaction distribution is typically long tail—a lot of noisy items or users have just a few interactions. So while randomly sampling may work just fine for many ML use cases, we need to apply it a bit more carefully here. Remember that ultimately we want to have a sample dataset where each user/item has at least X interactions.\nThe tricky part is that a basic random sampling of users and items would create sparsity cascade—a domino effect that breaks your dataset.\nHere’s what happens: You start with users and items that look fine on their own. User A has 10 interactions, Item X has 15 interactions. When you remove User B, you also lose all of User B’s interactions with Item X. Suddenly Item X only has 8 interactions. Oops, now it’s too sparse, so you remove it too. But removing Item X means User A loses some interactions and might become too sparse as well.\nIt’s like pulling threads from a sweater—everything starts unraveling.\nTo deal with this problem, we can take an iterative approach where we gradually drop random users from the dataset while keeping an eye on the conditions and our sampling target. The trade-off is that we would no longer have an exact fixed number of users and items in the dataset as we would like, but rather defining minimum acceptable thresholds, like below:\n# From cfg/common.yaml\nsample:\n  sample_users: 10000\n1  min_val_records: 5000\n  min_user_interactions: 5\n  min_item_interactions: 10\n\n1\n\nWe need to ensure sufficient validation data to evaluate our models.\n\n\nfrom src.sample import InteractionDataSampler\n\ndata_sampler = InteractionDataSampler(\n    user_col=cfg.data.user_col,\n    item_col=cfg.data.item_col,\n    sample_users=cfg.sample.sample_users,\n    min_val_records=cfg.sample.min_val_records,\n    random_seed=cfg.run.random_seed,\n    min_item_interactions=cfg.sample.min_item_interactions,\n    min_user_interactions=cfg.sample.min_user_interactions,\n    perc_users_removed_each_round=0.1,\n    debug=False,\n)\n...\n\nRandomly removing 2960 users - Round 18 started\nAfter randomly removing users - round 18: num_users=29,605\nNumber of users 29,605 are still greater than expected, keep removing...\n\nRandomly removing 2413 users - Round 19 started\nAfter randomly removing users - round 19: num_users=24,137\nNumber of users 24,137 are still greater than expected, keep removing...\nNumber of val_df records 4,282 are falling below expected threshold, stop and use `sample_df` as final output...\nlen(sample_users)=19,734 len(sample_items)=7,388\n\n\n\n\n\n\nInteractionDataSampler implementation\n\n\n\n\n\n\nIn the end, we would not have exact like 10000 users, but the numbers would be close to that. The distribution of the interaction is shown below:\n\n\n\n\n\n2. Train-test split\nWe need to have a validation test dataset to evaluate our models, so that we can have an estimate of how well it performs on unseen data.\nThere are two main types of train-test-split in RecSys:\n\n\n\nAFAIK, the last-one-out is often used more in academic settings while absolute timestamp is more common in industry. As whatever model you have deployed on production would be tested against data in the next day."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#id-mapping-from-strings-to-indices",
    "href": "projects/real-time-seq-recsys/c2/index.html#id-mapping-from-strings-to-indices",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "ID Mapping: From Strings to Indices",
    "text": "ID Mapping: From Strings to Indices\nAs discussed in Chapter 1, we’re modeling this problem with neural networks. Deep learning models work with numerical indices, not string IDs. So we use our IDMapper to provide deterministic mapping from user and item IDs to indices:\nfrom src.id_mapper import IDMapper\n\nuser_ids = train_df[cfg.data.user_col].values\nitem_ids = train_df[cfg.data.item_col].values\nunique_user_ids = list(set(user_ids))\nunique_item_ids = list(set(item_ids))\nidm = IDMapper()\nidm.fit(unique_user_ids, unique_item_ids)\n\n# Save for later use in model serving\nidm.save(\"data/idm.json\")"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#sequence-generation-the-heart-of-sequential-recsys",
    "href": "projects/real-time-seq-recsys/c2/index.html#sequence-generation-the-heart-of-sequential-recsys",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Sequence Generation: The Heart of Sequential RecSys",
    "text": "Sequence Generation: The Heart of Sequential RecSys\nThe key innovation in our approach is generating item sequences for each user interaction. This captures the temporal context that traditional collaborative filtering misses.\nfrom src.sequence.utils import generate_item_sequences\n\ndef generate_sequences(df, sequence_length=10):\n    \"\"\"\n    For each user interaction, create a sequence of their previous items.\n    This gives our model context about user behavior patterns.\n    \"\"\"\n    df_with_sequences = generate_item_sequences(\n        df=df,\n        user_col='user_indice',\n        item_col='item_indice', \n        timestamp_col='timestamp',\n        sequence_length=sequence_length,\n        padding=True,\n        padding_value=-1  # Special token for \"no previous item\"\n    )\n    \n    return df_with_sequences\n\nHow Sequence Generation Works\nLet’s trace through an example:\n# User interactions over time:\n# Time 1: User buys \"Python Programming\" (item_indice: 42)\n# Time 2: User buys \"Machine Learning\" (item_indice: 73) \n# Time 3: User buys \"Deep Learning\" (item_indice: 91)\n\n# Generated sequences:\n# Row 1: item_sequence = [-1, -1, ..., -1]           # No previous items\n# Row 2: item_sequence = [-1, -1, ..., 42]          # Previous: Python book\n# Row 3: item_sequence = [-1, -1, ..., 42, 73]      # Previous: Python, ML books\nThis gives our model the context it needs to understand user preferences and make sequential predictions.\n\n\nConfiguration for Sequences\n# From cfg/common.yaml\ntrain:\n  sequence:\n    sequence_length: 10  # Keep last 10 items as context\nThe sequence length is a key hyperparameter: - Longer sequences: More context but higher memory usage - Shorter sequences: Less context but faster training - Padding: Ensures all sequences have the same length for batch processing"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#dataset-classes-for-training",
    "href": "projects/real-time-seq-recsys/c2/index.html#dataset-classes-for-training",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Dataset Classes for Training",
    "text": "Dataset Classes for Training\nOur src/dataset.py provides PyTorch-compatible dataset classes:\nfrom src.dataset import UserItemBinaryDFDataset\n\n# Create dataset for binary feedback (clicked/not clicked)\ndataset = UserItemBinaryDFDataset(\n    df=df_with_sequences,\n    user_col='user_indice',\n    item_col='item_indice',\n    rating_col='rating',\n    timestamp_col='timestamp'\n)\n\n# Each sample contains:\nsample = dataset[0]\nprint(f\"User: {sample['user']}\")           # User index\nprint(f\"Item: {sample['item']}\")           # Item index  \nprint(f\"Rating: {sample['rating']}\")       # Binary label (0 or 1)\nprint(f\"Sequence: {sample['item_sequence']}\")  # Previous items\n\nWhy Binary Labels?\nWe convert ratings to binary labels because: - Implicit feedback: Focus on engagement (rating ≥ 1) vs no engagement - Training stability: Binary classification is more stable than rating prediction - Real-world relevance: Most recommendation systems optimize for engagement, not rating accuracy"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#data-flow-through-the-pipeline",
    "href": "projects/real-time-seq-recsys/c2/index.html#data-flow-through-the-pipeline",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Data Flow Through the Pipeline",
    "text": "Data Flow Through the Pipeline\nLet’s trace the complete data flow from our configuration:\n# 1. Raw data paths (from cfg/common.yaml)\ndata:\n  train_fp: \"$ROOT_DIR/data/train.parquet\"           # Processed train split\n  val_fp: \"$ROOT_DIR/data/val.parquet\"               # Processed validation split\n  train_features_fp: \"$ROOT_DIR/data/train_features.parquet\"  # With sequences\n  val_features_fp: \"$ROOT_DIR/data/val_features.parquet\"      # With sequences\n  idm_fp: \"$ROOT_DIR/data/idm.json\"                  # ID mappings\n\n# 2. Processing pipeline (notebooks/001-features.ipynb)\nraw_data → iterative_sampling → train_test_split → id_mapping → sequence_generation → save\n\nKey Processing Steps\n\nData Loading: Load preprocessed train/val splits\nID Mapping: Convert string IDs to numerical indices\n\nSequence Generation: Create item sequence features\nData Validation: Ensure data quality and consistency\nSerialization: Save processed features for model training"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#configuration-driven-development",
    "href": "projects/real-time-seq-recsys/c2/index.html#configuration-driven-development",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Configuration-Driven Development",
    "text": "Configuration-Driven Development\nEverything is controlled through our YAML configuration:\n# cfg/common.yaml - Data section\ndata:\n  hf_datasets:\n    name: \"McAuley-Lab/Amazon-Reviews-2023\"\n    mcauley_variant: \"Books\"\n  \n  # File paths with environment variable substitution\n  train_fp: \"$ROOT_DIR/data/train.parquet\"\n  val_fp: \"$ROOT_DIR/data/val.parquet\"\n  idm_fp: \"$ROOT_DIR/data/idm.json\"\n  \n  # Column mappings\n  user_col: \"user_id\"\n  item_col: \"parent_asin\" \n  rating_col: \"rating\"\n  timestamp_col: \"timestamp\"\n\n# Sampling configuration  \nsample:\n  sample_users: 10000\n  min_user_interactions: 5\n  min_item_interactions: 10\n\n# Sequence configuration\ntrain:\n  sequence:\n    sequence_length: 10\nThis approach provides: - Reproducibility: Same config = same results - Flexibility: Easy to experiment with different parameters - Environment handling: Automatic path resolution with $ROOT_DIR"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#data-quality-checks",
    "href": "projects/real-time-seq-recsys/c2/index.html#data-quality-checks",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Data Quality Checks",
    "text": "Data Quality Checks\nBefore moving to model training, we validate our processed data:\ndef validate_processed_data(df, idm):\n    \"\"\"Ensure our processed data is ready for model training.\"\"\"\n    \n    print(f\"Data shape: {df.shape}\")\n    print(f\"Users: {df['user_indice'].nunique()}\")\n    print(f\"Items: {df['item_indice'].nunique()}\")\n    \n    # Check for invalid indices\n    max_user_idx = len(idm.user_to_index) - 1\n    max_item_idx = len(idm.item_to_index) - 1\n    \n    assert df['user_indice'].max() &lt;= max_user_idx, \"Invalid user indices\"\n    assert df['item_indice'].max() &lt;= max_item_idx, \"Invalid item indices\"\n    \n    # Check sequence validity\n    if 'item_sequence' in df.columns:\n        seq_lengths = df['item_sequence'].apply(len)\n        print(f\"Sequence lengths: min={seq_lengths.min()}, max={seq_lengths.max()}\")\n    \n    print(\"✅ Data validation passed!\")"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#what-weve-accomplished",
    "href": "projects/real-time-seq-recsys/c2/index.html#what-weve-accomplished",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "What We’ve Accomplished",
    "text": "What We’ve Accomplished\nBy the end of this chapter, you have:\n✅ Understanding of recommendation data characteristics - Temporal patterns, sparsity, and user behavior\n✅ Processed dataset with proper ID mappings - Ready for deep learning models\n✅ Feature engineering pipeline for sequences - The core of session-based recommendations\n✅ Configuration-driven data processing - Reproducible and flexible pipeline\n✅ Data validation and quality checks - Ensuring reliability for model training\n\nKey Files Created\n\ndata/train_features.parquet - Training data with sequences\ndata/val_features.parquet - Validation data with sequences\n\ndata/idm.json - ID mappings for production deployment"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#whats-next",
    "href": "projects/real-time-seq-recsys/c2/index.html#whats-next",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "What’s Next",
    "text": "What’s Next\nIn Chapter 3, we’ll tackle the critical challenge of negative sampling. If our model only sees positive interactions (ratings), it can’t learn meaningful patterns for generalization. We need to generate negative examples so the model can distill the patterns by learning to distinguish between positive and negative interactions.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering. dvquys.com. https://dvquys.com/projects/real-time-seq-recsys/c2/."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#interactiondatasampler-implementation",
    "href": "projects/real-time-seq-recsys/c2/index.html#interactiondatasampler-implementation",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "InteractionDataSampler implementation",
    "text": "InteractionDataSampler implementation"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#sequence-generation",
    "href": "projects/real-time-seq-recsys/c2/index.html#sequence-generation",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Sequence Generation",
    "text": "Sequence Generation\nAt this point, you might wonder about the sequences I mentioned earlier. How does user-item rating data relate to sequences?\nTraditional collaborative filtering approaches like Matrix Factorization only use the user-item rating matrix. But one important signal gets left out: the timestamps.\nThe key insight is simple: when a user interacts with items over time, those interactions tell a story. We group each user’s interactions chronologically to create sequences of items, with the assumption that items a user engages with have meaningful relationships to each other.\nLet’s trace through an example to understand how sequence generation works:\n# User interactions over time:\n# Time 1: User buys \"Python Programming\" (item_indice: 42)\n# Time 2: User buys \"Machine Learning\" (item_indice: 73) \n# Time 3: User buys \"Deep Learning\" (item_indice: 91)\n\n# Generated sequences:\n# Row 1: item_sequence = [-1, -1, ..., -1]           # No previous items\n# Row 2: item_sequence = [-1, -1, ..., 42]          # Previous: Python book\n# Row 3: item_sequence = [-1, -1, ..., 42, 73]      # Previous: Python, ML books\n# Sample DataFrame\ndata = {\n    \"user_indices\": [0, 0, 1, 1, 1],\n    \"item_indices\": [0, 1, 2, 3, 4],\n    \"timestamp\": [0, 1, 2, 3, 4],\n    \"ratings\": [1, 4, 5, 3, 2],\n}\n\ndf = pd.DataFrame(data)\n\n# Generate the item sequences\ndf_with_sequences = generate_item_sequences(\n    df,\n    user_col=\"user_indices\",\n    item_col=\"item_indices\",\n    timestamp_col=\"timestamp\",\n    sequence_length=3,\n    padding=True,\n    padding_value=-1,\n)\n\nAs you can see, the above item_sequence column contains the sequence of items in chronological order which holds the context for the model to understand user preferences and make sequential predictions.\nIn this project, we use the user’s last 10 items as the sequence length, but this is configurable. The choice depends on experimentation, but generally there’s a trade-off: longer sequences provide more context but use more memory, while shorter sequences focus on recent items and process faster.\nDo take note that we need to pad sequences to the same length so we can batch process them in our PyTorch model.\n# From cfg/common.yaml\ntrain:\n  sequence:\n    sequence_length: 10  # Keep last 10 items as context"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#data-sampling-and-filtering",
    "href": "projects/real-time-seq-recsys/c2/index.html#data-sampling-and-filtering",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Data Sampling and Filtering",
    "text": "Data Sampling and Filtering\nAs ML is mostly about iterative development, the main rationale of sampling and filtering is to have a small but decent enough dataset to experiment different ideas fast. The more experiments we run, the more likely we would introduce improvements to our models.\nWhat does it mean for a decent RecSys dataset? One of the key criteria is sparsity—the ratio of observed interactions to all possible user-item pairs.\nTo understand why sparsity is problematic, consider the interaction matrix where each cell represents a potential user-item interaction:\n\nMatrix size: num_users × num_items (total possible interactions)\nActual interactions: Much smaller number of observed ratings/clicks\nSparsity: 1 - (actual_interactions / (num_users × num_items))\n\nThe sparsity problem gets quadratically worse as datasets grow:\n# Small dataset example\nusers = 1,000, items = 1,000 → possible interactions = 1M\nactual interactions = 50,000 → sparsity = 95%\n\n# Larger dataset \nusers = 10,000, items = 10,000 → possible interactions = 100M  \nactual interactions = 500,000 → sparsity = 99.5%\nEach new user adds an entire row of mostly empty interactions, and each new item adds an entire column of mostly empty interactions. Since users typically interact with only a tiny fraction of available items, the interaction matrix becomes increasingly sparse as the catalog grows.\nIn recommendation systems, interactions follow a long-tailed distribution—many users and items have only a few interactions. So while randomly sampling may work just fine for many ML use cases, we need to apply it a bit more carefully here. Ultimately we want to have a sample dataset where each user/item has at least X interactions.\nThe tricky part is that a basic random sampling of users and items would create sparsity cascade—a domino effect that breaks your dataset.\nHere’s what happens: You start with users and items that look fine on their own. User A has 10 interactions, Item X has 15 interactions. When you remove User B, you also lose all of User B’s interactions with Item X. Suddenly Item X only has 8 interactions. Oops, now it’s too sparse, so you remove it too. But removing Item X means User A loses some interactions and might become too sparse as well.\nIt’s like pulling threads from a sweater—everything starts unraveling.\nTo solve this, we take an iterative approach. We gradually drop random users from the dataset while watching our conditions and sampling targets. The trade-off is that while it’s hard to get an exact predefined number of users and items, we can control the minimum acceptable thresholds:\n# From cfg/common.yaml\nsample:\n  sample_users: 10000\n1  min_val_records: 5000\n  min_user_interactions: 5\n  min_item_interactions: 10\n\n1\n\nWe need to ensure sufficient validation data to evaluate our models.\n\n\nfrom src.sample import InteractionDataSampler\n\ndata_sampler = InteractionDataSampler(\n    user_col=cfg.data.user_col,\n    item_col=cfg.data.item_col,\n    sample_users=cfg.sample.sample_users,\n    min_val_records=cfg.sample.min_val_records,\n    random_seed=cfg.run.random_seed,\n    min_item_interactions=cfg.sample.min_item_interactions,\n    min_user_interactions=cfg.sample.min_user_interactions,\n    perc_users_removed_each_round=0.1,\n    debug=False,\n)\n...\n\nRandomly removing 2960 users - Round 18 started\nAfter randomly removing users - round 18: num_users=29,605\nNumber of users 29,605 are still greater than expected, keep removing...\n\nRandomly removing 2413 users - Round 19 started\nAfter randomly removing users - round 19: num_users=24,137\nNumber of users 24,137 are still greater than expected, keep removing...\nNumber of val_df records 4,282 are falling below expected threshold, stop and use `sample_df` as final output...\nlen(sample_users)=19,734 len(sample_items)=7,388\n\n\n\n\n\n\nInteractionDataSampler implementation\n\n\n\n\n\n\nIn the end, we won’t have exactly 10,000 users, but the numbers will be close. The distribution of interactions is shown below:"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#train-test-split",
    "href": "projects/real-time-seq-recsys/c2/index.html#train-test-split",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nAfter sampling our data, we need to split it into a train and validation dataset to evaluate our models. The validation dataset plays a critical role in providing an estimate of how well the model performs on unseen data.\nThere are two main types of train-test-split in RecSys:\n\n\n\nFrom what I’ve seen, last-one-out is used more in academic settings, while absolute timestamp is more common in industry. This makes sense from the perspective that any model deployed in production gets tested against future data."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#recap",
    "href": "projects/real-time-seq-recsys/c2/index.html#recap",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we covered:\n\nData Sampling and Filtering: We discussed problems with basic random sampling of dyadic data and introduced our iterative sampling approach\nID Mapping: We converted string IDs to numerical indices to work with PyTorch model\nSequence Generation: We created item sequence features based on user’s chronological interactions\n\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/000-prep-data.ipynb and notebooks/001-features.ipynb files in the project repository."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c1/index.html#recap",
    "href": "projects/real-time-seq-recsys/c1/index.html#recap",
    "title": "Building Real-time RecSys Chapter 1 - Introduction and Project Overview",
    "section": "Recap",
    "text": "Recap\nTo get a hands on the system that we will build together, you can clone the repo and follow the instructions in the README to get the project running.\nAfter that you should have:\n\nA clear picture of what session-based recommendations are and why they’re useful\nYour development environment ready to go with all services running\n\nFamiliarity with the project structure so you can navigate the codebase\n\nThe foundation knowledge to tackle the upcoming chapters"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c2/index.html#feature-engineering",
    "href": "projects/real-time-seq-recsys/c2/index.html#feature-engineering",
    "title": "Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nID Mapping: From Strings to Indices\nAs discussed in Chapter 1, we’re modeling this problem with neural networks. Deep learning models work with numerical indices, not string IDs. So we use our IDMapper to provide deterministic mapping from user and item IDs to indices:\nfrom src.id_mapper import IDMapper\n\nuser_ids = train_df[cfg.data.user_col].values\nitem_ids = train_df[cfg.data.item_col].values\nunique_user_ids = list(set(user_ids))\nunique_item_ids = list(set(item_ids))\nidm = IDMapper()\nidm.fit(unique_user_ids, unique_item_ids)\n\n# Save for later use in model serving\nidm.save(\"data/idm.json\")\n\n# Below is the example output of the indice mapping for user IDs:\ndisplay(idm.user_to_index)\n{\n  \"AE224PFXAEAT66IXX43GRJSWHXCA\": 0,\n  \"AE225Y3KDZ44DHLUKLE4RJ63HC5Q\": 1,\n  \"AE226YVDC3MAGJZMZ4IBGE7RFJSQ\": 2,\n  \"AE22EJZ4354VB7MN4IE2CDGHA2DQ\": 3,\n  \"AE22O3TURLPFCJKL7YCX5CPF22OA\": 4\n}\n\n\nSequence Generation\nAt this point, you might wonder about the sequences I mentioned earlier. How does user-item rating data relate to sequences?\nTraditional collaborative filtering approaches like Matrix Factorization only use the user-item rating matrix. But one important signal gets left out: the timestamps.\nThe key insight is simple: when a user interacts with items over time, those interactions tell a story. We group each user’s interactions chronologically to create sequences of items, with the assumption that items a user engages with have meaningful relationships to each other.\nLet’s trace through an example to understand how sequence generation works:\n# User interactions over time:\n# Time 1: User buys \"Python Programming\" (item_indice: 42)\n# Time 2: User buys \"Machine Learning\" (item_indice: 73) \n# Time 3: User buys \"Deep Learning\" (item_indice: 91)\n\n# Generated sequences:\n# Row 1: item_sequence = [-1, -1, ..., -1]           # No previous items\n# Row 2: item_sequence = [-1, -1, ..., 42]          # Previous: Python book\n# Row 3: item_sequence = [-1, -1, ..., 42, 73]      # Previous: Python, ML books\n# Sample DataFrame\ndata = {\n    \"user_indices\": [0, 0, 1, 1, 1],\n    \"item_indices\": [0, 1, 2, 3, 4],\n    \"timestamp\": [0, 1, 2, 3, 4],\n    \"ratings\": [1, 4, 5, 3, 2],\n}\n\ndf = pd.DataFrame(data)\n\n# Generate the item sequences\ndf_with_sequences = generate_item_sequences(\n    df,\n    user_col=\"user_indices\",\n    item_col=\"item_indices\",\n    timestamp_col=\"timestamp\",\n    sequence_length=3,\n    padding=True,\n    padding_value=-1,\n)\n\ndisplay(df_with_sequences)\n\nAs you can see, the above item_sequence column contains the sequence of items in chronological order which holds the context for the model to understand user preferences and make sequential predictions.\nIn this project, we use the user’s last 10 items as the sequence length, but this is configurable. The choice depends on experimentation, but generally there’s a trade-off: longer sequences provide more context but use more memory, while shorter sequences focus on recent items and process faster.\nDo take note that we need to pad sequences to the same length so we can batch process them in our PyTorch model.\n# From cfg/common.yaml\ntrain:\n  sequence:\n    sequence_length: 10  # Keep last 10 items as context"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html",
    "href": "projects/real-time-seq-recsys/c3/index.html",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "",
    "text": "In Chapter 2, we took a deep dive into our Amazon dataset and built a robust feature engineering pipeline. But we’re not quite yet ready to start training any personalization models—we still need to add the labels that will really empower our model to distinguish between what users liked and the rest.\nSince we already know which items users have purchased, one natural way to set up our recommendation challenge is as a sequence prediction problem. In short, we use a user’s past interactions to predict what they might try next—presenting the model with a sequence of actions and asking it to fill in the next likely item. Sounds familiar, doesn’t it?\nIn natural language processing, researchers have long figured out how to turn big piles of text into smart models. Take Word2Vec, for example—a simple yet brilliant model that learns word relationships by studying which words tend to appear together. Essentially, Word2Vec breaks a sentence into multiple input–output pairs: the surrounding context words serve as the input and a hidden target word is masked for prediction.\n\n\n\nSource: A simple Word2Vec tutorial\n\n\nAt each step, we pick a target word along with its neighboring context to form a positive training example. How about the negatives? That’s where negative sampling comes in. Rather than letting the model see only context words that belong together, we throw in some randomly selected words from outside the context. This mix challenges the model, teaching it to learn the patterns of words that appear in similar contexts.\nNow, let’s bring this idea to our book purchase dataset. When a user clicks, views, or buys an item, it sends a clear positive signal. But what about the millions of items they never interact with? Are these items unappealing, or might the user simply not have discovered them yet? Negative sampling bridges this gap, providing the model with examples of what a user isn’t likely to pick—balancing the training process by contrasting positives with smartly chosen negatives.\nI personally find this analogy helpful: Training a recommendation model without negative samples is like teaching someone to recognize cats by only showing them cat pictures—they might think everything is a cat.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/002-negative-sample.ipynb file and the src/negative_sampling.py module in the project repository."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#introduction",
    "href": "projects/real-time-seq-recsys/c3/index.html#introduction",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "",
    "text": "In Chapter 2, we took a deep dive into our Amazon dataset and built a robust feature engineering pipeline. But we’re not quite yet ready to start training any personalization models—we still need to add the labels that will really empower our model to distinguish between what users liked and the rest.\nSince we already know which items users have purchased, one natural way to set up our recommendation challenge is as a sequence prediction problem. In short, we use a user’s past interactions to predict what they might try next—presenting the model with a sequence of actions and asking it to fill in the next likely item. Sounds familiar, doesn’t it?\nIn natural language processing, researchers have long figured out how to turn big piles of text into smart models. Take Word2Vec, for example—a simple yet brilliant model that learns word relationships by studying which words tend to appear together. Essentially, Word2Vec breaks a sentence into multiple input–output pairs: the surrounding context words serve as the input and a hidden target word is masked for prediction.\n\n\n\nSource: A simple Word2Vec tutorial\n\n\nAt each step, we pick a target word along with its neighboring context to form a positive training example. How about the negatives? That’s where negative sampling comes in. Rather than letting the model see only context words that belong together, we throw in some randomly selected words from outside the context. This mix challenges the model, teaching it to learn the patterns of words that appear in similar contexts.\nNow, let’s bring this idea to our book purchase dataset. When a user clicks, views, or buys an item, it sends a clear positive signal. But what about the millions of items they never interact with? Are these items unappealing, or might the user simply not have discovered them yet? Negative sampling bridges this gap, providing the model with examples of what a user isn’t likely to pick—balancing the training process by contrasting positives with smartly chosen negatives.\nI personally find this analogy helpful: Training a recommendation model without negative samples is like teaching someone to recognize cats by only showing them cat pictures—they might think everything is a cat.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/002-negative-sample.ipynb file and the src/negative_sampling.py module in the project repository."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#the-implicit-feedback-problem",
    "href": "projects/real-time-seq-recsys/c3/index.html#the-implicit-feedback-problem",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "The Implicit Feedback Problem",
    "text": "The Implicit Feedback Problem\n\nUnderstanding the Challenge\nIn recommendation systems, we typically deal with implicit feedback—user actions like clicks, views, or purchases that indicate positive interest. Unlike explicit feedback (ratings, thumbs up/down), implicit feedback only tells us about positive interactions.\nConsider this user-item interaction matrix:\n        Item_A  Item_B  Item_C  Item_D  Item_E\nUser_1    1       0       1       0       0\nUser_2    0       1       0       0       1  \nUser_3    1       0       0       1       0\nThe 1s represent observed interactions (positive samples), but what do the 0s mean?\n\nTrue negatives: User genuinely dislikes the item\nMissing positives: User would like the item but hasn’t discovered it yet\nContextual negatives: User might like it in different circumstances\n\nThis ambiguity makes training challenging. Without negative samples, our model might simply learn to recommend the most popular items to everyone.\n\n\nWhy We Need Negative Samples\nTraining a recommendation model without negative samples is like teaching someone to recognize cats by only showing them cat pictures—they might think everything is a cat! Negative samples help our model learn:\n\nDiscrimination: What makes items relevant vs. irrelevant for specific users\nPersonalization: User-specific preferences rather than global popularity\nGeneralization: Patterns that transfer to unseen user-item pairs"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#negative-sampling-strategies",
    "href": "projects/real-time-seq-recsys/c3/index.html#negative-sampling-strategies",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Negative Sampling Strategies",
    "text": "Negative Sampling Strategies\n\nRandom Sampling\nLet’s start with the obvious approach: just pick random items the user hasn’t touched.\nThis is faily straightforward to code up. You grab all the items a user hasn’t interacted with, throw them in a hat, and pull out however many negatives you need. It’s fast, it’s unbiased, and it works.\nOne obvious but hard to mitigate downside is that some of those “random” negatives might actually be items the user would love if they discovered them. The other downside happens when the model may also be a little bit biased towards popular items, since they may see them appear in a lot of positive pairs.\n\n\nPopularity-Based Sampling\nTo deal with the above popularity biase, instead of picking negatives randomly, we sample them based on how popular they are in the dataset. This makes popular items get chosen as negatives more often.\nWhy does this work better? Compared to a random item, a popular one has a higher chance of being liked by any given user. So when we use popular items as negatives, we’re creating harder training examples. We’re forcing the model to learn why this user didn’t interact with this popular item, even though lots of other people did.\nThis approach has a nice side effect: it helps the model learn beyond simple popularity bias. Instead of just recommending whatever’s trending, it has to figure out what makes items relevant for specific users.\nThe downside? It’s more work to implement, and you need to keep track of popularity statistics. There is a catch that popular items aren’t always bad choices for a user. Sometimes they’re popular because they’re actually good. So as in reality a user may actually be interested in a particular popular item (and there’s a chance indeed if the item is a good one—hence popular), if we force a negative for the user-item pair in our training dataset, it can be a confusing example for the model.\n\n\nHard Negative Mining\nThis is the fancy approach. You need a model that’s already somewhat trained, and you use it to find items it thinks a user would like—but shouldn’t. These become your negative samples.\nIt’s like having a sparring partner who knows your weaknesses. The model keeps getting challenged by examples that are specifically designed to trip it up. As the model gets better, the negatives get harder, creating a virtuous cycle of improvement.\nLooks like an idea, right? But to be honest it’s not always worth the extra effort, especially from the start. You will need to train iteratively, which takes more time and compute. And there’s always the risk that you’re just teaching the model to overfit to its own biases.\nFor this project, we implement popularity-based negative sampling as it strikes a good balance between effectiveness and simplicity."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#our-implementation-popularity-aware-sampling",
    "href": "projects/real-time-seq-recsys/c3/index.html#our-implementation-popularity-aware-sampling",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Our Implementation: Popularity-Aware Sampling",
    "text": "Our Implementation: Popularity-Aware Sampling\nFor this project, we implement popularity-based negative sampling as it strikes a good balance between effectiveness and simplicity. Let’s examine our implementation:\nfrom src.negative_sampling import generate_negative_samples\n\ndef generate_negative_samples(\n    df,\n    user_col=\"user_indice\",\n    item_col=\"item_indice\", \n    label_col=\"rating\",\n    neg_label=0,\n    seed=None,\n):\n    \"\"\"\n    Generate negative samples for a user-item interaction DataFrame.\n    \n    The key insight: sample negative items proportional to their \n    popularity to create more challenging training scenarios.\n    \"\"\"\n\nStep 1: Calculate Item Popularity\n# Calculate item popularity based on interaction frequency\nitem_popularity = df[item_col].value_counts()\nitems = item_popularity.index.values\nall_items_set = set(items)\n\n# Convert to sampling probabilities\npopularity = item_popularity.values.astype(np.float64)\ntotal_popularity = popularity.sum()\nsampling_probs = popularity / total_popularity\nThis creates a probability distribution where more popular items have higher chances of being selected as negatives.\n\n\nStep 2: Identify Negative Candidates\n# Create user-item interaction mapping\nuser_item_dict = df.groupby(user_col)[item_col].apply(set).to_dict()\n\n# For each user, find items they haven't interacted with\nfor user, pos_items in user_item_dict.items():\n    negative_candidates = all_items_set - pos_items\nWe ensure we only sample from items the user hasn’t already interacted with.\n\n\nStep 3: Popularity-Weighted Sampling\n# Sample negatives proportional to popularity\ncandidate_indices = [item_to_index[item] for item in negative_candidates_list]\ncandidate_probs = sampling_probs[candidate_indices]\ncandidate_probs /= candidate_probs.sum()  # Normalize\n\nsampled_items = np.random.choice(\n    negative_candidates_list, \n    size=num_neg, \n    replace=False, \n    p=candidate_probs\n)\nThis ensures popular items are more likely to be selected as negatives, creating harder training examples."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#balancing-positive-and-negative-samples",
    "href": "projects/real-time-seq-recsys/c3/index.html#balancing-positive-and-negative-samples",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Balancing Positive and Negative Samples",
    "text": "Balancing Positive and Negative Samples\n\nThe 1:1 Ratio Strategy\nOur implementation generates equal numbers of positive and negative samples for each user:\nnum_pos = len(pos_items)  # Number of positive interactions\nnum_neg = min(num_pos, num_neg_candidates)  # Match positive count\nWhy 1:1 ratio?\n\nBalanced learning: Prevents model from being biased toward either class\nComputational efficiency: Reasonable dataset size for training\nEmpirical success: Works well in practice for many recommendation tasks\n\n\n\nAlternative Ratios\nDifferent ratios work better for different scenarios:\n\n1:4 (pos:neg): When you want to emphasize learning what users don’t want\n1:10: For very sparse datasets where negatives are abundant\nDynamic ratios: Adjust based on user activity levels"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#adding-temporal-context-to-negatives",
    "href": "projects/real-time-seq-recsys/c3/index.html#adding-temporal-context-to-negatives",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Adding Temporal Context to Negatives",
    "text": "Adding Temporal Context to Negatives\nA crucial aspect of our implementation is maintaining temporal consistency:\ndef add_features_to_neg_df(pos_df, neg_df, user_col, timestamp_col, feature_cols=[]):\n    \"\"\"\n    Add features from positive samples to negative samples DataFrame.\n    \n    Key insight: Negative samples should have realistic timestamps\n    that align with when the user was actually active.\n    \"\"\"\n    \n    # Create pseudo timestamps for negatives\n    neg_df = neg_df.assign(\n        timestamp_pseudo=lambda df: df.groupby(user_col).cumcount() + 1\n    )\n    \n    # Merge with corresponding positive interaction timestamps\n    neg_df = pd.merge(\n        neg_df,\n        pos_df.assign(\n            timestamp_pseudo=lambda df: df.groupby([user_col])[timestamp_col].rank(\n                method=\"first\"\n            )\n        )[[user_col, timestamp_col, \"timestamp_pseudo\", *feature_cols]],\n        how=\"left\",\n        on=[user_col, \"timestamp_pseudo\"],\n    ).drop(columns=[\"timestamp_pseudo\"])\n    \n    return neg_df\nThis ensures our negative samples have realistic timestamps, which is crucial for sequence modeling where temporal order matters."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#testing-our-implementation",
    "href": "projects/real-time-seq-recsys/c3/index.html#testing-our-implementation",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Testing Our Implementation",
    "text": "Testing Our Implementation\nLet’s see our negative sampling in action with a simple example:\n# Sample input: User-item interactions\ninteractions = [\n    (1, 101, 1, 1),  # user_id, item_id, rating, timestamp\n    (1, 102, 2, 2),\n    (1, 103, 3, 4),\n    (2, 101, 4, 1),\n    (2, 104, 5, 2),\n    (3, 105, 1, 1),\n    (3, 106, 2, 5),\n]\n\ndf = pd.DataFrame(\n    interactions,\n    columns=[\"user_indice\", \"item_indice\", \"rating\", \"timestamp\"]\n)\n\n# Generate negative samples\nneg_df = generate_negative_samples(df)\nneg_df = add_features_to_neg_df(df, neg_df, \"user_indice\", \"timestamp\")\nOutput:\n   user_indice  item_indice  rating  timestamp\n0            1          105       0          1\n1            1          104       0          2  \n2            1          106       0          4\n3            2          106       0          1\n4            2          105       0          2\n5            3          101       0          1\n6            3          102       0          5\nNotice how: - Each user gets negative samples for items they haven’t interacted with - The number of negatives matches the number of positives per user - Timestamps align with the user’s actual activity periods - Popular items (like 101, 102) appear more frequently as negatives"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#configuration-and-reproducibility",
    "href": "projects/real-time-seq-recsys/c3/index.html#configuration-and-reproducibility",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Configuration and Reproducibility",
    "text": "Configuration and Reproducibility\nOur implementation supports configurable parameters for experimentation:\n# From cfg/common.yaml\nsample:\n  sample_users: 10000\n  min_val_records: 5000\n  min_user_interactions: 5\n  min_item_interactions: 10\n\n# Negative sampling parameters (in code)\nneg_label: 0              # Label for negative samples\nseed: 41                  # For reproducible sampling\nsampling_strategy: \"popularity\"  # Could be \"random\", \"popularity\", etc."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#impact-on-model-training",
    "href": "projects/real-time-seq-recsys/c3/index.html#impact-on-model-training",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Impact on Model Training",
    "text": "Impact on Model Training\n\nBefore Negative Sampling\nTraining data: Only positive interactions\nModel learns: \"Recommend popular items to everyone\"\nResult: Poor personalization, popularity bias\n\n\nAfter Negative Sampling\nTraining data: Balanced positive/negative pairs\nModel learns: \"What makes items relevant for specific users\"\nResult: Better personalization, reduced popularity bias\nThe negative samples force our model to learn discriminative features that distinguish between items a user would and wouldn’t be interested in."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#running-the-complete-pipeline",
    "href": "projects/real-time-seq-recsys/c3/index.html#running-the-complete-pipeline",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Running the Complete Pipeline",
    "text": "Running the Complete Pipeline\nTo generate negative samples for your dataset:\n# 1. Ensure you've completed data preparation (Chapter 2)\n# 2. Run the negative sampling notebook\njupyter notebook notebooks/002-negative-sample.ipynb\n\n# Or use the module directly\npython -c \"\nfrom src.negative_sampling import generate_negative_samples\nfrom src.cfg import ConfigLoader\nimport pandas as pd\n\ncfg = ConfigLoader('cfg/common.yaml')\ntrain_df = pd.read_parquet(cfg.data.train_features_fp)\nneg_df = generate_negative_samples(train_df)\nprint(f'Generated {len(neg_df)} negative samples')\n\""
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#whats-next",
    "href": "projects/real-time-seq-recsys/c3/index.html#whats-next",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "What’s Next?",
    "text": "What’s Next?\nWith our balanced dataset of positive and negative samples, we’re ready to tackle model evaluation. In Chapter 4, we’ll set up our evaluation framework and experiment tracking with MLflow, establishing the foundation for systematic model development and comparison.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Building Real-time RecSys Chapter 3 - Negative Sampling. dvquys.com. https://dvquys.com/projects/real-time-seq-recsys/c3/."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#key-takeaways",
    "href": "projects/real-time-seq-recsys/c3/index.html#key-takeaways",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Key Takeaways",
    "text": "Key Takeaways\n\nImplicit feedback creates ambiguity - we need negative samples to train effective models\nPopularity-based sampling creates harder, more realistic training scenarios\nTemporal consistency in negative samples is crucial for sequence modeling\nBalanced datasets (1:1 positive:negative ratio) work well in practice\nReproducible sampling enables consistent experimentation and debugging\n\nThe negative sampling strategy you choose can significantly impact your model’s performance. Our popularity-aware approach helps create models that learn meaningful user preferences rather than just memorizing popularity patterns.\n\nNext: Chapter 4 - Evaluation Setup and MLflow Experiment Tracking"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#lets-implement",
    "href": "projects/real-time-seq-recsys/c3/index.html#lets-implement",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Let’s implement",
    "text": "Let’s implement\nThe full implementation is available here.\nFunction signature:\n\n\nsrc/negative_sampling.py\n\n\ndef generate_negative_samples(\n    df,\n    user_col=\"user_indice\",\n    item_col=\"item_indice\", \n    label_col=\"rating\",\n    neg_label=0,\n    seed=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate negative samples for a user-item interaction DataFrame.\n    \n    The key insight: sample negative items proportional to their \n    popularity to create more challenging training scenarios.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing user-item interactions.\n        user_col (str): Column name representing users.\n        item_col (str): Column name representing items.\n        label_col (str): Column name for the interaction label (e.g., rating).\n        neg_label (int): Label to assign to negative samples (default is 0).\n        seed (int, optional): Seed for random number generator to ensure reproducibility.\n\n    Returns:\n        pd.DataFrame: DataFrame containing generated negative samples.\n    \"\"\"\n\n\nStep 1: Calculate Item Popularity\n# Calculate item popularity based on interaction frequency\nitem_popularity = df[item_col].value_counts()\n\n# Convert to sampling probabilities to be used in the next step\npopularity = item_popularity.values.astype(np.float64)\ntotal_popularity = popularity.sum()\nsampling_probs = popularity / total_popularity\nThis creates a probability distribution where more popular items have higher chances of being selected as negatives.\n\n\nStep 2: Identify Negative Candidates\n# Create user-item interaction mapping\nuser_item_dict = df.groupby(user_col)[item_col].apply(set).to_dict()\n\n# For each user, find items they haven't interacted with\nfor user, pos_items in user_item_dict.items():\n    negative_candidates = all_items_set - pos_items\nWe ensure we only sample from items the user hasn’t already interacted with.\n\n\nStep 3: Popularity-Weighted Sampling\n# Create a mapping from item to index to quickly access item-related data.\nitems = item_popularity.index.values\nitem_to_index = {item: idx for idx, item in enumerate(items)}\n\n# Sample negatives proportional to popularity\ncandidate_indices = [item_to_index[item] for item in negative_candidates_list]\ncandidate_probs = sampling_probs[candidate_indices]\ncandidate_probs /= candidate_probs.sum()  # Normalize\n\nsampled_items = np.random.choice(\n    negative_candidates_list, \n    size=num_neg, \n    replace=False, \n    p=candidate_probs\n)\nThis ensures popular items are more likely to be selected as negatives, creating harder training examples.\nYou may find that in the implementation we choose to have the number of negative samples to be the same as the number of positive samples. This can be a starting point which helps us avoid dealing with imbalanced training data. However, feel free to experiment with different ratios (you need to update the implementation then).\nnum_pos = len(pos_items)  # Number of positive interactions\nnum_neg = min(num_pos, num_neg_candidates)  # Match positive count\nAs with other crucial components in our pipeline, let’s test the implementation with some mock data to ensure it works as expected."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#adding-features-to-the-generated-negative-samples",
    "href": "projects/real-time-seq-recsys/c3/index.html#adding-features-to-the-generated-negative-samples",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Adding Features to the generated negative samples",
    "text": "Adding Features to the generated negative samples\nAs you can see from the above output, we only have the item indice and the label. We also need to populate the new negative observations with the same features as the positive ones, i.e. the sequence of previously interacted items.\nSince this negative samples are generated from the same user, we can use the same features as the positive ones and use the timestamp of the corresponding positive interaction.\ndef add_features_to_neg_df(pos_df, neg_df, user_col, timestamp_col, feature_cols=[]):\n    \"\"\"\n    Add features from positive samples to negative samples DataFrame.\n    \n    Key insight: Negative samples should have realistic timestamps\n    that align with when the user was actually active.\n    \"\"\"\n    \n    # Create pseudo timestamps for negatives\n    # This timestamp pseudo column is used as join key to the positive samples, ensuring that each negative\n    # maps to one positive sample and get the positive's features.\n    neg_df = neg_df.assign(\n        timestamp_pseudo=lambda df: df.groupby(user_col).cumcount() + 1\n    )\n    \n    # Merge with corresponding positive interaction timestamps\n    neg_df = pd.merge(\n        neg_df,\n        pos_df.assign(\n            timestamp_pseudo=lambda df: df.groupby([user_col])[timestamp_col].rank(\n                method=\"first\"\n            )\n        )[[user_col, timestamp_col, \"timestamp_pseudo\", *feature_cols]],\n        how=\"left\",\n        on=[user_col, \"timestamp_pseudo\"],\n    ).drop(columns=[\"timestamp_pseudo\"])\n    \n    return neg_df"
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#concat-and-re-split-train-test",
    "href": "projects/real-time-seq-recsys/c3/index.html#concat-and-re-split-train-test",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Concat and re-split train-test",
    "text": "Concat and re-split train-test\nAfter adding the features to the negative samples, we can concatenate them with the positive samples and re-split the dataset based on the timestamp milestone we had earlier in our original train-test split."
  },
  {
    "objectID": "projects/real-time-seq-recsys/c3/index.html#recap",
    "href": "projects/real-time-seq-recsys/c3/index.html#recap",
    "title": "Building Real-time RecSys Chapter 3 - Negative Sampling",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we tackled the tricky problem of implicit feedback in recommendation systems. Here’s what we covered:\n\nWhy we need negative samples: Without them, our model would be like someone trying to recognize cats by only seeing cat pictures. We need examples of what users don’t want to create a balanced learning experience.\nThree approaches to negative sampling: We explored random sampling (simple but not very challenging), popularity-based sampling (our chosen approach that creates harder training scenarios), and hard negative mining (powerful but complex).\nOur popularity-based implementation: We built a system that samples negative items proportional to their popularity, forcing our model to learn why a user didn’t interact with popular items that others liked.\nKeeping things balanced: We generate equal numbers of positive and negative samples for each user (1:1 ratio) to avoid bias in either direction.\nAdding realistic features: We make sure our negative samples have proper timestamps and features that align with when users were actually active, maintaining temporal consistency for sequence modeling.\n\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/002-negative-sample.ipynb file and the src/negative_sampling.py module in the project repository."
  }
]