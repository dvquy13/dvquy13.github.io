[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I‚Äôm Quy‚Äîan Applied Data Scientist. I‚Äôm from Hanoi, Vietnam üáªüá≥. Currently living in Singapore üá∏üá¨.\nI blog about RecSys, ML, Productivity and other random things. You can find some of my side projects here.\nI used to teach a RecSys course for practitioners at FullStackDataScience."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Implement a RecSys, Chapter 7: Building the API Layer\n\n\nFrom trained models to production-ready APIs with BentoML and FastAPI\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nJun 2, 2025\n\n\n21 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 6: Preparing for Serving\n\n\nBuild the foundation that bridges model training to serving\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nJun 1, 2025\n\n\n17 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 5: Session-based Recommendation Model\n\n\nDesign and build sequence models that capture temporal patterns in user behavior\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 31, 2025\n\n\n28 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation\n\n\nEstablishing the evaluation foundation and implementing your baseline\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 30, 2025\n\n\n14 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 3: Negative Sampling\n\n\nWhy your model needs to see what users don‚Äôt want‚Äîand how to generate those examples\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 26, 2025\n\n\n10 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 2: Understanding the Data and Feature Engineering\n\n\nExploring Amazon product dataset and building features for sequence models\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 25, 2025\n\n\n8 min\n\n\n\n\n\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 1: Introduction and Project Overview\n\n\nUnderstanding session-based recommendations and setting up your development environment\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n11 min\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a Conversational Assistant for Restaurant Discovery and Booking\n\n\nLessons from Creating a Chatbot Using RAG, LLMs, and Advanced Retrieval Techniques\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs build an ML system\n\n\nEnd-to-end set up and deploy on Kubernetes\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html",
    "href": "projects/implement-recsys/c2/index.html",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "",
    "text": "This is the second chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#introduction",
    "href": "projects/implement-recsys/c2/index.html#introduction",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 1, we set up our development environment and explored the project architecture. Now it‚Äôs time to dive into the heart of any recommendation system: data. The quality of our recommendations depends entirely on how well we understand, process, and engineer features from our data.\nThis chapter focuses on the data pipeline from raw data to model-ready features. We‚Äôll explore the Amazon product dataset, understand user-item interaction sequences, and build the feature engineering pipeline that will power our session-based recommender.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/000-prep-data.ipynb and notebooks/001-features.ipynb files in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#dataset-overview-amazon-reviews-2023",
    "href": "projects/implement-recsys/c2/index.html#dataset-overview-amazon-reviews-2023",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Dataset Overview: Amazon Reviews 2023",
    "text": "Dataset Overview: Amazon Reviews 2023\nA good dataset for this kind of sequential recommendation project would typically have the following characteristics:\n\nTemporal richness: Each interaction has a timestamp, enabling sequence modeling\nScale: Millions of interactions across thousands of users and items\nReal-world patterns: Authentic user behavior with natural sparsity\n\nAmong the well-known public datasets, I chose the Amazon Reviews 2023 dataset from the McAuley Lab, specifically the ‚ÄúBooks‚Äù category. Beyond the characteristics above, this dataset has other useful features‚Äîlike reviews for both users and items‚Äîwith plenty of observations to work with.\n\n\n\n\n\n\nThe main schema of the dataset is as simple as follows:\n# From cfg/common.yaml\ndata:\n1  user_col: \"user_id\"\n2  item_col: \"parent_asin\"\n3  rating_col: \"rating\"\n4  timestamp_col: \"timestamp\"\n\n1\n\nUnique user identifier\n\n2\n\nProduct identifier (in our case, the ASIN‚ÄîAmazon Standard Identification Number)\n\n3\n\nUser rating (1-5 scale)\n\n4\n\nInteraction timestamp\n\n\n\n\n\n\n\n\nConfiguration Management\n\n\n\n\n\nfrom src.cfg import ConfigLoader\n\n# Load configuration\ncfg = ConfigLoader(\"cfg/common.yaml\")\nThroughout this project, we store most configuration in cfg/common.yaml. This design does not only make it easier for notebooks and scripts to access their inputs and outputs but also help us try different configurations quickly."
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#data-sampling-and-filtering",
    "href": "projects/implement-recsys/c2/index.html#data-sampling-and-filtering",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Data Sampling and Filtering",
    "text": "Data Sampling and Filtering\nAs ML is mostly about iterative development, the main rationale of sampling and filtering is to have a small but decent enough dataset to experiment different ideas fast. The more experiments we run, the more likely we would introduce improvements to our models.\nWhat does it mean for a decent RecSys dataset? One of the key criteria is sparsity‚Äîthe ratio of observed interactions to all possible user-item pairs.\nTo understand why sparsity is problematic, consider the interaction matrix where each cell represents a potential user-item interaction:\n\nMatrix size: num_users √ó num_items (total possible interactions)\nActual interactions: Much smaller number of observed ratings/clicks\nSparsity: 1 - (actual_interactions / (num_users √ó num_items))\n\nThe sparsity problem gets quadratically worse as datasets grow:\n# Small dataset example\nusers = 1,000, items = 1,000 ‚Üí possible interactions = 1M\nactual interactions = 50,000 ‚Üí sparsity = 95%\n\n# Larger dataset \nusers = 10,000, items = 10,000 ‚Üí possible interactions = 100M  \nactual interactions = 500,000 ‚Üí sparsity = 99.5%\nEach new user adds an entire row of mostly empty interactions, and each new item adds an entire column of mostly empty interactions. Since users typically interact with only a tiny fraction of available items, the interaction matrix becomes increasingly sparse as the catalog grows.\nIn recommendation systems, interactions follow a long-tailed distribution‚Äîmany users and items have only a few interactions. So while randomly sampling may work just fine for many ML use cases, we need to apply it a bit more carefully here. Ultimately we want to have a sample dataset where each user/item has at least X interactions.\nThe tricky part is that a basic random sampling of users and items would create sparsity cascade‚Äîa domino effect that breaks your dataset.\nHere‚Äôs what happens: You start with users and items that look fine on their own. User A has 10 interactions, Item X has 15 interactions. When you remove User B, you also lose all of User B‚Äôs interactions with Item X. Suddenly Item X only has 8 interactions. Oops, now it‚Äôs too sparse, so you remove it too. But removing Item X means User A loses some interactions and might become too sparse as well.\nIt‚Äôs like pulling threads from a sweater‚Äîeverything starts unraveling.\nTo solve this, we take an iterative approach. We gradually drop random users from the dataset while watching our conditions and sampling targets. The trade-off is that while it‚Äôs hard to get an exact predefined number of users and items, we can control the minimum acceptable thresholds:\n# From cfg/common.yaml\nsample:\n  sample_users: 10000\n1  min_val_records: 5000\n  min_user_interactions: 5\n  min_item_interactions: 10\n\n1\n\nWe need to ensure sufficient validation data to evaluate our models.\n\n\nfrom src.sample import InteractionDataSampler\n\ndata_sampler = InteractionDataSampler(\n    user_col=cfg.data.user_col,\n    item_col=cfg.data.item_col,\n    sample_users=cfg.sample.sample_users,\n    min_val_records=cfg.sample.min_val_records,\n    random_seed=cfg.run.random_seed,\n    min_item_interactions=cfg.sample.min_item_interactions,\n    min_user_interactions=cfg.sample.min_user_interactions,\n    perc_users_removed_each_round=0.1,\n    debug=False,\n)\n...\n\nRandomly removing 2960 users - Round 18 started\nAfter randomly removing users - round 18: num_users=29,605\nNumber of users 29,605 are still greater than expected, keep removing...\n\nRandomly removing 2413 users - Round 19 started\nAfter randomly removing users - round 19: num_users=24,137\nNumber of users 24,137 are still greater than expected, keep removing...\nNumber of val_df records 4,282 are falling below expected threshold, stop and use `sample_df` as final output...\nlen(sample_users)=19,734 len(sample_items)=7,388\n\n\n\n\n\n\nInteractionDataSampler implementation\n\n\n\n\n\n\nIn the end, we won‚Äôt have exactly 10,000 users, but the numbers will be close. The distribution of interactions is shown below:"
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#train-test-split",
    "href": "projects/implement-recsys/c2/index.html#train-test-split",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Train-Test Split",
    "text": "Train-Test Split\nAfter sampling our data, we need to split it into a train and validation dataset to evaluate our models. The validation dataset plays a critical role in providing an estimate of how well the model performs on unseen data.\nThere are two main types of train-test-split in RecSys:\n\n\n\nFrom what I‚Äôve seen, last-one-out is used more in academic settings, while absolute timestamp is more common in industry. This makes sense from the perspective that any model deployed in production gets tested against future data."
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#feature-engineering",
    "href": "projects/implement-recsys/c2/index.html#feature-engineering",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Feature Engineering",
    "text": "Feature Engineering\n\nID Mapping: From Strings to Indices\nAs discussed in Chapter 1, we‚Äôre modeling this problem with neural networks. Deep learning models work with numerical indices, not string IDs. So we use our IDMapper to provide deterministic mapping from user and item IDs to indices:\nfrom src.id_mapper import IDMapper\n\nuser_ids = train_df[cfg.data.user_col].values\nitem_ids = train_df[cfg.data.item_col].values\nunique_user_ids = list(set(user_ids))\nunique_item_ids = list(set(item_ids))\nidm = IDMapper()\nidm.fit(unique_user_ids, unique_item_ids)\n\n# Save for later use in model serving\nidm.save(\"data/idm.json\")\n\n# Below is the example output of the indice mapping for user IDs:\ndisplay(idm.user_to_index)\n{\n  \"AE224PFXAEAT66IXX43GRJSWHXCA\": 0,\n  \"AE225Y3KDZ44DHLUKLE4RJ63HC5Q\": 1,\n  \"AE226YVDC3MAGJZMZ4IBGE7RFJSQ\": 2,\n  \"AE22EJZ4354VB7MN4IE2CDGHA2DQ\": 3,\n  \"AE22O3TURLPFCJKL7YCX5CPF22OA\": 4\n}\n\n\nSequence Generation\nAt this point, you might wonder about the sequences I mentioned earlier. How does user-item rating data relate to sequences?\nTraditional collaborative filtering approaches like Matrix Factorization only use the user-item rating matrix. But one important signal gets left out: the timestamps.\nThe key insight is simple: when a user interacts with items over time, those interactions tell a story. We group each user‚Äôs interactions chronologically to create sequences of items, with the assumption that items a user engages with have meaningful relationships to each other.\nLet‚Äôs trace through an example to understand how sequence generation works:\n# User interactions over time:\n# Time 1: User buys \"Python Programming\" (item_indice: 42)\n# Time 2: User buys \"Machine Learning\" (item_indice: 73) \n# Time 3: User buys \"Deep Learning\" (item_indice: 91)\n\n# Generated sequences:\n# Row 1: item_sequence = [-1, -1, ..., -1]           # No previous items\n# Row 2: item_sequence = [-1, -1, ..., 42]          # Previous: Python book\n# Row 3: item_sequence = [-1, -1, ..., 42, 73]      # Previous: Python, ML books\n# Sample DataFrame\ndata = {\n    \"user_indices\": [0, 0, 1, 1, 1],\n    \"item_indices\": [0, 1, 2, 3, 4],\n    \"timestamp\": [0, 1, 2, 3, 4],\n    \"ratings\": [1, 4, 5, 3, 2],\n}\n\ndf = pd.DataFrame(data)\n\n# Generate the item sequences\ndf_with_sequences = generate_item_sequences(\n    df,\n    user_col=\"user_indices\",\n    item_col=\"item_indices\",\n    timestamp_col=\"timestamp\",\n    sequence_length=3,\n    padding=True,\n    padding_value=-1,\n)\n\ndisplay(df_with_sequences)\n\nAs you can see, the above item_sequence column contains the sequence of items in chronological order which holds the context for the model to understand user preferences and make sequential predictions.\nIn this project, we use the user‚Äôs last 10 items as the sequence length, but this is configurable. The choice depends on experimentation, but generally there‚Äôs a trade-off: longer sequences provide more context but use more memory, while shorter sequences focus on recent items and process faster.\nDo take note that we need to pad sequences to the same length so we can batch process them in our PyTorch model.\n# From cfg/common.yaml\ntrain:\n  sequence:\n    sequence_length: 10  # Keep last 10 items as context"
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#recap",
    "href": "projects/implement-recsys/c2/index.html#recap",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we covered:\n\nData Sampling and Filtering: We discussed problems with basic random sampling of dyadic data and introduced our iterative sampling approach\nID Mapping: We converted string IDs to numerical indices to work with PyTorch model\nSequence Generation: We created item sequence features based on user‚Äôs chronological interactions\n\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/000-prep-data.ipynb and notebooks/001-features.ipynb files in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c2/index.html#whats-next",
    "href": "projects/implement-recsys/c2/index.html#whats-next",
    "title": "Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering",
    "section": "What‚Äôs Next",
    "text": "What‚Äôs Next\nIn Chapter 3, we‚Äôll tackle the critical challenge of negative sampling. If our model only sees positive interactions (ratings), it can‚Äôt learn meaningful patterns for generalization. We need to generate negative examples so the model can distill the patterns by learning to distinguish between positive and negative interactions.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 2: Understanding the Data and Feature Engineering. dvquys.com. https://dvquys.com/projects/implement-recsys/c2/."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html",
    "href": "projects/implement-recsys/c4/index.html",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "",
    "text": "This is the fourth chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#introduction",
    "href": "projects/implement-recsys/c4/index.html#introduction",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 3, inspired by how Word2Vec cleverly samples out-of-context words to train its language model, we implemented our own popularity-based sampling strategy to generate negative samples for our training dataset. Now we‚Äôre ready to tackle one of the most critical aspects of any ML project: evaluation.\nWe will discuss building our evaluation framework, setting up MLflow for experiment tracking, and implementing a popularity-based baseline model to illustrate the end-to-end process. A solid measuring foundation provides the groundwork for systematic model development‚Äîyou‚Äôll use these tools throughout all your different attempts at improving the model performance.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/010-baseline-popular.ipynb file and the src/eval/ directory in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#the-challenges-of-evaluation-in-recommendation-systems",
    "href": "projects/implement-recsys/c4/index.html#the-challenges-of-evaluation-in-recommendation-systems",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "The Challenges of Evaluation in Recommendation Systems",
    "text": "The Challenges of Evaluation in Recommendation Systems\nRecommendation systems are notoriously tricky to evaluate. Unlike classification problems where accuracy is straightforward, the most significant challenge originates from the fact that RecSys has its root from information retrieval‚Äîwhere we essentially try to rank items. The issue comes when your new model produces a new ranking, but we don‚Äôt really know how users would react to this alternative ranking unless we deploy it in production. This makes offline evaluation feel quite limited for gauging real-world performance.\nYou may ask: Why don‚Äôt just run online experiment for any idea we have? Because they are typically expensive and slow. Methods like A/B tests in general require real users, real traffic, and real time to see if your changes work. So to iterate quickly on your backlog of a hundred different ideas, we don‚Äôt really have any other choices rather than resorting to offline evaluation as our development tool. That assumes, of course, that getting scores on old data actually points you towards better real-world models.\nIf you ask me how I navigate through all these sorts of uncertainties, here‚Äôs my take: if your offline metrics show clear improvements, that‚Äôs a good sign to move forward. If they show no change but you have strong reasons to believe your improvements are effective, don‚Äôt let that stop you from running an online experiment. Sometimes the best ideas don‚Äôt show up in offline metrics until they meet real users. Just make sure you can roll back quickly if you see significant drops in the early days.\nBeyond this counterfactual challenge, recommendation systems must also struggle with implicit feedback where users rarely provide explicit ratings, ranking quality where the order of recommendations matters as much as the items themselves, personalization where a perfect recommendation for one user might be terrible for another, and temporal dynamics where user preferences evolve over time. In short, recommendations involve nuanced concepts of relevance, satisfaction, and utility that resist simple measurement.\nTo address these challenges, we need a comprehensive evaluation framework that can measure multiple aspects of recommendation quality. For the scope of this tutorial, we would be focusing on measuring both the ranking quality and the classification quality of the recommendations, while leaving the problem of counterfactual evaluation for a future post perhaps1.\nAll in all, we‚Äôll focus on offline evaluation because it lets us iterate fast and learn the fundamentals. But important message worths repeating‚Äîthe real test happens when your recommendations meet actual users."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#evaluation-metrics-for-recommendation-systems",
    "href": "projects/implement-recsys/c4/index.html#evaluation-metrics-for-recommendation-systems",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "Evaluation Metrics for Recommendation Systems",
    "text": "Evaluation Metrics for Recommendation Systems\nThere are plenty of great posts diving into the details of these metrics so I would just briefly mention them here. Our evaluation framework implements several key metrics:\n\nRanking Metrics\n\nPrecision@K: What fraction of top-K recommendations are relevant?\nRecall@K: What fraction of relevant items appear in top-K recommendations?\nNDCG@K: Normalized Discounted Cumulative Gain‚Äîrewards relevant items appearing higher in the ranking\n\n\n\nDiversity Metrics\n\nPersonalization: Measures how different recommendations are across users (higher is better)\n\n\n\nClassification Metrics\n\nROC-AUC: Area under the ROC curve for binary relevance prediction\nPrecision-Recall curves: Trade-offs between precision and recall at different thresholds\n\nWhy classification metrics? Isn‚Äôt this a ranking problem? Well, the answer is yes and no. Essentially the choice of modeling approach is up to us, but there is one valid approach in which we would formulate the problem as predicting whether a user would interact with an item or not. As the output of this model would normally be a score between 0 and 1, we can then use the ROC-AUC to evaluate how well a model is able to rank the relevant items higher that the irrelevant ones, which is a form of ranking quality in and of itself. The other main benefit for using ROC-AUC is that it‚Äôs also a built-in metric for almost all ML frameworks, which makes it easy to measure without the need to implement it from scratch (lazy person saying).\nIf you wish to learn more, I suggest checking out this post by Amit Chaudhary and the Evidently documentation on Ranking metrics."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#the-baseline-recommender-system",
    "href": "projects/implement-recsys/c4/index.html#the-baseline-recommender-system",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "The baseline recommender system",
    "text": "The baseline recommender system\nTo demonstrate how we evaluate a model, let‚Äôs first build a baseline one for easier illustration. This will serve as a sanity check for our evaluation framework and provide a performance benchmark for more complex models to beat.\n\n\n\n\n\n\nStart MLflow\n\n\n\nIf you have followed the README instructions (see Chapter 1), you should have already started MLflow. If not, you can start it by running the following command:\nmake ml-platform-up && make ml-platform-logs\n\n\nIn the world of RecSys, the most popular base line is ‚Äúpopular‚Äù recommender (pun intended). It‚Äôs simple, fast, easy to implement, easy to understand for users, and sometimes quite effective, especially compared to the effort required to put up.\nTo get the most popular items, we only need a few lines of Pandas code to group by item and count the number of interactions: \nThen, for each of the user in our validation set, we can simply assign the most popular items to them: \nThat‚Äôs it. We now have recommendations provided for our users. Now we can move to the actual evaluation part."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#how-we-set-up-evaluation-for-our-project",
    "href": "projects/implement-recsys/c4/index.html#how-we-set-up-evaluation-for-our-project",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "How We Set Up Evaluation for our Project",
    "text": "How We Set Up Evaluation for our Project\nWe need to align the recommendations with the ground truth labels. The src/eval/utils.py module provides two functions for this purpose:\n\n\nsrc/eval/utils.py\n\nfrom src.eval import (\n    create_label_df,           # Create ground truth labels\n    merge_recs_with_target,    # Merge recommendations with labels  \n)\n\n\ndef create_label_df(df, user_col, item_col, rating_col, timestamp_col):\n    \"\"\"\n    Creates a ranking of items for each user based on their ratings and interaction timestamps, providing the ground truth for evaluation.\n    \"\"\"\n    label_cols = [user_col, item_col, rating_col, \"rating_rank\"]\n    label_df = (\n        df.sort_values([timestamp_col], ascending=[False])\n        .assign(\n            rating_rank=lambda df: df.groupby(user_col)[rating_col].rank(\n                method=\"first\", ascending=False\n            )\n        )\n        .sort_values([\"rating_rank\"], ascending=[True])[label_cols]\n    )\n    return label_df\n\ndef merge_recs_with_target(recs_df, label_df, user_col, item_col, rating_col, k=10):\n    \"\"\"\n    Align your model's recommendations with the ground truth and handling cases where recommended items don't appear in the validation set.\n    \"\"\"\n    return (\n        recs_df.pipe(\n            lambda df: pd.merge(\n                df, label_df[[user_col, item_col, rating_col, \"rating_rank\"]],\n                on=[user_col, item_col], how=\"outer\"\n            )\n        )\n        .assign(\n            rating=lambda df: df[rating_col].fillna(0).astype(int),\n            # Fill the recall with ranking = top_k_retrieve + 1 so that the recall calculation is correct\n            rec_ranking=lambda df: df[\"rec_ranking\"].fillna(k + 1).astype(int),\n        )\n        .sort_values([user_col, \"rec_ranking\"])\n    )\n\n\nMetric Logging with Evidently\nThe src/eval/log_metrics.py module uses the Evidently library to compute and log comprehensive metrics:\n\n\nsrc/eval/log_metrics.py\n\ndef log_ranking_metrics(cfg: Config, eval_df):\n    \"\"\"\n    Compute and log ranking metrics using Evidently.\n    Automatically integrates with MLflow for experiment tracking.\n    \"\"\"\n    column_mapping = ColumnMapping(\n        recommendations_type=\"rank\",\n        target=cfg.data.rating_col,\n        prediction=\"rec_ranking\", \n        item_id=cfg.data.item_col,\n        user_id=cfg.data.user_col,\n    )\n\n    report = Report(\n        metrics=[\n            NDCGKMetric(k=cfg.eval.top_k_rerank),\n            RecallTopKMetric(k=cfg.eval.top_k_retrieve),\n            PrecisionTopKMetric(k=cfg.eval.top_k_rerank),\n            FBetaTopKMetric(k=cfg.eval.top_k_rerank),\n            PersonalizationMetric(k=cfg.eval.top_k_rerank),\n        ]\n    )\n    \n    report.run(reference_data=None, current_data=eval_df, column_mapping=column_mapping)\n    \n    # Automatically log to MLflow if configured\n    if cfg.run.log_to_mlflow:\n        mlflow.log_artifact(evidently_report_fp)\n        # Log individual metrics for easy comparison\n        for metric_result in report.as_dict()[\"metrics\"]:\n            # ... metric logging logic\n    \n    return report\n\nThere is also a log_classification_metrics function that follows the similar pattern, but for the task of evaluation classification accuracy.\nThe output is we have HTML report like this:\n\n\n\nDo you notice that those report files are available in a nice web-based UI? If you are not familiar with MLflow, the next section will explain."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#experiment-tracking-with-mlflow",
    "href": "projects/implement-recsys/c4/index.html#experiment-tracking-with-mlflow",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "Experiment Tracking with MLflow",
    "text": "Experiment Tracking with MLflow\nMachine learning development is fundamentally an iterative process‚Äîthe more ideas we systematically test, the better our chances of discovering meaningful improvements. This iterative nature means we‚Äôre constantly cycling through hypotheses: trying new features, adjusting hyperparameters, experimenting with different architectures, and refining our approach based on results.\nTo maximize this exploration while maintaining scientific rigor, we need more than just robust evaluation metrics; we need a convenient way to track every experiment, compare results across iterations, and easily revisit previous approaches. Comprehensive experiment logging creates a safety net that encourages bold experimentation‚Äîwhen you know you can reproduce any previous result or quickly understand what changed between runs, you‚Äôre more willing to try ambitious ideas that might not work. This psychological safety accelerates the entire development cycle.\nThere are often no restrictions on how you should track your experiments. Even manually keeping things in a spreadsheet works just fine. Based on my own experience, I feel like MLflow is the most convenient way though, since this tool is mature and well-integrated with other ML frameworks. It also offers other fundamental capabilities like model registry which becomes very handy when we talk about model serving and deployment.\nAs MLflow should be enabled in our every model training run, I put most of the MLflow integration implementation in our configuration code:\nfrom src.cfg import ConfigLoader\n\n# Load configuration and initialize MLflow\ncfg = ConfigLoader(\"../cfg/common.yaml\")\ncfg.run.run_name = \"001-baseline-popular\"\ncfg.run.experiment_name = \"Retrieve - Binary\"\ncfg.init()\nThe cfg.init() method handles all the MLflow setup:\nfrom lightning.pytorch.loggers import MLFlowLogger\n\nclass ConfigLoader:\n    # ...\n\n    def init(self):\n        \"\"\"Initialize MLflow experiment tracking automatically.\"\"\"\n        if self.run.log_to_mlflow:\n            logger.info(f\"Setting up MLflow experiment {self.run.experiment_name}...\")\n            mlflow.set_experiment(self.run.experiment_name)\n            mlflow.start_run(run_name=self.run.run_name)\n            \n1            self._mlf_logger = MLFlowLogger(\n                experiment_name=self.run.experiment_name,\n                run_id=mlflow.active_run().info.run_id,\n                tracking_uri=mlflow_uri,\n                log_model=True,\n            )\n        return self\n\n1\n\nNote that MLFlowLogger here is a PyTorch Lightning logger, which we would use later when defining our PyTorch Lightning model for logging our model training progress.\n\n\nIn the end of our run, we can call this method cfg.log_config_to_mlflow() to log all parameters to MLflow:\ndef log_config_to_mlflow(self):\n    \"\"\"Log all configuration parameters to MLflow with dot notation.\"\"\"\n    flat_config = flatten_dict(self.config.model_dump())\n    for key, value in flat_config.items():\n        mlflow.log_param(key, value)  # e.g., \"train.learning_rate\": 0.01\nThe result is we can access our experiment parameters like this:\n\n\n\nIf for some reasons we don‚Äôt want to log your run to MLflow e.g.¬†when we starts to develop our notebook, we can easily control this behavior by setting the log_to_mlflow flag to false in our configuration file:\n# cfg/common.yaml\nrun:\n  log_to_mlflow: true\n  experiment_name: \"Retrieve - Binary\"\nAt this point, you‚Äôve got a working popularity-based recommender, a full offline evaluation pipeline, and MLflow tracking wired into your runs. With those pieces in place, you can safely try out new ideas, compare experiments side by side, and roll back if something goes sideways."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#recap",
    "href": "projects/implement-recsys/c4/index.html#recap",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we established the critical evaluation infrastructure that will power your recommendation system development. Here‚Äôs what we covered:\n\nThe fundamental evaluation challenges: We explored why recommendation systems are uniquely difficult to evaluate, mostly from the angle of counterfactual problem (we only see what users actually encountered) which makes simple accuracy metrics inadequate.\nOnline vs offline evaluation strategy: We established the principle that while online metrics are the ultimate business goal, offline evaluation serves as our rapid iteration tool. The key insight: offline improvements don‚Äôt always translate to online wins, but they provide essential guidance for systematic development.\nComprehensive evaluation metrics: We implemented a multi-dimensional framework using Evidently that captures ranking quality (Precision@K, Recall@K, NDCG@K), diversity (Personalization), and classification performance (ROC-AUC, Precision-Recall curves). We also justified why classification metrics make sense for recommendation systems when framed as binary relevance prediction.\nPopularity-based baseline implementation: We built our first recommender system‚Äîa simple but effective popularity-based model that assigns the most frequently interacted items to all users. This baseline serves as our performance benchmark and validates that our evaluation pipeline works correctly.\nMLflow experiment tracking integration: We created a configuration-driven system that automatically handles experiment setup, parameter logging, and artifact storage. This infrastructure encourages bold experimentation by providing the safety net of full reproducibility.\n\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/010-baseline-popular.ipynb file and the src/eval/ directory in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#whats-next",
    "href": "projects/implement-recsys/c4/index.html#whats-next",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "What‚Äôs Next",
    "text": "What‚Äôs Next\nIn Chapter 5, we‚Äôll build on this foundation to implement session-based recommendation models that can capture temporal patterns in user behavior. The evaluation framework you‚Äôve built here will be essential for measuring whether these more complex models actually improve upon our simple baseline. And if the our evaluation setup does a good job, the results would be hard to miss:\n\n\n\nAs you can see, MLflow helps comparing model iterations easily üòâ.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation. dvquys.com. https://dvquys.com/projects/implement-recsys/c4/."
  },
  {
    "objectID": "projects/implement-recsys/c4/index.html#footnotes",
    "href": "projects/implement-recsys/c4/index.html#footnotes",
    "title": "Implement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou may take a look at this writing Counterfactual Evaluation for Recommendation Systems to learn more.‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html",
    "href": "projects/implement-recsys/c6/index.html",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "",
    "text": "This is the sixth chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#introduction",
    "href": "projects/implement-recsys/c6/index.html#introduction",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 5, we have come a long way to build our sequence-based recommendation model which demonstrates significant metric uplifts over the popularity baseline. It can capture temporal patterns in user behavior and personalize recommendations based on interaction sequences. We‚Äôre finally ready to tell the world about our exciting new model! To do that, we need to package our work in a way that end-users can benefit from.\nThis chapter aims to provide a transition ground from model development to deployment readiness. The challenge isn‚Äôt just loading a model and calling predict()‚Äîit‚Äôs building a proper system that can respond in milliseconds.\nDeploying ML models in production is actually a very huge area and deserves a series of its own. Within the scope of this tutorial, we will discuss the topic from the angle of an end-to-end data scientist‚Äîwhose job is to design the complete pipeline from training to serving.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/020-ann-index.ipynb and notebooks/021-store-user-item-sequence.ipynb files, along with the infrastructure configuration in compose.yml and the mlflow/ directory in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#serving-paradigms-from-model-to-user-experience",
    "href": "projects/implement-recsys/c6/index.html#serving-paradigms-from-model-to-user-experience",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Serving Paradigms: From Model to User Experience",
    "text": "Serving Paradigms: From Model to User Experience\nI have been always thinking that building a recommendation model is like crafting a perfect recipe for a gourmet dish. You might have the most sophisticated ingredients, precise measurements, and innovative techniques, but until that dish reaches hungry customers, it‚Äôs just potential sitting in the kitchen. The recipe itself doesn‚Äôt feed anyone‚Äîit‚Äôs the execution, plating, and delivery that creates value. Similarly, our sequence-based recommendation model, no matter how accurate its predictions, remains useless until it can respond to real user requests in real-time.\nThis analogy extends further when we consider the restaurant‚Äôs serving strategy. A high-end restaurant doesn‚Äôt prepare each dish from scratch when a customer orders. Instead, they use a combination of pre-preparation (mise en place) and real-time cooking. Some components are prepped in batches during quiet hours, while others must be prepared fresh to order. Recommendation systems follow the same principle: we balance pre-computed results with real-time inference based on the specific requirements of each use case.\n\nHow Recommendations Reach Users\n\n\n\nUnderstanding serving paradigms starts with tracing the journey from user action to displayed recommendations. When a user opens your app, a carefully orchestrated process begins:\n\nFrontend Request: The mobile app or web interface detects the user‚Äôs need for recommendations (homepage load, category browse, search query)\nAPI Call: The frontend makes a REST API call to the backend, passing user context (user ID, current page, device info)\nBackend Processing: The recommendation service processes this request, either by looking up pre-computed results or triggering real-time model inference\nResponse Assembly: Results are ranked, filtered for business rules, and formatted for display\nFrontend Rendering: The app receives the recommendations and renders them in the user interface\n\nWhile the actual requirements vary between use cases, normally this entire pipeline must complete within 200ms at the 95th percentile to maintain user engagement. Any longer, and users start experiencing the dreaded loading spinner that kills conversion rates.\n\n\nTwo Paradigms: Batch Pre-computation vs Online Serving\nThe critical decision in this pipeline happens at step 3: how does the backend generate recommendations? There are two fundamental paradigms, each with distinct trade-offs:\nBatch Pre-computation follows the ‚Äúprepare ahead‚Äù strategy. Recommendations are generated offline for all users during low-traffic periods and stored in a key-value store like Redis or DynamoDB. When a user makes a request, the API simply looks up their pre-computed recommendations‚Äîa blazingly fast operation that typically completes in under 10ms.\n# Batch pre-computation approach\ndef batch_generate_recommendations():\n    \"\"\"Run offline to pre-compute recommendations for all users.\"\"\"\n    for user_id in all_users:\n        recommendations = model.predict(user_id)\n        redis_client.set(f\"recs:{user_id}\", recommendations, ex=3600)\n\ndef serve_recommendations(user_id):\n    \"\"\"Fast lookup during serving.\"\"\"\n    return redis_client.get(f\"recs:{user_id}\")\nOnline Serving follows the ‚Äúcook to order‚Äù strategy. The model inference happens in real-time when users make requests. This requires the recommendation model to be deployed as a service that can respond to API calls within milliseconds.\n# Online serving approach\ndef serve_recommendations(user_id, context):\n    \"\"\"Real-time model inference during serving.\"\"\"\n    user_features = get_user_features(user_id)\n    contextual_features = extract_context(context)\n    return model.predict(user_features, contextual_features)\n\n\nWhen to Choose Each Paradigm\nThe choice between batch pre-computation and online serving isn‚Äôt arbitrary‚Äîit depends on your specific constraints and requirements. Batch pre-computation shines when recommendations don‚Äôt need real-time context. It‚Äôs also ideal when you can tolerate slightly stale recommendations that are updated periodically, perhaps hourly or daily, in exchange for guaranteed fast response times under all conditions. And it‚Äôs particularly attractive when your infrastructure needs to be simple and cost-effective, especially if you‚Äôre serving millions of users and want predictable resource utilization patterns.\nOn the other hand, online serving becomes essential when having access to real-time contexts for recommendations is crucial in ensuring the relevance of our recommendations. Those contexts can be a user‚Äôs current shopping cart contents, her search query, or time-sensitive factors like current location or weather. All of these signals share an important property: we don‚Äôt know them in advance, so pre-computation is not an option.\nIn practice, most production recommendation systems use a hybrid approach that combines both paradigms strategically. The default fallback homepage might display pre-computed ‚Äútrending items‚Äù in cases of new users or errors from the recommendation services. In the remaining happy cases, we pass the relevant contexts to the model endpoints to provide better browsing experiences for users. This hybrid strategy optimizes for both performance and personalization.\nAs you can see, the UI demo for our project also starts by showing the popular recommendations before updating the list with respect to user‚Äôs interactions."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#three-pillar-serving-architecture",
    "href": "projects/implement-recsys/c6/index.html#three-pillar-serving-architecture",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Three-Pillar Serving Architecture",
    "text": "Three-Pillar Serving Architecture\nWe‚Äôll set up three main components that form the backbone of serving recommendation systems: MLflow for model registry and deployment, Qdrant for vector similarity search, and Redis as KV store for user sequences and metadata. Each addresses a specific aspect of the real-time recommendation challenge.\n\n\n\n\nThree-pillar serving architecture with Model Registry, Vector Store, and Key-Value Cache\n\n\n\n\nPillar 1: Model Registry (MLflow)\nThe Model Registry serves as the single source of truth for all model artifacts and metadata. Instead of need to look up which S3 path pointing to which model version, we have a nice abstraction layer to manage and use our trained models much more easily.\nMLflow Model Registry provides several useful capabilities for model serving:\n\nModel Versioning: Every model training run produces a versioned artifact with associated metadata (metrics, parameters, training data lineage)\nStage Management: Models progress through stages like ‚ÄúStaging‚Äù ‚Üí ‚ÄúChampion‚Äù ‚Üí ‚ÄúArchived‚Äù with clear governance\nA/B Testing Support: Multiple model versions can be deployed simultaneously with traffic splitting\n\n\n\nPillar 2: Vector Store (Qdrant)\nThe goal of Vector Store is to make similarity search efficient. While traditional databases excel at exact matches for structured data, the nature of our embeddings demands a different indexing approach. With the rising popularity of LLMs and their ability to generate embeddings, a handful of vector databases have emerged to provide solutions.\nIn our case, our sequence model‚Äôs candidate tower generates embeddings for all items in the catalog. These embeddings are indexed in Qdrant, enabling fast retrieval of the most relevant candidates based on a user‚Äôs current sequence.\nThe choice of using Qdrant here is nothing too special rather than personal, as I happen to find myself comfortable with it.\n\n\nPillar 3: Key-Value Cache (Redis)\nThe Key-Value Cache serves as the system‚Äôs memory for frequently accessed data and computational shortcuts. In recommendation systems, this typically includes user sequences, popular items, and pre-computed features that would be expensive to generate on-the-fly.\nFor our recommender system, Redis stores each user‚Äôs recent interaction history, enabling the query tower to generate contextual embeddings without expensive database lookups. It also contains the popular items as a form of pre-computed recommendations.\nWhen thinking about which open source tool to use for the key-value store, I must admit that Redis is so widely used that I don‚Äôt really consider other options, especially for a tutorial project like this.\nAfter understanding those three pillars, let‚Äôs take a closer look at how we configure them running on our local environment."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#docker-compose-configuration",
    "href": "projects/implement-recsys/c6/index.html#docker-compose-configuration",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Docker Compose Configuration",
    "text": "Docker Compose Configuration\n\n\n\nDocker logo\n\n\nDocker is a containerization platform that packages applications and their dependencies into lightweight, portable containers. Think of it as creating a standardized shipping container1 for your software. Just as shipping containers can move seamlessly between trucks, ships, and trains regardless of their contents, Docker containers can run consistently across different environments, from your laptop to production servers. This is particularly valuable for ML systems where dependency management can become complex, with different services requiring specific versions of Python, database drivers, or system libraries. It‚Äôs becoming a standard approach to package application services for a lot of teams nowadays.\nDocker Compose takes this concept one step further by orchestrating multiple containers as a unified application stack. Instead of manually starting each service and configuring their networking, Docker Compose allows us to define our entire infrastructure‚ÄîMLflow, Qdrant, Redis, and their supporting databases‚Äîin a single configuration file. With a simple docker-compose up command, we can spin up our complete serving environment with all services properly connected and configured. This approach eliminates the ‚Äúworks on my machine‚Äù problem that often plagues ML deployments, ensuring that anyone can reproduce the exact same serving environment that we‚Äôve designed.\nHere‚Äôs what our Docker Compose file looks like:\n\n\ncompose.yml\n\nservices:\n  qdrant:\n    image: qdrant/qdrant:v1.12.0\n    ports:\n      - \"6333:6333\"\n      - \"6334:6334\"\n    volumes:\n      - ./data/qdrant_storage:/qdrant/storage:z\n    restart: always\n\n  kv_store:\n    image: redis:7.2-bookworm\n    container_name: kv_store\n    ports:\n      - \"${REDIS_PORT}:6379\"\n    volumes:\n      - ./data/redis:/data\n    env_file:\n      - .env\n    healthcheck:\n      test: [\"CMD\", \"redis-cli\", \"-p\", \"6379\", \"ping\"]\n      interval: 10s\n      timeout: 30s\n      retries: 50\n      start_period: 30s\n\n  mlflow_server:\n    restart: always\n    build: ./mlflow\n    image: mlflow_server\n    container_name: mlflow_server\n    depends_on:\n      - mlflow_mc\n      - mlflow_db\n    ports:\n      - \"5002:5000\"\n    environment:\n      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000\n      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}\n      - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}\n    command: &gt;\n      mlflow server\n        --backend-store-uri mysql+pymysql://${MYSQL_USER}:${MYSQL_PASSWORD}@mlflow_db:3306/${MYSQL_DATABASE}\n        --artifacts-destination s3://mlflow\n        --serve-artifacts\n        --host 0.0.0.0\n        --port 5000\n\nThis configuration defines our entire serving infrastructure as code. A simple docker compose up -d command brings up all services with proper networking, volumes, and health checks configured.\nAlthough it‚Äôs pretty straightforward when you look how Redis and Qdrant are defined, I need to elaborate a bit on the MLflow part.\n\nSetting up MLflow Model Registry\nMLflow Model Registry serves as our model deployment pipeline, providing versioning, staging, and rollback capabilities essential for production ML systems. By design MLflow coordinates between two backend systems: a MySQL Database that stores all the metadata‚Äîexperiments, runs, parameters, metrics, and model registry information‚Äîand MinIO S3 Storage that acts as the artifact store where the actual model binaries, datasets, and logs are persisted. MLflow Server acts as the central orchestration service, providing REST APIs for experiment tracking, model registration, and serving.\nThis separation of metadata and artifacts provides both performance and scalability benefits. The database can quickly serve metadata queries for experiment comparisons and model lookups, while the object storage efficiently handles large model files without putting pressure on the database.\nNow it should be clear why we need to run make ml-platform-up before running our notebooks in previous chapters. We do not need to worry too much about where the train models are stored, cause they sit happily in MLflow Registry. Below is the example code snippet for loading models:\n\n\nnotebooks/020-ann-index.ipynb\n\nmlf_client = mlflow.MlflowClient()\nmlf_model = mlflow.pyfunc.load_model(\n    model_uri=f\"models:/{cfg.train.retriever.mlf_model_name}@champion\"\n)\nrun_id = mlf_model.metadata.run_id\nrun_info = mlf_client.get_run(run_id).info\nartifact_uri = run_info.artifact_uri\n\ninferer = mlf_model.unwrap_python_model()\nid_mapping = inferer.idm\n\nNext we will populate Qdrant and Redis with the data they need to support online serving."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#building-the-vector-index-with-qdrant",
    "href": "projects/implement-recsys/c6/index.html#building-the-vector-index-with-qdrant",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Building the Vector Index with Qdrant",
    "text": "Building the Vector Index with Qdrant\n\nItem Embedding Extraction\nThe first step is extracting embeddings from our trained sequence model. Our two-tower implementation contains a method get_candidate_embeddings that makes this straightforward:\n\n\nsrc/sequence/model.py\n\nclass SoleSequenceRetriever(BaseSequenceRetriever):\n    # ...\n\n    def get_candidate_embeddings(self, inputs: Dict[str, torch.Tensor]) -&gt; torch.Tensor:\n        candidate_items = inputs.get(\"candidate_items\")\n        if candidate_items is None:\n            raise ValueError(\"Missing required input key: 'candidate_items'\")\n        candidate_embedding = self.item_embedding(candidate_items)\n        return candidate_embedding\n\n\n\nnotebooks/020-ann-index.ipynb\n\n# inferer is the model wrapper loaded from MLflow in the previous snippet\nmodel = inferer.model\n\nall_item_ids = torch.arange(num_items)\ninputs = {\"candidate_items\": all_item_ids}\n\nwith torch.no_grad():\n    candidate_embeddings = model.get_candidate_embeddings(inputs).detach().numpy()\n\n\n\nQdrant Collection Setup\nQdrant organizes vectors into collections with configurable distance metrics and indexing parameters:\n\n\nnotebooks/020-ann-index.ipynb\n\nfrom qdrant_client import QdrantClient, models\n\n# Initialize Qdrant client\nclient = QdrantClient(url=cfg.vectorstore.qdrant.url)\n\n# Create collection for item embeddings\nclient.create_collection(\n    collection_name=cfg.vectorstore.qdrant.collection_name,\n    vectors_config=models.VectorParams(\n        size=cfg.train.embedding_dim,\n1        distance=models.Distance.COSINE\n    )\n)\n\n\n1\n\nSpecify COSINE as the distance to align with how we implement the model forward pass.\n\n\n\n\nBatch Indexing\nWe do not only upload the embeddings to Qdrant but also their metadata. This becomes quite handy later when preparing the demo as we can get the item metadata by ID from Qdrant without the need to set up new database.\npoints = []\nfor idx, vector in enumerate(candidate_embeddings):\n    id_ = id_mapping.get_item_id(idx)\n    payload = metadata_map[id_]\n    payload[cfg.data.item_col] = id_\n    point = PointStruct(id=idx, vector=vector.tolist(), payload=payload)\n    points.append(point)\n\nupsert_result = ann_index.upsert(\n    collection_name=cfg.vectorstore.qdrant.collection_name,\n1    points=points,\n)\nassert str(upsert_result.status) == \"completed\"\n\n1\n\nNote that we can send a batch of points to Qdrant in one go.\n\n\nOnce indexed, we can search for nearest neighbors by inputing a query embedding:\nid_ = \"043935806X\"\nidx = id_mapping.get_item_index(id_)\ninputs = {\"item_seq\": torch.tensor([[idx]])}\nquery_embedding = model.get_query_embeddings(inputs)[0]\n\nhits = ann_index.search(\n    collection_name=cfg.vectorstore.qdrant.collection_name,\n    query_vector=query_embedding,\n    limit=cfg.eval.top_k_retrieve,\n)\n\ndisplay(hits)\nExample output when cfg.eval.top_k_retrieval = 2:\n[\n    ScoredPoint(\n        id=794,\n        version=0,\n        score=0.89372206,\n        payload={\n            'main_category': 'Books',\n            'title': 'The Sword of Shannara',\n            'average_rating': 4.4,\n            'rating_number': 5470,\n            'price': '8.25',\n            'subtitle': 'Mass Market Paperback ‚Äì July 12, 1983',\n            'image_url': 'https://placehold.co/350x525/0057a3/ffffff.png?text=The%0ASword%0Aof%0AShannara&font=raleway',\n            'parent_asin': '0345314255'\n        },\n        vector=None,\n        shard_key=None,\n        order_value=None\n    ),\n    ScoredPoint(\n        id=1381,\n        version=0,\n        score=0.8898184,\n        payload={\n            'main_category': 'Books',\n            'title': 'Harry Potter and the Chamber of Secrets',\n            'average_rating': 4.8,\n            'rating_number': 85813,\n            'price': '6.81',\n            'subtitle': 'Hardcover ‚Äì Big Book, July 1, 1999',\n            'image_url': 'https://m.media-amazon.com/images/I/519HQF7Vl6L._SY291_BO1,204,203,200_QL40_FMwebp_.jpg',\n            'parent_asin': '0439064864'\n        },\n        vector=None,\n        shard_key=None,\n        order_value=None\n    )\n]"
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#caching-user-sequences-in-redis",
    "href": "projects/implement-recsys/c6/index.html#caching-user-sequences-in-redis",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Caching User Sequences in Redis",
    "text": "Caching User Sequences in Redis\nFirst we need to prepare our dataframe containing our user sequence signals. Since this is for serving, we would combine both train and val dataset and get the latest interaction for each user.\n\n\nnotebooks/021-store-user-item-sequence.ipynb\n\ntrain_features_df = pd.read_parquet(cfg.data.train_features_fp)\nval_features_df = pd.read_parquet(cfg.data.val_features_fp)\nfull_df = pd.concat([train_features_df, val_features_df], axis=0)\n\n# Locate the last instance per user from our interaction data\nlatest_df = full_df.assign(\n    recency=lambda df: df.groupby(cfg.data.user_col)[cfg.data.timestamp_col].rank(\n        method=\"first\", ascending=False\n    )\n).loc[lambda df: df[\"recency\"].eq(1)]\n\nDumping data into Redis is quite easy.\n\n\nnotebooks/021-store-user-item-sequence.ipynb\n\nr = redis.Redis(host=cfg.redis.host, port=cfg.redis.port, db=0, decode_responses=True)\nassert r.ping(), (\n    f\"Redis at {cfg.redis.host}:{cfg.redis.port} is not running, please make sure you have started the Redis docker service\"\n)\n\nfor i, row in tqdm(latest_df.iterrows(), total=latest_df.shape[0]):\n    # Since the row containing previous interacted items, we can get them and append the current item to compose the full sequence\n    prev_item_indices = [int(item) for item in row[\"item_sequence\"] if item != -1]\n    prev_item_ids = [idm.get_item_id(idx) for idx in prev_item_indices]\n    updated_item_sequences = prev_item_ids + [row[cfg.data.item_col]]\n    user_id = row[cfg.data.user_col]\n    key = cfg.redis.keys.recent_key_prefix + user_id\n    # Here we convert those list of string IDs into a single string with \"__\" as separator\n    # This is for convenience only since in Redis there are other data structures like the list\n    # which can also be used to store the sequence\n    value = \"__\".join(updated_item_sequences)\n    r.set(key, value)\n\nGetting data is no harder.\ntest_user_id = latest_df.sample(1)[cfg.data.user_col].values[0]\nresult = r.get(cfg.redis.keys.recent_key_prefix + test_user_id)\ndisplay(result)\nExample output:\n'B078GWN38X__B078JJFFGK__B07ZDG34ZC__B079QG6L98__B00M9GZTXG__B07CWSSFL3__B0031W1E86__B07LF2YL9S__B07BJZJ34M__B077XVF99N__B07F668MBT'\nIn a technically similar way, we can populate the popular items in Redis. I will skip the details here, as you can easily find it from the notebook."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#recap",
    "href": "projects/implement-recsys/c6/index.html#recap",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we mimic our local environment with the infrastructure foundation needed to transform our sequence-based model from a model prototype into a deployable recommendation system.\nWe use Docker Compose to set up our stack of MLflow for model registry and versioning, Qdrant for fast vector similarity search, and Redis for caching user sequences and popular items. Then we implemented data population scripts to extract candidate embeddings from our trained model, index them in Qdrant with rich metadata, and populate Redis with user interaction sequences."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#next-steps",
    "href": "projects/implement-recsys/c6/index.html#next-steps",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Next Steps",
    "text": "Next Steps\nWith our serving infrastructure ready and populated with data, we‚Äôre prepared for the final step: building the API layer that will expose our recommendation system to users. In Chapter 7, we‚Äôll learn how to use BentoML to host our model as an API endpoint and develop a FastAPI orchestrator to serve our recommendations.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 6: Preparing for Serving. dvquys.com. https://dvquys.com/projects/implement-recsys/c6/."
  },
  {
    "objectID": "projects/implement-recsys/c6/index.html#footnotes",
    "href": "projects/implement-recsys/c6/index.html#footnotes",
    "title": "Implement a RecSys, Chapter 6: Preparing for Serving",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI love how Docker‚Äôs logo is actually a whale carrying containers on its back‚Äîit‚Äôs a perfect visual metaphor for what the platform does üëç.‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html",
    "href": "projects/implement-recsys/c7/index.html",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "",
    "text": "This is the seventh and the final chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#introduction",
    "href": "projects/implement-recsys/c7/index.html#introduction",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 6, we successfully prepared our serving infrastructure with MLflow model registry, Qdrant vector database, and Redis cache. We‚Äôve also populated these systems with our trained model artifacts, item embeddings, and user sequence data. We have come to the end of the journey! What‚Äôs left is to build the actual API layer that brings everything together and serves real-time recommendations to users.\nThis chapter explores how to package our trained models for serving using BentoML, and how to orchestrate the complete recommendation pipeline using FastAPI. By the end of this chapter, you‚Äôll have a fully functional recommendation API running on your local machine that can adapt to user interests in real-time.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the api/ and model_server/ directories, along with the compose.api.yml file in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#the-serving-logic",
    "href": "projects/implement-recsys/c7/index.html#the-serving-logic",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "The Serving Logic",
    "text": "The Serving Logic\n\n\n\n\n\n\n\nRetriever Serving Architecture\n\n\n\nThe above diagram illustrates how our recommendation system orchestrates multiple components to deliver personalized recommendations in real-time. Let‚Äôs walk through the complete flow to understand how each piece works together.\n\nRequest Processing and Fallback Strategy\nOur serving architecture prioritizes personalization when possible but ensures users always receive relevant recommendations, even when personalization fails.\nInitial Request and Context Gathering\nWhen a recommendation request arrives at our FastAPI orchestrator, it contains the user‚Äôs current session context‚Äîitems they‚Äôve recently viewed or interacted with‚Äîalong with an optional user identifier. The orchestrator‚Äôs first priority is to enrich this context with historical user data.\nUser Feature Retrieval \nThe system immediately attempts to fetch the user‚Äôs historical interaction data from Redis. When user context is successfully retrieved and contains meaningful interaction data, the system proceeds with the full personalization pipeline:\n\nQuery Embedding Generation: The enriched user context (historical + current session) is sent to our BentoML model server, which generates the dense representation of the user‚Äôs preferences and current intent.\nVector Similarity Search: This query embedding is used to search Qdrant‚Äôs vector database, finding items most similar to the user‚Äôs demonstrated preferences.\nFiltering and Ranking: Results are filtered to remove items the user has already interacted with and ranked by relevance score.\n\nBut wait. What is model server? What is the difference between BentoML and FastAPI?"
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#model-server-vs-orchestrator",
    "href": "projects/implement-recsys/c7/index.html#model-server-vs-orchestrator",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "Model Server vs Orchestrator",
    "text": "Model Server vs Orchestrator\nAs can be seen above, serving recommendations typically requires two kinds of logic: model inference and business rules. While business rules are operations often found in a backend system, model inference is a specialized task that has a different set of characteristics:\nIndependent Scaling: Model servers sometimes require GPU resources for inference and benefit from batch processing to maximize throughput. The orchestrator, conversely, handles lightweight business logic and can run efficiently on CPU-only instances.\nVersion Management: We should expect ourselves to experiment with new models frequently. Having a dedicated model server means we can deploy new model versions, conduct A/B tests, or rollback problematic releases without touching the orchestrator logic. This reduces the blast radius of model-related issues.\nTechnology Flexibility: The model server can use specialized ML serving frameworks (like BentoML, TorchServe, or TensorFlow Serving) optimized for inference performance, while the orchestrator can use general-purpose web frameworks (like FastAPI, Flask, or Django) optimized for API development productivity.\nA clear separation ensures that each component can be optimized for its specific responsibilities while maintaining clean interfaces between them.\nFor our implementation, we chose FastAPI for the orchestrator due to its popularity and developer-friendly async capabilities, while BentoML serves as our model server with its robust MLflow integration and specialized ML serving features.\nWhile there‚Äôs not much to discuss about FastAPI, there are some specific elaborations needed on the side of BentoML regarding how I use it in this project."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#bentoml-x-mlflow",
    "href": "projects/implement-recsys/c7/index.html#bentoml-x-mlflow",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "BentoML x MLflow",
    "text": "BentoML x MLflow\nOne of BentoML‚Äôs strengths is its seamless integration with MLflow. Remember how we logged our sequence model to MLflow in previous chapters? BentoML can directly import these models along with all their dependencies and custom inference logic.\n\n\nmodel_server/service.py\n\nmodel_cfg = {\n    model_name: {\n        \"name\": model_name,\n        \"deploy_alias\": \"champion\",\n        \"model_uri\": f\"models:/{model_name}@champion\",\n    },\n}\n\nfor name, cfg in model_cfg.items():\n    bentoml.mlflow.import_model(\n        name,\n        model_uri=cfg[\"model_uri\"],\n        signatures={\n            \"predict\": {\"batchable\": True},\n        },\n    )\n\nThis code demonstrates how BentoML imports our trained model directly from MLflow using the champion alias. The batchable: True flag enables automatic batching for improved throughput‚Äîmultiple concurrent requests will be batched together for more efficient GPU utilization.\nLater when you start the API services via Docker, you can observe that BentoML downloads the model from MLflow:\n\n\n\n\nCustom Inference Logic Integration\nHere‚Äôs where the design decision from our training phase pays off. In the previous chapters we didn‚Äôt talk about the src/sequence/inference.py file that exists in the src/sequence module, but if you pay attention in the training notebook, here‚Äôs what you would find:\n\n\nnotebooks/011-sequence-modeling.ipynb\n\ninferrer = SequenceRetrieverInferenceWrapper(best_model)\ninferrer.load_context(ctx)\n\nif cfg.run.log_to_mlflow:\n    run_id = trainer.logger.run_id\n    sample_output_np = sample_output\n    signature = infer_signature(sample_input, sample_output_np)\n    idm_filename = idm_fp.split(\"/\")[-1]\n    mlflow.pyfunc.log_model(\n        python_model=inferrer,\n        artifact_path=\"inferrer\",\n        # We log the id_mapping to the predict function so that it can accept item_id and automatically convert ot item_indice for PyTorch model to use\n        artifacts={\"idm\": mlflow.get_artifact_uri(idm_filename)},\n        signature=signature,\n        input_example=sample_input,\n        registered_model_name=cfg.train.retriever.mlf_model_name,\n    )\n\nThis SequenceRetrieverInferenceWrapper class wraps our model with inference logic that handles ID mapping, sequence padding, and tensor conversion to help BentoML to load and use the model correctly.\n\n\nsrc/sequence/inference.py\n\nclass SequenceRetrieverInferenceWrapper(mlflow.pyfunc.PythonModel):\n    def load_context(self, context):\n        \"\"\"\n        This load_context method is automatically called when later we load the model.\n        \"\"\"\n        json_path = context.artifacts[\"idm\"]\n        self.idm = IDMapper().load(json_path)\n\n    def predict(self, context, model_input, params=None):\n        \"\"\"\n        Args:\n            model_input: Expected to contain keys 'user_ids', 'candidate_items', and 'item_seq'.\n        \"\"\"\n        if not isinstance(model_input, dict):\n            model_input = model_input.to_dict(orient=\"records\")[0]\n        infer_output = self.infer(model_input).tolist()\n        return {**model_input, \"scores\": infer_output}\n\n\n\nBentoML Service Implementation\nFor our retrieval serving use case, instead of just calling the model.predict() method like for a normal ML model, we need to access the model‚Äôs query embedding generation function. Our BentoML service provides multiple endpoints for different inference needs:\n\n\nmodel_server/service.py\n\n@bentoml.service(name=\"seq_retriever_service\")\nclass SeqRetrieverService:\n    def __init__(self):\n        self.model = bentoml.mlflow.load_model(self.bento_model)\n        self.inferer = self.model.unwrap_python_model()\n\n    @bentoml.api\n    def predict(self, ctx: RetrieveContext):\n        resp = self.model.predict(ctx.model_dump())\n        return self._augment_response(resp, ctx)\n\n    @bentoml.api\n    def get_query_embeddings(self, ctx: RetrieveContext):\n        item_seq = [\n            self.inferer.idm.get_item_index(item_id) for item_id in ctx.item_seq_raw[0]\n        ]\n        inputs = {\"item_seq\": torch.tensor([item_seq])}\n        query_embedding = self.inferer.model.get_query_embeddings(inputs)\n        resp = {\"query_embedding\": query_embedding.detach().numpy().tolist()}\n        return self._augment_response(resp, ctx)"
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#fastapi-orchestrator-coordinating-the-complete-pipeline",
    "href": "projects/implement-recsys/c7/index.html#fastapi-orchestrator-coordinating-the-complete-pipeline",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "FastAPI Orchestrator: Coordinating the Complete Pipeline",
    "text": "FastAPI Orchestrator: Coordinating the Complete Pipeline\nWhile our model server handles the heavy lifting of neural network inference, the FastAPI orchestrator coordinates the complete recommendation pipeline. It‚Äôs responsible for fetching user context, calling the model server, querying the vector database, and assembling the final response.\nOur API provides several endpoints that support different aspects of the recommendation system:\n\n\n\n\n\napi/app.py\n\n@app.post(\"/recs/retrieve\")\nasync def retrieve(\n    ctx: RetrieveContext,\n    count: Optional[int] = 10,\n    rec_service: RecommendationService = Depends(get_recommendation_service),\n):\n    return await rec_service.retrieve_recommendations(ctx, count)\n\n@app.get(\"/recs/popular\")\nasync def get_recommendations_popular(\n    request: PopularItemsRequest = Depends(),\n    rec_service: RecommendationService = Depends(get_recommendation_service),\n):\n    return await rec_service.get_popular_recommendations(request.count)\n\n@app.post(\"/vendor/seq_retriever\")\nasync def seq_retriever(\n    request: SeqRetrieverRequest,\n    rec_service: RecommendationService = Depends(get_recommendation_service),\n):\n    return await rec_service.call_seq_retriever(request.ctx, request.endpoint)\n\nEach endpoint serves a specific purpose:\n\n/recs/retrieve: The main recommendation endpoint that provides personalized suggestions based on user context\n/recs/popular: Fallback endpoint that returns popular items when personalization isn‚Äôt possible\n/vendor/seq_retriever: Direct access to the model server for debugging and testing\n\n\nThe Service Layer\nYou may notice that above the endpoint almost does nothing except for calling a method from the RecommendationService class. One of the key organizational decisions in our orchestrator is the introduction of a service layer that encapsulates all business logic. This pattern, borrowed from domain-driven design, provides several benefits.\nFastAPI endpoints are reserved to focus on HTTP-specific concerns like request parsing, response formatting, and error handling, while the service layer focuses on domain logic such as fetching user context, calling external services, and filtering results. This separation makes the codebase more maintainable and easier to reason about.\nFrom a testing perspective, business logic can be unit tested independently of the FastAPI framework, making tests faster and more focused. You can test complex recommendation logic without spinning up a web server or dealing with HTTP request/response cycles. Additionally, the same service logic could be reused by different interfaces‚Äîwhether REST APIs, GraphQL endpoints, or background jobs‚Äîwithout code duplication.\nThe service layer also improves dependency management by clearly declaring its dependencies, such as Redis clients, Qdrant clients, and ID mappers. This explicit dependency declaration makes the system‚Äôs architecture more transparent and easier to understand, while also facilitating dependency injection for testing and different deployment environments.\n\n\nRequest Processing Pipeline\nLet‚Äôs trace through a complete recommendation request to understand how all components work together.\nFirst, we have a RetrieveContext class that defines the schema for the information needed to make the retrieval request.\n\n\nsrc/dto.py\n\nclass RetrieveContext(BaseModel):\n    user_ids_raw: Optional[List[str]] = []\n    item_seq_raw: Optional[List[List[str]]] = [[]]\n    candidate_items_raw: Optional[List[str]] = []\n\nThis is the input to the retrieve_recommendations method.\n\n\napi/services.py\n\nclass RecommendationService:\n    # ...\n\n    async def retrieve_recommendations(\n        self, ctx: RetrieveContext, count: int\n    ) -&gt; RecommendationResponse:\n        # Items to exclude from recommendations\n        items_to_exclude = set()\n\n        if len(ctx.user_ids_raw) &gt; 0 and (user_id := ctx.user_ids_raw[0]):\n            logger.info(f\"Getting recent interactions for user: {user_id}\")\n            user_id = ctx.user_ids_raw[0]\n            user_prev_interactions = self.get_user_prev_interactions(user_id)[\n                \"recent_interactions\"\n1            ]\n\n            # Add user's previous interactions to exclusion set\n            items_to_exclude.update(user_prev_interactions)\n\n            curr_item_seq = ctx.item_seq_raw[0]\n            ctx.item_seq_raw = [user_prev_interactions + curr_item_seq]\n\n        # Add items from input sequence to exclusion set\n        if ctx.item_seq_raw and ctx.item_seq_raw[0]:\n            items_to_exclude.update(ctx.item_seq_raw[0])\n\n        logger.info(\n            f\"[DEBUG] Items to exclude from recommendations: {items_to_exclude}\"\n        )\n\n        if len(ctx.item_seq_raw[0]) == 0:\n            logger.info(\"Empty RetrieveContext, fallback to popular recommendations\")\n2            return await self.get_popular_recommendations(count)\n\n    query_embedding_resp = await self.call_seq_retriever(\n        ctx, \"get_query_embeddings\"\n    )\n    query_embedding = np.array(query_embedding_resp.result[\"query_embedding\"])\n\n    # Get more recommendations than needed since we'll filter some out\n    buffer_count = count + len(items_to_exclude)\n    hits = self.services.ann_index.search(\n        collection_name=cfg.vectorstore.qdrant.collection_name,\n        query_vector=query_embedding[0],\n        limit=buffer_count,\n    )\n\n    # Filter out items that should be excluded\n    filtered_recommendations = []\n    for hit in hits:\n        item_id = hit.payload.get(\"parent_asin\", \"\")\n        if item_id not in items_to_exclude:\n            filtered_recommendations.append(\n                {\"score\": hit.model_dump()[\"score\"], **hit.payload}\n            )\n            if len(filtered_recommendations) &gt;= count:\n                break\n\n    return RecommendationResponse(\n        recommendations=filtered_recommendations,\n        ctx=ctx.model_dump(),\n    )\n\n\n1\n\nGet the user‚Äôs previous interactions from Redis.\n\n2\n\nFallback to popular recommendations.\n\n\nThis above implementation demonstrates several important patterns that make our recommendation system robust and effective. The most fundamental pattern is context enrichment, where the system fetches user‚Äôs historical interactions from Redis and merges them with the current session data. This creates a comprehensive view of user behavior that spans both past preferences and immediate intent.\nThe system also implements graceful degradation by falling back to popular recommendations when no meaningful sequence data is available. This ensures that new users or those with insufficient interaction history still receive valuable recommendations rather than empty results.\nTo improve user experience, the pipeline implements a business rule-based filtering that excludes items the user has already interacted with, preventing redundant recommendations. Finally, the buffer strategy retrieves more results than actually needed to account for this filtering process, ensuring we can always return the requested number of fresh recommendations even after removing items the user has already seen.\n\n\nAsynchronous Processing: Why async and await\nYou‚Äôll notice extensive use of async and await throughout our orchestrator code. This isn‚Äôt just a modern Python practice‚Äîit‚Äôs essential for building responsive APIs that handle multiple concurrent requests efficiently.\nOur recommendation pipeline involves multiple I/O operations including Redis lookups, HTTP calls to the model server, and Qdrant queries. Traditional synchronous code would block the entire process during each operation, severely limiting concurrency and creating bottlenecks that degrade user experience. With async/await, Python can handle hundreds of concurrent requests with a single thread, switching between requests during I/O waits. This dramatically reduces memory usage compared to thread-per-request models while ensuring that users don‚Äôt have to wait for other users‚Äô requests to complete‚Äîeach request is processed as quickly as possible without blocking others.\n# Multiple concurrent operations that can be performed in parallel\nasync with httpx.AsyncClient() as client:\n    response = await client.post(\n        f\"{self.seq_retriever_model_server_url}/{endpoint}\",\n        json=payload,\n        headers={\n            \"accept\": \"application/json\",\n            \"Content-Type\": \"application/json\",\n        },\n    )\n\n\nRequest Tracing: Debugging in Production\nProduction ML systems can be complex to debug when things go wrong. Our API implements comprehensive request tracing to help identify issues quickly:\n\n\napi/logging_utils.py\n\nclass RequestIDMiddleware(BaseHTTPMiddleware):\n    async def dispatch(self, request: Request, call_next):\n        rec_id = str(uuid.uuid4())\n        request.state.rec_id = rec_id\n\n        # Contextualize logger with the request ID\n        with logger.contextualize(rec_id=rec_id):\n            response = await call_next(request)\n\n        # Add rec_id to the output response\n        if response.headers.get(\"Content-Type\", \"\") == \"application/json\":\n            # ... add rec_id to response metadata\n\nUnique Request IDs: Every API request gets a unique identifier that‚Äôs included in all log messages and response metadata. This makes it possible to trace a single request‚Äôs journey through all system components.\nStructured Logging: Debug information is logged in structured formats that can be easily parsed and analyzed:\nlogger.debug(\n    f\"[COLLECT] Payload prepared: &lt;features&gt;{json.dumps(payload)}&lt;/features&gt;\"\n)\nResponse Enrichment: The middleware automatically adds request metadata to responses, providing clients with debugging information without cluttering the main API logic.\nThis tracing infrastructure becomes invaluable when debugging issues like:\n\nWhy did a specific user receive unexpected recommendations?\nWhich requests are taking longer than expected?\nAre there patterns in failing requests?\n\n\n\nReal-Time Context Integration\nAs we discussed in Chapter 1, our recommendation system uses a simplified request-payload approach for real-time context integration rather than complex streaming architectures. This design choice allows us to focus on the ML aspects while maintaining the benefits of real-time personalization. Let‚Äôs see how this works in practice within our serving architecture.\n\n\n\n\nExample API Payload showing real-time context\n\n\n\nThe API payload demonstrates the core implementation of this approach‚Äîrecent user interactions are included directly in each recommendation request through the item_seq_raw field, which contains items the user has interacted with in their current session. Our orchestrator then merges this real-time session data with historical user data retrieved from Redis, creating a comprehensive view of user intent that spans both immediate behavior and long-term preferences.\nThis implementation delivers the advantages we outlined in Chapter 1, starting with minimal signal delay since user interactions are captured client-side and sent immediately with recommendation requests, eliminating the latency of updating server-side state. The stateless nature of each request makes the system easier to scale and debug, as all necessary context is self-contained within the request payload. Additionally, users can continue browsing and building session context even when temporarily disconnected, as the frontend maintains the interaction history locally until the next API call."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#a-new-docker-compose-file",
    "href": "projects/implement-recsys/c7/index.html#a-new-docker-compose-file",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "A New Docker Compose File",
    "text": "A New Docker Compose File\nWith both our model server and orchestrator implemented, we need to deploy them as a coordinated system. Below is our new compose.api.yml:\n\n\ncompose.api.yml\n\nservices:\n  api:\n    container_name: api\n    build:\n      context: .\n      dockerfile: api/Dockerfile\n    ports:\n      - \"8000:8000\"\n    environment:\n      - REDIS_HOST=kv_store\n1      - SEQ_RETRIEVER_MODEL_SERVER_URL=http://seq_retriever_model_server:3000\n      - QDRANT_HOST=http://qdrant\n    command: [\"uvicorn\", \"api.app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--reload\"]\n\n  seq_retriever_model_server:\n    container_name: seq_retriever_model_server\n    build:\n      context: .\n      dockerfile: model_server/Dockerfile\n    ports:\n      - \"3000:3000\"\n    environment:\n      - MLFLOW_TRACKING_URI=http://mlflow_server:5000\n    entrypoint: [\"bentoml\", \"serve\", \"service:SeqRetrieverService\", \"--reload\"]\n\nnetworks:\n  recsys-mvp:\n    external: true\n\n\n1\n\nNote how our API orchestrator points to the model server by the container name seq_retriever_model_server and the port 3000."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#testing-the-complete-system",
    "href": "projects/implement-recsys/c7/index.html#testing-the-complete-system",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "Testing the Complete System",
    "text": "Testing the Complete System\nWith our API layer complete, we can test the entire recommendation pipeline end-to-end. Here‚Äôs how to start the complete system and verify everything works:\n# Start the base infrastructure (from Chapter 6)\nmake ml-platform-up\n\n# Start the API layer\nmake api-up\n\n# Test the personalized recommendations\ncurl -X 'POST' \\\n  'http://localhost:8000/recs/retrieve?count=2&debug=false' \\\n  -H 'accept: application/json' \\\n  -H 'Content-Type: application/json' \\\n  -d '{\n  \"user_ids_raw\": [\"AE224PFXAEAT66IXX43GRJSWHXCA\"],\n  \"item_seq_raw\": [\n    [\"0439064864\", \"043935806X\"]\n  ],\n  \"candidate_items_raw\": []\n}'\nThe system responds with structured recommendation data including items, scores, and metadata. Each response includes the request ID for tracing and debugging.\n{\n  \"recommendations\": [\n    {\n      \"score\": 0.6588092,\n      \"main_category\": \"Buy a Kindle\",\n      \"title\": \"Still Life with Crows (Pendergast Series Book 4)\",\n      \"average_rating\": 4.6,\n      \"rating_number\": 5120,\n      \"price\": \"None\",\n      \"subtitle\": \"Kindle Edition\",\n      \"image_url\": \"https://placehold.co/350x525/0057a3/ffffff.png?text=Still%0ALife%0Awith%0ACrows%0A%28Pendergast%0ASeries%0ABook%0A4%29&font=raleway\",\n      \"parent_asin\": \"B000Q9INGI\"\n    },\n    {\n      \"score\": 0.6399034,\n      \"main_category\": \"Buy a Kindle\",\n      \"title\": \"Blue Labyrinth (Pendergast Book 14)\",\n      \"average_rating\": 4.5,\n      \"rating_number\": 11969,\n      \"price\": \"9.99\",\n      \"subtitle\": \"Kindle Edition\",\n      \"image_url\": \"https://m.media-amazon.com/images/I/51HKSzxX3OL.jpg\",\n      \"parent_asin\": \"B00IRISI74\"\n    }\n  ]\n}"
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#recap",
    "href": "projects/implement-recsys/c7/index.html#recap",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we completed the journey from model training to model serving by building the API layer that transforms our sequence-based recommendation model into a system that can respond to real user requests in real-time.\nWe implemented a two-layer architecture that separates model inference concerns from business logic orchestration. The BentoML model server handles the computationally intensive neural network inference and seamless MLflow integration, while the FastAPI orchestrator coordinates the complete recommendation pipeline including user context retrieval, fallback logic, and result filtering. This separation enables independent scaling, version management, and technology optimization for each layer.\nThe serving pipeline successfully implements the simplified payload approach for real-time context integration that we outlined in Chapter 1, where user interactions are captured client-side and included directly in API requests. Combined with the three-pillar infrastructure from Chapter 6, our system can now fetch user sequences from Redis, generate query embeddings through the model server, perform vector similarity search in Qdrant, and gracefully fall back to popular recommendations when personalization isn‚Äôt possible‚Äîall within the response time requirements for production recommendation systems.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the api/ and model_server/ directories, along with the compose.api.yml file in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c7/index.html#final-step",
    "href": "projects/implement-recsys/c7/index.html#final-step",
    "title": "Implement a RecSys, Chapter 7: Building the API Layer",
    "section": "Final Step",
    "text": "Final Step\nLet‚Äôs not forget that in our code repository, we have a ui/ directory that contains the frontend demo that you saw in the introduction chapter.\nNow with all the pieces in place, refer to the project README.md &gt; Start UI section for how to get the frontend web application running. It‚Äôs a great way to conclude our project!\n\n\n\n\nThe Demo"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html",
    "href": "posts/deploy-ml-gcp/index.html",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "",
    "text": "This article is originally published in VinID Engineering\nImagine a company named Rainbow imports boxes of flowers and need to classify them into species. For six months, they have some staff label the boxes manually. Now, they hire you to build a Machine Learning model to do the task.\nWith a small amount of labelled data as input and tons of experience working on Kaggle projects, you quickly develop a 95% accuracy using simple RandomForestClassifier from the popular scikit-learn library. Nice. Stakeholders approve and ask you when you could deploy that model to production.\nHmm, deploy a model from my laptop?¬†‚Ä¶\nIn case you wonder, I hope this tutorial will help you understand one among some common and most simple approaches. The diagram below depicts how we will use Google Cloud Platform to do the job in a batch-processing manner.\nI choose the Iris data set as our input to help you see how our approach works with small-sized problems. All the codes are in this repo."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#introduction",
    "href": "posts/deploy-ml-gcp/index.html#introduction",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Introduction",
    "text": "Introduction\nLike many other self-taught data people, I am familiar with manipulating data and develop a model on my laptop.\nHowever, when you‚Äôre solving real-world problems, your duty does not stop after you deliver a presentation. You will have to think about how to bring that solution to the production environment.\nOver the last few months, I have tried to deploy multiple computing pipelines. They are different in their scopes and complexity, ranging from processing a dozen of MB to 400 GB data per run. In this article, I want to summarize and share what I learned."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#the-targeted-audience",
    "href": "posts/deploy-ml-gcp/index.html#the-targeted-audience",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "The targeted¬†audience",
    "text": "The targeted¬†audience\nThis post is for data analysts/scientists who want to deploy their local solution, especially those without a software engineering background.\nYou will need Cloud Dataproc to proceed. This product allows you to spin up a cluster of machines to run your computing job in a distributed manner. Please refer to this documentation if you don‚Äôt know what Dataproc is."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#agenda",
    "href": "posts/deploy-ml-gcp/index.html#agenda",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Agenda",
    "text": "Agenda\n\nDiscuss the approach\nStep-by-step instructions to create the infrastructure and run the pipeline\nExplain codebase\nIntroduce other extended components, including Big Data processing with Apache Spark, scheduler with Airflow, local development environment, unit testing\nSummary"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#approaches",
    "href": "posts/deploy-ml-gcp/index.html#approaches",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Approaches",
    "text": "Approaches\n\nAbout writing¬†codes\nInstead of writing a long script to do everything, we break a pipeline into tasks and checkpoint interim data to disk. For example, after doing preprocess on train and test data, we dump both the data outputs and the transformer to Google Cloud Storage. We then load those objects as inputs for the next step.\nThis strategy has several purposes. First, for a long-running task, if a job fails at one of the last steps, we can re-run the pipeline from the nearest checkpoint rather than wasting time and resources restarting the whole pipeline. Second, it allows us to (1) debug more easily, (2) get alert when things break and (3) monitor interim outputs. Lastly, decoupled components can be understood more clearly, and easier to be replaced or extended later.\n\n\nAbout computing resources\nNormally for a small input size, we are fine with setting up a single virtual machine on the cloud. However, in some companies with mature cloud practice, the overhead of managing that VM is a type of cost that is difficult to justify. Especially when we have better options. For instance, Cloud Dataproc provides us with virtual machines that only live for the duration of one run, thereby free us from managing the machines. In this post, we explore Dataproc as our main engine for all the computing process."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#step-by-step-instructions",
    "href": "posts/deploy-ml-gcp/index.html#step-by-step-instructions",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Step-by-step instructions",
    "text": "Step-by-step instructions\n\nCreate a GCP project and enable necessary components\n\nüëâ Create a free GCP account with $300 credit by going to console.cloud.google.com. Beware that by following this tutorial, you might incur a cost of about $0.2‚Äì$0.5.\n\n\n\n\n\nüëâ Click Billing at the left sidebar and initiate a billing account to be able to use the components used in this tutorial\n\n\n\n\n\nüëâ Select Library, then search and enable the following API: Cloud Dataproc, Cloud Storage and Cloud Firestore.\nüëâ Navigate to the Firestore either by scrolling the sidebar to the left or search from the top menu bar. When you arrive at the below screen, choose SELECT NATIVE MODE, then choose us-east1 as the location.\n\n\n\n\n\n\nEnvironment setup\n\nStep 1: Launch terminal window\n\nüëâ At the home page of your GCP project, select the command button to the right of your menubar. The CloudShell window then appears as you can see below:\n\n\n\n\n\nüëâ Launch Cloud Shell Editor:\n\n\n\n\n\n\nIt‚Äôs recommended to use Cloud Shell to follow this tutorial. However, if you‚Äôre using Linux and want to use terminal on your local machine, make sure you first install the Google Cloud SDK and firebase CLI.\n\n\nStep 2: Clone Github repo\n\nüëâ In the Terminal window:\n\ngit clonehttps://github.com/dvquy13/gcp_ml_pipeline.git\ncd gcp_ml_pipeline\n\nüëâ Select File then open the file gcp_ml_pipeline/configs/.project_env:\n\n\n\n\n\nüëâ Replace the values enclosed by &lt;&gt;. For the GCP_PROJECT, you need to provide the id of your GCP project. For the remaining, feel free to choose some random names for the global variables that identify your resources. The final output looks like this:\n\nGCP_PROJECT='zinc-primer-230105'\nGCS_BUCKET=dvquys-tut-gcp-ml-pipeline\nDATA_LOCATION=us-east1\nBQ_DATASET=tut_iris\nBQ_ORG_TABLE=F_ORIGINAL\nCLUSTER_NAME=iris-pred\n\nüëâ Grant execute permission to the folder scripts by running the command: chmod +x -R¬†./scripts. Then, run¬†./scripts/00_import_data_to_bigquery.sh. Link to the script.\n\n\n\nStep 3: Create Dataproc cluster and submit¬†jobs\nWe use Makefile to orchestrate our actions. You can find it here.\nÔ∏èNow, run the following commands in sequence:\n\nmake create-dataproc-cluster: This command creates a Dataproc cluster. The single-node flag indicates that this is a cluster containing only one machine. n1-standard-1 is the cheapest machine we can rent. To install Python packages, we supply the metadata and initialization-actions params.\nmake build: Package your code, including your source code and other 3rd party libraries that you can not pre-install when creating the cluster (PyYAML for example). To submit a job to the cluster, we will send these codes to those machines via the gcloud dataproc jobs submit pyspark command.\nmake submit-job ENV=dev MODULE=data_import TASK=query_train_pred: Submit job cloning input data for training and predicting. The submit-job make command allows you to use this interface to run on both local and development environments.\nmake submit-job ENV=dev MODULE=feature_engineer TASK=normalize: Prepare features. In this illustrative example, we choose to include only normalization in the pipeline. After learning the normalization parameters from the train data set, we save those configurations for later usage.\nmake submit-job ENV=dev MODULE=model TASK=fit: Train model. Here we build a pipeline consisting of 2 steps, Normalization and Logistic Regression. After that, we persist the fit pipeline.\nmake submit-job ENV=dev MODULE=predict TASK=batch_predict: Batch predict. This job demonstrates the process when you use your learned model to make predictions.\nmake submit-job ENV=dev MODULE=predict TASK=store_predictions: Store predictions. The reason we do not combine this with the above step is two-fold. First, writing to a database often takes time and requires several retries. Second, we write to a document database like Cloud Firestore because when other team uses, they typically retrieve one document per query. However, there are times when we want to inspect the whole batch of predictions (e.g.¬†debugging, count number of documents scored more than 0.9). For this query pattern, we will better off using the persisted outputs from the previous step, stored as parquet files in Cloud Storage.\nmake delete-dataproc-cluster: Delete Dataproc cluster. After the process finishes, delete the cluster so no further cost incurs.\n\n\n\n\nSucceeded Dataproc¬†jobs\nYou can see that your predictions are stored at Cloud Firestore by accessing its web console.\n\n\n\nFirestore populated with predictions\nAlong the way, you will see that the output data of each step is persisted in Cloud Storage. I use parquet rather than CSV as the serialization format because it can embed schema information (therefore you do not have to specify column types when reading) and reduce storage size. For more detail, please refer to this benchmark.\n\n\n\nClean up\n\nüëâ Finally, when you‚Äôre done exploring the results, you can delete all resources by running these commands:\n\n./scripts/01_erase_resources.sh\n./scripts/02_disable_resources.sh\n./scripts/03_delete_project.sh"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#explain-codebase",
    "href": "posts/deploy-ml-gcp/index.html#explain-codebase",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Explain codebase",
    "text": "Explain codebase\nscripts/: This directory contains some initial scripts, which are the steps to help you set things up. In practice, I also favor using script rather than user interfaces such as web console because it is self-documented and easy for others to follow the exact steps.\nconfigs/: Store all the arguments that need to be set initially.¬†.project_env is a file to store the global variables used to work with GCP. We also have the runtime.yaml, where we use Anchor, Alias and Extension in YAML to define runtime parameters for multiple environments. Both of these files serve as a centralized config store so that we can easily look up and make changes, instead of finding the configs scattered elsewhere in the code.\nMakefile: Originally Makefile is used to orchestrate the build process in C programming language. But it has done so well out of being just a shortcut so people start using it to facilitate ML model development. I have seen many tutorials using this tool, including the one that inspires me to design my Pyspark codebase.In this small project, we also use Makefile to save us a lot of time. As you can see above in Step 3, I put there our frequently used commands so that I can easily type make &lt;something&gt; to run a particular step.\niris_pred/: Source code.\nmain.py: is the interface to all tasks. This file parses the arguments to load config and get the job name, then call analyze function in entry_point.py from the appropriate module.\njobs/: contain tasks as modules. Inside jobs, we have one module corresponding to a step in our pipeline. All these modules expose an entry_point.py file where we unify the API to easily and consistently communicate with main.py.\n\n\nCode: train.py\n\n\n\niris_pred/jobs/model/train.py\n\nimport logging\nlogger = logging.getLogger(__name__)\nimport subprocess\n\nfrom shared.io_handler import IOHandler\nfrom jobs.feature_engineer.normalize import FeatureNormalizer\nfrom sklearn.externals import joblib\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\n\nclass Trainer:\n    def __init__(self, params, load: bool):\n        self.params = params\n\n        self.io_handler = IOHandler(params)\n        self.interim_output_path, self.final_output_path = \\\n            self._get_fpath()\n\n        self.normalizer = None\n        self.learner = None\n        self.pipeline = None\n\n    def _get_fpath(self):\n        interim_output_path = \\\n            f'../{self.params.io.pipeline}/pipeline.joblib'\n        final_output_path = \\\n            f'{self.io_handler.fpath_dict.pipeline}/pipeline.joblib'\n        return interim_output_path, final_output_path\n\n    def _load_train_data(self):\n        X_train = self.io_handler.load('X_train')\n        y_train = self.io_handler.load('y_train')['species']\n        return X_train, y_train\n\n    def _load_transformer(self):\n        normalizer_wrapper = FeatureNormalizer(self.params, load=True)\n        self.normalizer = normalizer_wrapper.normalizer\n\n    def _initiate_learner(self):\n        self.learner = LogisticRegression()\n\n    def _make_pipeline(self):\n        self.pipeline = make_pipeline(\n            self.normalizer,\n            self.learner)\n\n    def _fit(self, X_train, y_train):\n        self.pipeline.fit(X_train, y_train)\n\n    def _persist_pipeline(self):\n        # Temporarily save model to disk\n        joblib.dump(self.pipeline, self.interim_output_path)\n\n        # Copy model to GCS\n        if self.params.env_name != 'local':\n            logger.info(f\"Persisting {self.final_output_path}...\")\n            subprocess.check_output([\n                'gsutil', '-m', 'cp', '-r',\n                self.interim_output_path,\n                self.final_output_path])\n\n    def run(self):\n        X_train, y_train = self._load_train_data()\n        self._load_transformer()\n        self._initiate_learner()\n        self._make_pipeline()\n        self._fit(X_train, y_train)\n        self._persist_pipeline()\n\n\nAs you can see in the snippet above, the class Trainer expose a function run. Each step in the process corresponds to a private function declared in the same class.\nshared/: functions and classes to be reused across modules\nIn io_handler.py, the class IOHandler applies the principle Composition Over Inheritance to ease the process of loading outputs from the previous step."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#further-discussion",
    "href": "posts/deploy-ml-gcp/index.html#further-discussion",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Further discussion",
    "text": "Further discussion\nTo completely build and operate a pipeline, there is still more to be considered.\n\nApache Spark for bigger¬†data\nIn this tutorial, we rent one small machine from Dataproc and use pandas as our preprocessing engine, which perfectly handles the case of data fit into the memory of that machine. However, often data input in real-world situations will be much bigger, therefore require us to use a distributed computing framework for scalability. In that case, you can just switch to using Apache Spark. From version 1.3, Spark introduces its DataFrame API, which greatly bears resemblance to Pandas counterpart. After porting your code from Pandas to Spark, to be able to run jobs across multiple machines, you just need to create a bigger cluster with a master and multiple workers.\n\n\nApache Airflow for orchestration\nMost of the batch job is not ad hoc. If it is, we should not even think about putting effort to standardize the process in the first place. Apache Airflow can play the role of both a scheduler and a monitor. It keeps metadata of each run and can send you alerts when things fail.\n\n\n\n\nExample of Airflow DAG: Source\n\n\n\nAn alternative is Dataproc Workflows. This is a native solution offered by GCP, but I haven‚Äôt tried it myself so I will just leave the documentation here.\n\n\nLocal development\nBecause rarely our codes work the first time we write them, it‚Äôs very important to be able to quickly test without having to go through all the boilerplate steps from setting up variables to requesting cloud resources. My suggestion is that we should set up our local environment asap. We can install Apache Spark 2.4.3+ to act as our runner engine, and MongoDB to be our alternative for Cloud Firestore. Here in the code repo, you can still refer to some line containing what I call the ‚Äúenvironment branching logic‚Äù, which enables you to switch between running the same code on both local and cloud environments.\n\n\nUnit testing\nMany people have already talked about unit testing, so I won‚Äôt go too detailed here. I also don‚Äôt do unit testing in this tutorial for the sake of simplicity. However, I strongly encourage you to add testing yourself. Whatever it takes, unit testing forces us to modularize our code and add a layer of alerting. This is very important because things in data science often break in silence."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#summary",
    "href": "posts/deploy-ml-gcp/index.html#summary",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Summary",
    "text": "Summary\nHere is a summary of what you have learned in this tutorial:\n\nHow to utilize different Google Cloud Platform components to build a batch job pipeline (whether it involves ML or not).\nA product named Google Cloud Dataproc, where you can both submit a light-weight job via single-node mode and easily scale to a cluster of computers.\nOne approach to structurize ML pipeline codebase: Link to the repo.\nSome convenient components in model development, e.g.¬†Makefile, runtime config, parquet persistence. This mostly helps people with little or no software engineering background.\n\nAgain, one of my main goals in writing this article is to receive feedback from the community, so I can do my job better. Please feel free me leave me comments, and I hope you guys enjoy this tutorial."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#references",
    "href": "posts/deploy-ml-gcp/index.html#references",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "References",
    "text": "References\n\nRicky Kim. (Dec 2018). PySpark Sentiment Analysis on Google Dataproc. towardsdatascience.com. https://towardsdatascience.com/step-by-step-tutorial-pyspark-sentiment-analysis-on-google-dataproc-fef9bef46468.\nEvan Kampf. (Jan 2017). Best Practices Writing Production-Grade PySpark Jobs. developerzen.com. https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f.\nKing Chung Huang. (Oct 2017). Don‚Äôt Repeat Yourself with Anchors, Aliases and Extensions in Docker Compose Files. medium.com. https://medium.com/@kinghuang/docker-compose-anchors-aliases-extensions-a1e4105d70bd."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#credits",
    "href": "posts/deploy-ml-gcp/index.html#credits",
    "title": "From Model to Production: Deploying Your Machine Learning Solution on Google Cloud",
    "section": "Credits",
    "text": "Credits\nKudos to Bido for reviewing my work; to anh Khanh, anh Linh, anh Tuan for providing me feedback.\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Feb 2020). From Model to Production: Deploying Your Machine Learning Solution on Google Cloud. dvquys.com. https://dvquys.com/posts/deploy-ml-gcp/."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html",
    "href": "posts/site-search-elasticsearch/index.html",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "",
    "text": "This article is originally published in Towards Data Science"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#intro",
    "href": "posts/site-search-elasticsearch/index.html#intro",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Intro",
    "text": "Intro\nOver the last 22 months I have been working as a site-search engineer who uses Elasticsearch to help improve relevance in our restaurant platform. I have deployed in total 83 releases including 3 major versions.\nWith roughly one release per week, I can say that not only our search engine is much better than it was 2 years ago, but I have also learned quite a lot. Though still far from a great search engine, here are some things worth sharing in my opinion. More importantly, I want to get feedback about them.\nThis blog post is to provide an approach to design the Elasticsearch query template to deal with common site-search problems including searching for matches across different fields, boosting results and testing. Together we will identify issues with the default approach and then gradually come up with a new one to address the issues altogether.\nThis Github repo contains the examples and code discussed in this post."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#main",
    "href": "posts/site-search-elasticsearch/index.html#main",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Main",
    "text": "Main\nWe now play the role of a search engineer for a restaurant platform, which allows diners to discover and make reservation for their next meals. We haven‚Äôt had much experience, but luckily the app does not require accuracy level of Google from the start. The key is to make gradual visible progresses!\nAlright, let‚Äôs dive into it. First off, we make sure user can search for restaurant by name. Here we can rely on the simple default query-match to get the job done.\n\n\nCode: Index our first two restaurants\n\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"001sabichuong\" } }\n{ \"restaurant_name\": \"Sa Bi Chuong\", \"cuisine\": \"Vietnamese\", \"rating\": 5.0 }\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"002vietnamesephonoodle\" } }\n{ \"restaurant_name\": \"Vietnamese Pho Noodle\", \"cuisine\": \"Vietnamese\", \"rating\": 4.0 }\n\n# Test searching for one\n# Should return Vietnamese Pho Noodle\nGET restaurant/_search\n{\n  \"query\" : {\n    \"match\" : { \"restaurant_name\": \"vietnamese\" }\n  }\n}\n\nThe above snippet can be run at Kibana‚Äôs Dev Tools &gt; Console, which will be available at your localhost:5601 if you follow the repo.\nThe code is self-explained. We ask Elasticsearch to return restaurants whose name contains vietnamese. And we get back one result for Vietnamese Pho Noodle. No problems.\nBut we quickly find out that name is not the only place we want to look for when user submit a query. Given keywordvietnamese we should also return the restaurant Sa Bi Chuong, because it‚Äôs a Vietnamese restaurant as tagged in the cuisine. A multi_match query allows us to do exactly that.\n\n\nCode: Matching multiple fields\n\n# Should return all 2 Vietnamese restaurant with the Vietnamese Pho Noodle on top\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  }\n}\n\n\n\nResult\n\n\"hits\": {\n    ...\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.6931471,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.18232156,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      }\n    ]\n  }"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#problems-with-the-default-tfidf",
    "href": "posts/site-search-elasticsearch/index.html#problems-with-the-default-tfidf",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Problems with the default¬†TFIDF",
    "text": "Problems with the default¬†TFIDF\nNotice the above scores. The first one is like 4 times higher than the second, indicating that it‚Äôs much more relevant given query vietnamese. One might have an assumption that because matching at multiple fields will make the score higher.\nWhenever we have doubts, we can use Elasticsearch explain to get a detailed breakdown of its scoring components.\n\n\nCode: Let‚Äôs use explain=true to see what happens under the hood\n\n# Vietnamese Pho Noodle is on top because of the default implementation of TFIDF that penalizes the matching at cuisine field because there are multiple restaurants with cuisine=Vietnamese while there are only one restaurant with name=Vietnamese\n# Question: But why having the name Vietnamese in its name makes it more Vietnamese than other restaurants?\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  },\n  \"explain\": true\n}\n# Result üëá\n\"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.6931471,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        },\n        \"_explanation\": {\n          \"value\": 0.6931471,\n          \"description\": \"max of:\",\n          \"details\": [\n            # Matching in field `cuisine` yields score=0.18\n            # Note that by default the score is calculated by TFIDF\n            # More info about Elasticsearch TFIDF: https://www.elastic.co/guide/en/elasticsearch/reference/8.6/index-modules-similarity.html#bm25\n            {\n              \"value\": 0.18232156,\n              \"description\": \"weight(cuisine:vietnamese in 1) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            },\n            # Matching in field `restaurant_name` yields score=0.69\n            {\n              \"value\": 0.6931471,\n              \"description\": \"weight(restaurant_name:vietnamese in 1) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            }\n            # Because the final score is \"max of:\" those two above scores,\n            # it is equal to the matching score with `restaurant_name`\n          ]\n        }\n      },\n      {\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.18232156,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        },\n        # Similarly since there's no matching with `restaurant_name`,\n        # here the final score is equal to the matching score of `cuisine`\n        \"_explanation\": {\n          \"value\": 0.18232156,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.18232156,\n              \"description\": \"weight(cuisine:vietnamese in 0) [PerFieldSimilarity], result of:\",\n              \"details\": [...]\n            }\n          ]\n        }\n      }\n    ]\n  }\n\nAbove we can see that Vietnamese Pho Noodle is on top because of the default implementation of TFIDF that penalizes the matching at cuisine field because there are multiple restaurants with cuisine=Vietnamese while there are only one restaurant with name=Vietnamese.\nDiving into the _explanation block, we realize that score difference originates from the TFIDF matching output for restaurant_name. This is expected as the algorithm assumes that a keyword is a better signal if it is not common and usually found in a lot of documents (sort of a solution to automatically handle stopwords). In our examples, both the restaurants have cuisine Vietnamese so according to TFIDF, that match does not say much about the relevance of the documents.\nWhether we should encourage this behavior is a question. Is it true that having Vietnamese in the name make one restaurant more ‚ÄúVietnamese‚Äù than the other?\nAnother problem with TFIDF is that it takes into account the length of the field.\n\n\nCode: Let‚Äôs add another restaurant\n\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"003vietnamesepho\" } }\n{ \"restaurant_name\": \"Vietnamese Pho\", \"cuisine\": \"Vietnamese\", \"rating\": 3.0 }\n\n# In the below example we see that the new Vietnamese Pho restaurant is ranked higher...\nGET restaurant/_search\n{\n  \"query\" : {\n    \"multi_match\" : {\n      \"query\": \"vietnamese pho\",\n      \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n    }\n  },\n  \"explain\": true\n}\n\nYou can find the detailed and lengthy result in Appendix 1. In short, we see that the result ranks restaurant Vietnamese Pho first and then Vietnamese Pho Noodle. Analyzing the component scores indicates that the key difference is that Vietnamese Pho has length=2 (words) while Vietnamese Pho Noodle has length=3. It feels unintuitive since we know that the second restaurant has higher rating, given that both, in practice, are equally matching to user‚Äôs keyword."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#reranking-boosting-with-function_score",
    "href": "posts/site-search-elasticsearch/index.html#reranking-boosting-with-function_score",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Reranking (boosting) with function_score",
    "text": "Reranking (boosting) with function_score\nAs we talk about rating, we can wrap our query with function_score to incorporate that information to modify our matching scores, hence have a better control over our ranking.\n\n\nCode: Function score\n\nGET restaurant/_search\n{\n  \"query\": {\n    \"function_score\": {\n      # Our main query is wrapped in a function_score clause\n      \"query\": {\n        \"multi_match\" : {\n          \"query\": \"vietnamese\",\n          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n        }\n      },\n      # We define the functions that will be applied on top of the matching scores\n      # returned by our main query\n      \"functions\": [\n        {\n          \"field_value_factor\": {\n            \"field\": \"rating\",\n            \"modifier\": \"none\",\n            \"missing\": 1\n          }\n        }\n      ],\n      # Retrieve the max boosting defined inside `functions`\n      # Above there is only one boosting so it's applied by default\n      \"score_mode\": \"max\",\n      # Multiply the matching score with the boosting calculated from functions\n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 1.7885544,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 1.5706451,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 0.66765696,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      }\n    ]\n  }\n}\n\nThe higher rating restaurant is on top now. But how about restaurant Sa Bi Chuong with rating=5? It being the last result seems like we haven‚Äôt boosted ‚Äúenough‚Äù.\nWe might start tinkering a bit more with function_score to make that happen. Here is one of the implementation which models the boosting in a non-linear manner to effectively apply a strong boost on documents with rating=5.\n\n\nCode: Non-linear function score boosting\n\nGET restaurant/_search\n{\n  \"query\": {\n    \"function_score\": {\n      \"query\": {\n        \"multi_match\" : {\n          \"query\": \"vietnamese\",\n          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n        }\n      },\n      \"functions\": [\n        # Apply a non-linear function to model that\n        # a rating of 5 has much more weight than rating of 4 (not just 25% more)\n        {\n          \"filter\": {\n            \"range\": {\n              \"rating\": {\n                \"gte\": 5,\n                \"lte\": 5\n              }\n            }\n          },\n          \"weight\": 10\n        },\n        {\n          \"filter\": {\n            \"range\": {\n              \"rating\": {\n                \"gte\": 4,\n                \"lt\": 5\n              }\n            }\n          },\n          \"weight\": 2\n        }\n      ],\n      \"score_mode\": \"max\",\n      \"boost_mode\": \"multiply\"\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"001sabichuong\",\n        \"_score\": 1.3353139,\n        \"_source\": {\n          \"restaurant_name\": \"Sa Bi Chuong\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 5\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.8942772,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 0.52354836,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        }\n      }\n    ]\n  }\n}\n\nYou may wonder that: ‚ÄúIsn‚Äôt the function boosting now looking too arbitrary? Will it work for other cases?‚Äù. Indeed, that‚Äôs the question we should ask ourselves. Overtime, with more and more requirements, our query template will grow in complexity, leading to conflicts between the modifications we make.\nLet‚Äôs move to the next example to illustrate what I mean by ‚Äúconflict‚Äù."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#the-complexity-comes-with-fuzzy-matching",
    "href": "posts/site-search-elasticsearch/index.html#the-complexity-comes-with-fuzzy-matching",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "The complexity comes with fuzzy¬†matching",
    "text": "The complexity comes with fuzzy¬†matching\nWhile not vital, the ability to handle user‚Äôs typo is always a nice-to-have feature, especially when they are now familiar with smart search engine like Google‚Äôs. Elasticsearch has a built-in mechanism called fuzzy matching, which is configurable with the option fuzziness.\n\n\nCode: fuzzy matching\n\n# The use of `bool` query below is to implement the logic: At least one condition should match\nPUT _scripts/01-default-fuzzy-search-template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"function_score\": {\n          \"query\": {\n            \"bool\": {\n              \"must\": [\n                {\n                  \"bool\": {\n                    \"should\": [\n                      {\n                        \"multi_match\" : {\n                          \"query\": \"{{query_string}}\",\n                          \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n                        }\n                      },\n                      {\n                        \"multi_match\" : {\n                          \"query\": \"{{query_string}}\",\n                          \"fields\": [ \"restaurant_name\", \"cuisine\" ],\n                          # For the purpose of this demo, default behavior works well enough\n                          \"fuzziness\": \"AUTO\"\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          },\n          \"functions\": [\n            {\n              \"filter\": {\n                \"range\": {\n                  \"rating\": {\n                    \"gte\": 5,\n                    \"lte\": 5\n                  }\n                }\n              },\n              \"weight\": 10\n            },\n            {\n              \"filter\": {\n                \"range\": {\n                  \"rating\": {\n                    \"gte\": 4,\n                    \"lt\": 5\n                  }\n                }\n              },\n              \"weight\": 2\n            }\n          ],\n          \"score_mode\": \"max\",\n          \"boost_mode\": \"multiply\"\n        }\n      }\n    },\n    \"params\": {\n      \"query_string\": \"My query string\"\n    }\n  }\n}\n\nNotice that we just created a query template instead of running a query. We can now invoke the query with paramaters, which is a nice feature Elasticsearch introduces to make our code look less overwhelming. Like this:\nGET /_search/template\n{\n  \"id\": \"01-default-fuzzy-search-template\",\n  \"params\": {\n    \"query_string\": \"vietnames\"\n  }\n}\nThe above query returns our expected Vietnamese restaurant given a typo keyword vietnames. Under the hood, fuzzy matching uses Levenshtein edit distance, which measures similarity between strings by the number of modifications one make to make one become another. In our example, we just need to add one letter e at the end to make vietnames become vietnamese. Quite an easy task for the algorithm. One might also argue that it‚Äôs quite easy for our developers as well. 2 lines of code and a new beautiful feature.\nWell, the interesting bit lies elsewhere. One day, our sales team suddenly comes to us with a complaint that search result is wrong. People are getting Japanese BBQ restaurants over Korean ones even when they explicitly search for kbbq (which is a common acronym for korean bbq).\nHere are the restaurants:\nPOST _bulk\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"004parkhangseokbbq\" } }\n{ \"restaurant_name\": \"Park Hang-seo's KBBQ\", \"cuisine\": \"Korean\", \"rating\": 2.0 }\n{ \"index\" : { \"_index\" : \"restaurant\", \"_id\" : \"005bestbbqintown\" } }\n{ \"restaurant_name\": \"Best BBQ in town\", \"cuisine\": \"Japanese\", \"rating\": 5.0 }\nQuery:\n{\n  \"id\": \"01-default-fuzzy-search-template\",\n  \"params\": {\n    \"query_string\": \"kbbq\"\n  }\n}\n\n\nResult\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"005bestbbqintown\",\n        \"_score\": 8.384459,\n        \"_source\": {\n          \"restaurant_name\": \"Best BBQ in town\",\n          \"cuisine\": \"Japanese\",\n          \"rating\": 5\n        }\n      },\n      {\n        \"_index\": \"restaurant\",\n        \"_id\": \"004parkhangseokbbq\",\n        \"_score\": 2.5153382,\n        \"_source\": {\n          \"restaurant_name\": \"Park Hang-seo's KBBQ\",\n          \"cuisine\": \"Korean\",\n          \"rating\": 2\n        }\n      }\n    ]\n  }\n}\n\nTo understand what is happening, we need to enable explain=true to see what contributes to the final scores. As this time the output is too verbose, here are the findings:\n\nThe keyword matching score (before boosting) for the Best BBQ in town restaurant is 0.8, less than the 1.2 of Park Hang-seo's KBBQ\nSo if no boosting applied, we will see Park Hang-seo's KBBQ restaurant ranks at the first position\nBut then the boosting from rating modifies the score, leading to the ordering as we can see\n\nOne way to frame the issue is that we have imperfect boosting. Say we have a better formula that strikes the right balances, then the problem should be solved. But it‚Äôs close to impossible to guarantee that the new formula will not cause any other issues. We don‚Äôt want these kinds of issue creep into the system without any notice and then some day being flagged out by stakeholders. We want to be the first to be aware of those issues, especially whenever we make any changes. Therefore, before discussing potential solutions, I hope we all agree that the very next important thing we should do is (yes, you are probably thinking about the same thing as I am) setting up a testing/evaluation mechanism."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#how-should-we-create-test-cases-for-this-search-application",
    "href": "posts/site-search-elasticsearch/index.html#how-should-we-create-test-cases-for-this-search-application",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "How should we create test cases for this search application?",
    "text": "How should we create test cases for this search application?\nIMHO, the first challenge is about moving data. The queries and the documents can both grow over time, so a static mock dataset might not be a very good representative of the search relevance anymore after a month. The next bit is related to our mindset. Sometimes we might need to think about whether we need 100% passed test cases in order to fix this new very urgent issue. For example, there are cases where if you fix some issues then the search result orderings of the other test cases might alter a bit. If we hard-code the rankings, then we might sweat ourselves trying to tweak our query template. But in practice a lot of the times we neither don‚Äôt need the ranking to be exactly pre-defined nor we are perfectly sure about which ordering is actually optimal. We should consider using a soft mechanism where we quantify the relevance of the system and using threshold instead.\nHere we look at how we can use Elasticsearch Ranking Evaluation API to implement such evaluation scheme:\n\n\nCode: Evaluation API\n\nGET restaurant/_rank_eval\n{\n  # Query template comes in really handy when used in conjunction with _rank_eval\n  \"templates\": [\n     {\n        \"id\": \"01-default-fuzzy-search-template\",\n        \"template\": {\n            \"id\": \"01-default-fuzzy-search-template\"\n        }\n     }\n  ],\n  \"requests\": [\n    {\n     \"id\": \"kbbq_query\",\n     # Here we manually define the true positives with rating &gt;= 1.0\n     # The actual rating number helps when using metrics that takes into account\n     # the ranking of the search results\n     \"ratings\": [\n        { \"_index\": \"restaurant\", \"_id\": \"004parkhangseokbbq\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"005bestbbqintown\", \"rating\": 1 }\n      ],\n     \"template_id\": \"01-default-fuzzy-search-template\",\n     \"params\": {\n        \"query_string\": \"kbbq\"\n      }\n    },\n    {\n     \"id\": \"vietnamese_query\",\n     \"ratings\": [\n        { \"_index\": \"restaurant\", \"_id\": \"001sabichuong\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"002vietnamesephonoodle\", \"rating\": 3 },\n        { \"_index\": \"restaurant\", \"_id\": \"003vietnamesepho\", \"rating\": 3 }\n      ],\n     \"template_id\": \"01-default-fuzzy-search-template\",\n     \"params\": {\n        \"query_string\": \"vietnamese\"\n      }\n    }\n  ],\n  \"metric\": {\n    \"dcg\": {\n      \"k\": 5,\n      \"normalize\": true\n    }\n  }\n}\n\n\n\nResult\n\n{\n  \"metric_score\": 0.8549048706984328,  # This is the overall metric score, best is 1.0, worst is 0.0\n  \"details\": {\n    \"kbbq_query\": {\n      # This kbbq_query has a imperfect score because it ranks the more relevant result lower\n      \"metric_score\": 0.7098097413968655,\n      \"unrated_docs\": [],\n      \"hits\": [\n        {\n          \"hit\": {\n            \"_index\": \"restaurant\",\n            \"_id\": \"005bestbbqintown\",\n            \"_score\": 8.384459\n          },\n          \"rating\": 1\n        },\n        {\n          \"hit\": {\n            \"_index\": \"restaurant\",\n            \"_id\": \"004parkhangseokbbq\",\n            \"_score\": 2.5153382\n          },\n          \"rating\": 3\n        }\n      ],\n      \"metric_details\": {\n        ...\n      }\n    },\n    \"vietnamese_query\": {\n      \"metric_score\": 1,\n      \"unrated_docs\": [],\n      \"hits\": [\n        ...\n      ],\n      \"metric_details\": {\n        ...\n      }\n    }\n  },\n  \"failures\": {}\n}\n\nLet‚Äôs try to better our search by introducing changes that move the evaluation score closer to the perfect 1.0."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#our-revised-search-model",
    "href": "posts/site-search-elasticsearch/index.html#our-revised-search-model",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Our revised search¬†model",
    "text": "Our revised search¬†model\nBefore start designing a new query template, we can take a step back and really think about how we should model the search engine. Below are the essentials:\n\nExact matching will always surface on top of not-exact ones like fuzzy matching;\nExact matches does not take into account field length or word/document frequencies. If two documents have the same exact match in a field, they should have the same keyword matching score;\nWithin the same level of matching (whether exact or fuzzy), while the initial keyword matching scores should be the same, they can be reranked by certain modifiers such as distance, popularity,¬†‚Ä¶ However, the modified scores should not make the final score to exceed the base score of the upper level, e.g.¬†modifed fuzzy score should not be greater than exact base score. This is to ensure the essential #1.\n\nIf you watch football, this is similar to how the leagues such as Premiere League rank their teams. No matter how much more goals the team L has scored compared to team M‚Äôs or their head-to-head results, if team M has more points than team M has a higher ranking. The other measures are for tie-breaker only.\nThis understanding can be then transferred to how we use Elasticsearch to express our model.\nOne approach is to use dis_max query combined with constant_score query. The idea is to categorize each type of matching into different levels of score where one level will have twice the score of the below level. The documents fall into one level of matching (tie) will be reranked by modifiers but eventually the new scores will not exceed the upper base score. Here is the new query template:\n\n\nCode: New query template\n\nPUT _scripts/02-constant-score-search-template\n{\n  \"script\": {\n    \"lang\": \"mustache\",\n    \"source\": {\n      \"query\": {\n        \"function_score\": {\n          \"query\": {\n            \"bool\": {\n              \"must\": [\n                {\n                  \"bool\": {\n                    \"should\": [\n                      {\n                        # `dis_max` query gets the max score of an array of clauses\n                        \"dis_max\": {\n                          \"queries\": [\n                            {\n                              # `constant_score` says that if matches, return a constant score\n                              \"constant_score\": {\n                                \"filter\": {\n                                  \"multi_match\" : {\n                                    \"query\": \"{{query_string}}\",\n                                    \"fields\": [ \"restaurant_name\", \"cuisine\" ]\n                                  }\n                                },\n                                # This is the constant that is returned as score\n                                # Note that the exact number is chosen intentionally\n                                # Here the upper level will be twice the lower level\n                                # and we will restrict the modifiers to be only\n                                # able to boost by at most 100% the base score\n                                # so that the lower level can not exceed the upper\n                                \"boost\": 2\n                              }\n                            },\n                            {\n                              \"constant_score\": {\n                                \"filter\": {\n                                  \"multi_match\" : {\n                                    \"query\": \"{{query_string}}\",\n                                    \"fields\": [ \"restaurant_name\", \"cuisine\" ],\n                                    \"fuzziness\": \"AUTO\"\n                                  }\n                                },\n                                \"boost\": 1\n                              }\n                            }\n                          ]\n                        }\n                      }\n                    ]\n                  }\n                }\n              ]\n            }\n          },\n          \"functions\": [\n            # Design the modifiers to be multiplier of maximum 1.9999 the base score\n            {\n              \"weight\": 1\n            },\n            {\n              \"field_value_factor\": {\n                \"field\": \"rating\",\n                \"modifier\": \"ln\",\n                \"missing\": 1\n              },\n              \"weight\": 0.1\n            }\n          ],\n          \"score_mode\": \"sum\",\n          \"boost_mode\": \"multiply\"\n        }\n      }\n    },\n    \"params\": {\n      \"query_string\": \"My query string\"\n    }\n  }\n}\n\nWhen we re-run the evaluation, we can observe that the normalized DCG metric now has score equal to 1.0, denoting a perfect accuracy!"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#summary",
    "href": "posts/site-search-elasticsearch/index.html#summary",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Summary",
    "text": "Summary\nThis blog post focuses on putting you in the shoe of an Elasticsearch engineer who has to derive query templates that fit the needs of a site-search enginer. We have briefly coverred the following topics:\n\nKeyword matching with multiple fields\nUnderstanding default Elasticsearch scoring\nProblems with the default TFIDF\nBoosting search results by attributes\nFuzzy matching\nElasticsearch query templateEvaluation with Rank Evaluation API\nConstructing query with dis_maxand constant_score\n\nThough definitely not optimal, I hope that parts of the blog post help you come closer to utilize Elasticsearch to help solve your own problems.\nI also much appreciate any comments or feedbacks. If you want to discuss more, please comment on this post or open an issue in the Github repo.\nThanks all!"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#appendix",
    "href": "posts/site-search-elasticsearch/index.html#appendix",
    "title": "A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration",
    "section": "Appendix",
    "text": "Appendix\n\nAppendix 1: Detailed breakdown of default TFIDF matching where length of the field value affect overall matching¬†score\n\n\nResults\n\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 1.0470967,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        },\n        \"_explanation\": {\n          \"value\": 1.0470967,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.13353139,\n                  \"description\": \"weight(cuisine:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            },\n            {\n              \"value\": 1.0470967,\n              \"description\": \"sum of:\",\n              \"details\": [\n                # Matching score with \"vietnamese\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.52354836,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        {\n                          \"value\": 2.2,\n                          \"description\": \"boost\",\n                          \"details\": []\n                        },\n                        {\n                          \"value\": 0.47000363,\n                          \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\n                          ...\n                        },\n                        {\n                          \"value\": 0.50632906,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            {\n                              \"value\": 1,\n                              \"description\": \"freq, occurrences of term within document\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 1.2,\n                              \"description\": \"k1, term saturation parameter\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 0.75,\n                              \"description\": \"b, length normalization parameter\",\n                              \"details\": []\n                            },\n                            # Notice the length=2 here is in the denominator,\n                            # which means that the higher the length the less\n                            # the score\n                            {\n                              \"value\": 2,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 2.6666667,\n                              \"description\": \"avgdl, average length of field\",\n                              \"details\": []\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                # Matching score with \"pho\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:pho in 0) [PerFieldSimilarity], result of:\",\n                  # Details are exactly like above\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.8942772,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        },\n        \"_explanation\": {\n          \"value\": 0.8942772,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [...]\n            },\n            {\n              \"value\": 0.8942772,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:vietnamese in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.4471386,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        ...,\n                        {\n                          \"value\": 0.4324324,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            ...,\n                            # Here the length=3 (greater than length=2 of the\n                            # above restaurant)\n                            {\n                              \"value\": 3,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            ...\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:pho in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2023). A Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration. dvquys.com. https://dvquys.com/posts/site-search-elasticsearch/."
  },
  {
    "objectID": "projects/review-rec-bot/index.html",
    "href": "projects/review-rec-bot/index.html",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "",
    "text": "Project Details"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#demo",
    "href": "projects/review-rec-bot/index.html#demo",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Demo",
    "text": "Demo"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#problem-statement",
    "href": "projects/review-rec-bot/index.html#problem-statement",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Problem Statement",
    "text": "Problem Statement\nListing/reservation businesses like Yelp offer value to users by providing useful information to make them find out where to go next. Good search and recommendation systems go a long way, but they are still far from delivering the ultimate experience where users can interact naturally with the system for complex queries or have a conversation to drill down their needs."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#approach",
    "href": "projects/review-rec-bot/index.html#approach",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Approach",
    "text": "Approach\nBuild a chatbot assistant to assist users in discovering places to go and booking.\nWorkflow:\n\nDownload Yelp reviews data. Sample 5,240 reviews from 100 businesses.\nSet up development environment including experimentation tracking via MLflow, observability with Arize Phoenix\nBuild MVP version using LlamaIndex, Qdrant\nBuild synthetic evaluation datasets with 30 questions for retrieval and response. Manual response dataset are gradually built up and added based on error analysis\nConduct error analysis and look at the model‚Äôs output to come up with new iteration ideas. Run a total of 10 experiments to improve the RAG, with notable attempts including: Replacing Llama-8.1B with GPT-4o-mini, Fine-tuning Embedding Model, Hybrid Retrievers, Semantic Chunking, BGE Reranker, Query Expansion\nBuild RAG Agent based on OpenAI API with Query Engine tool and Reservation Service tool. Chatbot UI is built with Chainlit."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#evaluation",
    "href": "projects/review-rec-bot/index.html#evaluation",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Evaluation",
    "text": "Evaluation\nEvaluation results can be found here.\nTwo proposed key metrics are Retrieval Hit Rate and Response Correctness.\nRetrieval is a critical component in any RAG system. Along with data prep, retrieval sits at the top of the pipeline so any improvements in these fronts is more likely to improve the overall system. Hit rate is chosen as a key metric because since we can employ rerank as a subsequent step, we have room to optimize for the ranking issues.¬†\nFor response, Correctness measures both how relevant the answer is with respect to the query and how correct it is compared to the referenced answer. It‚Äôs therefore a better indicator than pure relevance, which is just based on the query and hence easier to get right.\nFor reference, Response Correctness on synthetic dataset has improved +166% from 1.75 / 5.00 from MVP version to 4.67 / 5.00 on the current version. The current Retrieval Hit Rate @ 50 reaches 73%, not directly comparable but at MVP version Retrieval Hit Rate @ 10 was 20%.\nAs next steps, while there is not much room to improve Response Correctness, we ought to increase Retrieval Hit Rate to 90% which should be doable since this dataset only contains a small amount of data."
  },
  {
    "objectID": "projects/review-rec-bot/index.html#learningsremarks",
    "href": "projects/review-rec-bot/index.html#learningsremarks",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Learnings/Remarks",
    "text": "Learnings/Remarks\n\nUsing question-style query leads to 5-20% uplift in retrieval hit rate compared to using keyword search\nBM25 Retriever alone results in 200% increase in retrieval effectiveness including hit rate, average precision, MRR and NDCG\nFine-tuning small embedding model like Snowflake/snowflake-arctic-embed-m-v1.5 yield +80% retrieval effectiveness, especially rankings of the retrieved nodes\nUsing GPT-4o-mini as response synthesizer significantly improve the quality of response in all aspects (especially correctness from 2.6 to 3.8) compared to Llama 3.1-8B-Instruct\nUsing TreeSummarize with custom prompt yields a +10% uplift on response correctness evaluation, from 3.97 to 4.37. Based on eyeballing we also see a way better response that is recommendation-like"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#challenges",
    "href": "projects/review-rec-bot/index.html#challenges",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Challenges",
    "text": "Challenges\n\nChallenge 1: Auto-retrieval not reliable\nWhile theoretically both precision and recall should be greatly improved if we are able to apply the right filters for User questions instead of relying on embedding/keyword matching, my first attempt to apply auto-retrieval with ChromaDB did not yield promising results. There were at least two syntactic issues which broke the agentic workflow. Even after fixing those two the unreliable nature of this approach is still there and I also witnessed a -10% degradation in Retrieval Hit Rate.\nIn the end I forfeited the feature but nevertheless look forward to a way to re-applying this technique.\n\n\nChallenges 2: Indexing pipeline takes too long\nIndexing 70K nodes from 30K reviews for 400 businesses takes more than 6 hours!"
  },
  {
    "objectID": "projects/review-rec-bot/index.html#future-improvements",
    "href": "projects/review-rec-bot/index.html#future-improvements",
    "title": "Building a Conversational Assistant for Restaurant Discovery and Booking",
    "section": "Future Improvements",
    "text": "Future Improvements\n\nGuardrail system inputs and outputs\nExperiment with Contextual Compression and Filters\nFine tune LLM Re-ranker (FlagEmbedding BGE Reranker)\nTry ColBERT as a new retriever (may be add to the list of retrievers)\nTry different loss function in training embeddings\nImprove the diversity by implement custom re-ranker that weight downs the reviews from the already seen biz_id\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Sep 2024). Building a Conversational Assistant for Restaurant Discovery and Booking. dvquys.com. https://dvquys.com/projects/review-rec-bot/."
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html",
    "href": "projects/implement-recsys/c1/index.html",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "",
    "text": "This is the first chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#introduction",
    "href": "projects/implement-recsys/c1/index.html#introduction",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Introduction",
    "text": "Introduction\nImagine you‚Äôre browsing an e-commerce site and notice something very interesting: the recommendations update almost instantly as you click through products. You view a pair of headphones, and suddenly phone cases, audio cables, and portable chargers appear. You check out a laptop, and the ‚Äúyou might also like‚Äù section refreshes with wireless mice, laptop bags, and USB hubs. You think to yourself‚ÄîHow do they capture your evolving intent so quickly? How do they serve personalized suggestions fast enough to feel real-time? How do they handle the complexity of understanding your browsing patterns on the fly?\nIf you‚Äôve ever have those questions, this tutorial series is for you.\nOver the next 7 chapters, I‚Äôll walk you through building an end-to-end recommendation system that reacts in real-time based on user behavior sequences. We‚Äôll go from raw data to a complete recommender system that personalizes recommendations as users interact with your platform. The final product looks like this:\n\n\n\n\nThe Demo\n\n\n\n\n\n\n\n\n\nEnough Talking, Show Me the Code!\n\n\n\nIf you‚Äôre familiar with RecSys and just want to see the code, here it is: https://github.com/dvquy13/recsys-seq-model.\n\n\nA quick recap for those who aren‚Äôt well aware of what a recommender system is: Recommender systems are specialized tools designed to suggest items‚Äîwhether products, content, or services‚Äîto users based on what we know about their preferences and behaviors. At their core, they comprise a set of technologies, algorithms, and processes that analyze user interaction signals (clicks, views, purchases, ratings, etc.) to predict what individual users might find relevant or engaging.\n\n\n\nFrom the user‚Äôs perspective, these systems help uncover content or products they might never have discovered on their own and save time by surfacing the most relevant options up front.\nFrom a business standpoint, the primary goals of a recommender system are to create a sense of ‚Äúwe understand you‚Äù for each user and to drive revenue through personalized cross-selling opportunities. By tailoring recommendations to each user, companies can boost engagement and average order value.\nIn practice, recommender systems power some of the world‚Äôs largest digital platforms. For example, Amazon‚Äôs ‚ÄúFrequently Bought Together‚Äù suggestions guide shoppers toward complementary products, Facebook uses recommendation algorithms to prioritize posts and ads in users‚Äô feeds, and Netflix‚Äôs home screen is largely driven by personalized movie and show recommendations.\n\n\n\nThe impact of these systems is profound: According to this report from McKinsey, roughly 75 percent of Netflix viewing hours come from recommended titles, 35 percent of Amazon‚Äôs purchases are influenced by its recommendations, and Netflix alone saves around $1 billion per year through improved user retention and engagement. These metrics underscore why recommender systems have become a critical component of modern digital businesses."
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#what-youll-build",
    "href": "projects/implement-recsys/c1/index.html#what-youll-build",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "What You‚Äôll Build",
    "text": "What You‚Äôll Build\nBy the end of this series, you‚Äôll have constructed an end-to-end recommendation system while knowing how to:\n\nDesign and Train Session-based RecSys models to personalize recommendations in real-time based on users‚Äô recent interaction sequences\nHandle complex data engineering challenges including sampling techniques that avoid sparsity cascade, sequence generation from timestamps, and popularity-based negative sampling\nTrack experiments with MLflow for reproducible ML workflows, automated model versioning, and systematic performance comparison\nImplement a PyTorch sequential retrieval architecture optimized for serving millions of items with millisecond response times\nDeploy model servers using BentoML with seamless MLflow integration\nBuild recommendation serving pipelines through a FastAPI orchestrator that coordinates vector similarity search, user context retrieval, and fallback strategiesz\nMimic production infrastructure setup with Docker Compose orchestrating MLflow (model registry), Qdrant (vector database), and Redis (key-value store)"
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#target-audience",
    "href": "projects/implement-recsys/c1/index.html#target-audience",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Target Audience",
    "text": "Target Audience\nThis tutorial series is designed for:\n\nData Scientists looking beyond training models and into ML services\nML Engineers building scalable recommendation systems\nBackend Developers interested in ML/RecSys architecture\nAnyone curious about a modern and real-time end-to-end RecSys project\n\nIn most of the discussions, I assume readers have basic understanding of Machine Learning and Python to follow along."
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#session-based-vs-traditional-recommendations",
    "href": "projects/implement-recsys/c1/index.html#session-based-vs-traditional-recommendations",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Session-Based vs Traditional Recommendations",
    "text": "Session-Based vs Traditional Recommendations\nTraditional recommendation systems often rely on collaborative filtering, which uses historical user-item interaction signals to find similar users or items. While effective, these approaches have limitations:\n\nStatic representations: User preferences are treated as fixed\nCold start problems: Difficulty with new users or items\nLimited temporal understanding: Doesn‚Äôt capture evolving user interests within a session\n\nSession-based recommendation systems take a fundamentally different approach by focusing on user behavior patterns within individual sessions or short time windows. Instead of building static user profiles from historical data, these systems analyze the sequence of actions a user takes during their current browsing session‚Äîthe order they click through products, the time they spend on each item, and how their interests evolve in real-time.\nConsider a typical e-commerce browsing session that illustrates why this sequential understanding matters:\n\nUser searches for ‚Äúwireless headphones‚Äù\nViews a Sony model\nChecks reviews for Audio-Technica alternatives\nCompares prices across brands\n\nEach step provides context for the next recommendation. Traditional systems might miss this sequential pattern and treat each interaction independently, while session-based models capture the evolving intent throughout the entire journey‚Äîunderstanding that the user is comparison shopping for headphones within a specific price range.\n\n\n\n\nThe signals we can leverage to deliver better recommenations\n\n\n\nIn essence, this approach addresses the limitations of traditional methods by:\n\nModeling sequences: Understanding the order and timing of user interactions\nCapturing short-term intent: Focusing on recent behaviors within a session\nHandling anonymity: Working even without persistent user identifiers\nReal-time adaptation: Continuously updating recommendations as users interact\n\nNow that we understand why session-based recommendations make sense, it might be tempting to jump straight into building one. But I would advise you to hold on, cause there‚Äôs still one big question we need to address: How do you ensure your model gets access to the data it needs? When talking about serving this kind of real-time model in production, even the most powerful one is useless if it can‚Äôt receive timely user interaction signals."
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#a-simplified-payload-approach",
    "href": "projects/implement-recsys/c1/index.html#a-simplified-payload-approach",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "A Simplified Payload Approach",
    "text": "A Simplified Payload Approach\nLarge-scale real-time recommendation engines are typically built on top of streaming architectures with event pipelines (Kafka, Kinesis, etc.) that process user interactions. The streaming system is normally managed in a centralized manner by a platform team with the aim to provide real-time data processing capabilities to multiple downstream usages. This approach also ensures scalability and reliability.\nHowever, for the purpose of this tutorial, we can relax the requirements of serving multiple use cases. This leaves us with more room to think about how we can approach getting real-time user signals. Why can‚Äôt we just ask our app client to store and directly provide us the recent user interactions as context whenever it requests our recommendation service?\nIf the answer is yes, then I have a name for our approach: The Simplied Payload.\nBasically we would attach the real-time events along with the requests coming to our RecSys APIs (in the payload‚Äîhence its name). In practice, this implementation can be done by frontend engineers capturing user interactions locally (clicks, views, etc.).\nWhile this approach looks like a hacky-workaround ways, I have seen teams using this in production for early-stage real-time ML RecSys. When really thinking about it, the idea has some merits in and of itself. They include (1) minimal signal delay, (2) users not needing to refresh to get updated recommendations and (3) easier to implement and debug. (4) User privacy is another reason worth considering.\nIn the below screenshot you can see that in the POST request payload sending to a recommendation endpoint, we include the recent user interacted item IDs in item_seq_raw in the payload.\n\n\n\n\nExample request payload"
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#our-technology-stack",
    "href": "projects/implement-recsys/c1/index.html#our-technology-stack",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Our Technology Stack",
    "text": "Our Technology Stack\nBelow is a list of the technology frameworks we use in this project. There is no need to understand all of them in detail, so do not worry too much if you are not familiar. However, I do encourage you to at least learn the basics of each of them. Wherever possible I would also explain the rationale and what roles they play in the system that we build together.\n\n\n\nComponent\nTechnology\nPurpose\n\n\n\n\nML Framework\nPyTorch\nDeep learning model training\n\n\nAPI Server\nFastAPI\nHigh-performance API endpoints\n\n\nModel Server\nBentoML\nModel server\n\n\nExperiment Tracking\nMLflow\nModel versioning and experiments\n\n\nVector Database\nQdrant\nSimilarity search and retrieval\n\n\nCaching\nRedis\nReal-time data storage\n\n\nPackage Management\nuv\nFast Python dependency management\n\n\nContainerization\nDocker\nConsistent deployment environments"
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#project-structure-overview",
    "href": "projects/implement-recsys/c1/index.html#project-structure-overview",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Project Structure Overview",
    "text": "Project Structure Overview\nThe codebase can be found here. Below is a quick glance into its structure.\nrecsys-seq-model/\n‚îú‚îÄ‚îÄ notebooks/\n‚îÇ   ‚îú‚îÄ‚îÄ 000-prep-data.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ 001-features.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ 002-negative-sample.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ 010-baseline-popular.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ 011-sequence-modeling.ipynb\n‚îÇ   ‚îú‚îÄ‚îÄ 020-ann-index.ipynb\n‚îÇ   ‚îî‚îÄ‚îÄ 021-store-user-item-sequence.ipynb # Sequence storage\n‚îú‚îÄ‚îÄ src/                         # Core implementation modules\n‚îÇ   ‚îú‚îÄ‚îÄ cfg.py                   # Configuration management\n‚îÇ   ‚îú‚îÄ‚îÄ dataset.py               # Data loading utilities  \n‚îÇ   ‚îú‚îÄ‚îÄ id_mapper.py             # ID mapping functionality\n‚îÇ   ‚îú‚îÄ‚îÄ negative_sampling.py     # Sampling strategies\n‚îÇ   ‚îú‚îÄ‚îÄ sequence/                # Sequence model implementations\n‚îÇ   ‚îú‚îÄ‚îÄ eval/                    # Evaluation frameworks\n‚îÇ   ‚îî‚îÄ‚îÄ vectorstore.py           # Qdrant integration\n‚îú‚îÄ‚îÄ api/                         # FastAPI Orchestrator service\n‚îÇ   ‚îú‚îÄ‚îÄ app.py                   # Main application\n‚îÇ   ‚îú‚îÄ‚îÄ services.py              # Business logic\n‚îÇ   ‚îî‚îÄ‚îÄ models.py                # Request/response schemas\n‚îú‚îÄ‚îÄ model_server/                # BentoML model server\n‚îú‚îÄ‚îÄ data/                        # Dataset storage\n‚îú‚îÄ‚îÄ compose.yml                  # ML support toolings\n‚îî‚îÄ‚îÄ compose.api.yml              # Build API services\nNotice the numbered notebook sequence (000, 001, 002, etc.). This design ensures you build knowledge progressively:\n\n000-series: Data preparation and exploration\n010-series: Model training and evaluation\n\n020-series: Post-training preparation for serving\n\nEach notebook represents a complete milestone, allowing you to pause and resume at any point.\n\nA note on the commonly used commands\nYou may found the below commands useful, as I have myself running them regularly when developing the projects. I have put them in the Makefile as they are my shortcuts:\n# Install Python dependencies\nuv sync --all-groups\n\n# Start Jupyter Lab\nmake lab\n\n# Start supporting tools like MLflow and check their logs\nmake ml-platform-up && make ml-platform-logs\n\n# API Services\nmake api-up\nmake api-test\n\n# Start the Demo\nmake ui-up\n\n# Shut down all services\nmake down"
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#recap",
    "href": "projects/implement-recsys/c1/index.html#recap",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "Recap",
    "text": "Recap\nTo get a hand on the system that we will build together, you can clone the repo and follow the instructions in the README to get the project running. If you‚Äôre familiar with the technical details, mainly exploring the code and occasionally come back to the blog series for rationale/design decisions may be a great way to get the most from the tutorial.\nAfter following the README (hopefully no issues), you should have:\n\nA clear picture of what session-based recommendations are and why they‚Äôre useful\nYour development environment ready to go with all services running\n\nFamiliarity with the project structure so you can navigate the codebase\n\nThe foundation knowledge to tackle the upcoming chapters"
  },
  {
    "objectID": "projects/implement-recsys/c1/index.html#whats-next",
    "href": "projects/implement-recsys/c1/index.html#whats-next",
    "title": "Implement a RecSys, Chapter 1: Introduction and Project Overview",
    "section": "What‚Äôs Next",
    "text": "What‚Äôs Next\nFor those who want a linear progression, let‚Äôs get started! Here‚Äôs a preview of the roadmap ahead of us:\n\n\n\nIn Chapter 2, we‚Äôll get our hands dirty while setting up the input data in a format that‚Äôs designed for sequence modeling.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 1: Introduction and Project Overview. dvquys.com. https://dvquys.com/projects/implement-recsys/c1/."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html",
    "href": "projects/implement-recsys/c3/index.html",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "",
    "text": "This is the third chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#introduction",
    "href": "projects/implement-recsys/c3/index.html#introduction",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 2, we dug into our Amazon dataset and built a solid feature preprocessing pipeline. But we‚Äôre not ready to train any models yet. We‚Äôre missing something crucial: the labels that will teach our model the difference between what users want and what they don‚Äôt.\nWe already know which items users bought. So here‚Äôs a natural idea: let‚Äôs turn this into a sequence prediction problem. Show the model a user‚Äôs past actions, then ask it to predict what comes next.\nIf this idea of predicting next thing based on a sequence of things sounds familiar, then it‚Äôs expected. This is exactly what researchers in natural language processing figured out years ago. Take Word2Vec. It‚Äôs a simple but brilliant model that learns word relationships by studying which words hang out together. Word2Vec breaks a sentence into input-output pairs where the surrounding words are the input and the hidden target word is what you‚Äôre trying to predict.\n\n\n\nSource: A simple Word2Vec tutorial\n\n\nAt each step, we pick a target word along with its neighboring context to form a positive training example. How about the negatives? That‚Äôs where negative sampling comes in. Rather than letting the model see only context words that belong together, we throw in some randomly selected words from outside the context. This mix challenges the model, teaching it to learn the patterns of words that appear in similar contexts.\nNow let‚Äôs apply this to our book dataset. When a user clicks, views, or buys something, that‚Äôs a clear positive signal. But what about the millions of items they never touch? Are these items bad, or did the user just never discover them? Negative sampling helps us figure this out. It gives the model examples of what a user probably won‚Äôt pick.\nI personally find this analogy helpful: Training a recommendation model without negative samples is like teaching someone to recognize cats by only showing them cat pictures. They might think everything is a cat!\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is in notebooks/002-negative-sample.ipynb and src/negative_sampling.py in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#negative-sampling-strategies",
    "href": "projects/implement-recsys/c3/index.html#negative-sampling-strategies",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Negative Sampling Strategies",
    "text": "Negative Sampling Strategies\n\nRandom Sampling\nLet‚Äôs start with the obvious approach: just pick random items the user hasn‚Äôt touched.\nThis is faily straightforward to code up. You grab all the items a user hasn‚Äôt interacted with, throw them in a hat, and pull out however many negatives you need. It‚Äôs fast, it‚Äôs unbiased, and it works.\nBut there‚Äôs a problem. Some of those ‚Äúrandom‚Äù negatives might actually be items the user would love if they found them. There‚Äôs also another issue: the model might get biased toward popular items, since they show up in lots of positive pairs.\n\n\nPopularity-Based Sampling\nTo deal with the above popularity biase, instead of picking negatives randomly, we sample them based on how popular they are in the dataset. This makes popular items get chosen as negatives more often.\nWhy does this work better? A popular item has a higher chance of being liked by any user than a random item. So when we use popular items as negatives, we‚Äôre creating harder training examples. We‚Äôre forcing the model to learn why this specific user didn‚Äôt interact with this popular item, even though lots of other people did.\nThis has a nice side effect. It helps the model learn beyond simple popularity bias. Instead of just recommending whatever‚Äôs trending, it has to figure out what makes items relevant for specific users.\nThe downside? It‚Äôs more work to implement. You need to track popularity statistics. And there‚Äôs a catch: popular items aren‚Äôt always bad choices for a user. Sometimes they‚Äôre popular because they‚Äôre actually good. So when we force a negative for a user-item pair, it might confuse the model.\n\n\nHard Negative Mining\nThis is the fancy approach. You need a model that‚Äôs already somewhat trained, and you use it to find items it thinks a user would like‚Äîbut shouldn‚Äôt. These become your negative samples.\nIt‚Äôs like having a sparring partner who knows your weaknesses. The model keeps getting challenged by examples that are specifically designed to trip it up. As the model gets better, the negatives get harder, creating a virtuous cycle of improvement.\nSounds great, right? But it‚Äôs not always worth the extra effort, especially at the start. You need to train iteratively, which takes more time and compute. And there‚Äôs always the risk that you‚Äôre just teaching the model to overfit to its own biases.\nFor this project, we‚Äôll use popularity-based negative sampling. It strikes a good balance between effectiveness and simplicity."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#lets-implement",
    "href": "projects/implement-recsys/c3/index.html#lets-implement",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Let‚Äôs implement",
    "text": "Let‚Äôs implement\nThe full implementation is here.\nFunction signature:\n\n\nsrc/negative_sampling.py\n\n\ndef generate_negative_samples(\n    df,\n    user_col=\"user_indice\",\n    item_col=\"item_indice\", \n    label_col=\"rating\",\n    neg_label=0,\n    seed=None,\n) -&gt; pd.DataFrame:\n    \"\"\"\n    Generate negative samples for a user-item interaction DataFrame.\n    \n    The key insight: sample negative items proportional to their \n    popularity to create more challenging training scenarios.\n\n    Args:\n        df (pd.DataFrame): DataFrame containing user-item interactions.\n        user_col (str): Column name representing users.\n        item_col (str): Column name representing items.\n        label_col (str): Column name for the interaction label (e.g., rating).\n        neg_label (int): Label to assign to negative samples (default is 0).\n        seed (int, optional): Seed for random number generator to ensure reproducibility.\n\n    Returns:\n        pd.DataFrame: DataFrame containing generated negative samples.\n    \"\"\"\n\n\nStep 1: Calculate Item Popularity\n# Calculate item popularity based on interaction frequency\nitem_popularity = df[item_col].value_counts()\n\n# Convert to sampling probabilities to be used in the next step\npopularity = item_popularity.values.astype(np.float64)\ntotal_popularity = popularity.sum()\nsampling_probs = popularity / total_popularity\nThis creates a probability distribution where more popular items have higher chances of being selected as negatives.\n\n\nStep 2: Identify Negative Candidates\n# Create user-item interaction mapping\nuser_item_dict = df.groupby(user_col)[item_col].apply(set).to_dict()\n\n# For each user, find items they haven't interacted with\nfor user, pos_items in user_item_dict.items():\n    negative_candidates = all_items_set - pos_items\nWe make sure we only sample from items the user hasn‚Äôt already interacted with.\n\n\nStep 3: Popularity-Weighted Sampling\n# Create a mapping from item to index to quickly access item-related data.\nitems = item_popularity.index.values\nitem_to_index = {item: idx for idx, item in enumerate(items)}\n\n# Sample negatives proportional to popularity\ncandidate_indices = [item_to_index[item] for item in negative_candidates_list]\ncandidate_probs = sampling_probs[candidate_indices]\ncandidate_probs /= candidate_probs.sum()  # Normalize\n\nsampled_items = np.random.choice(\n    negative_candidates_list, \n    size=num_neg, \n    replace=False, \n    p=candidate_probs\n)\nThis makes sure popular items are more likely to be selected as negatives, creating harder training examples.\nIn our implementation, we choose to have the same number of negative samples as positive samples. This helps us avoid dealing with imbalanced training data. But feel free to experiment with different ratios (you‚Äôll need to update the implementation).\nnum_pos = len(pos_items)  # Number of positive interactions\nnum_neg = min(num_pos, num_neg_candidates)  # Match positive count\nLike other crucial parts of our pipeline, let‚Äôs test this with some mock data to make sure it works."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#adding-features-to-the-generated-negative-samples",
    "href": "projects/implement-recsys/c3/index.html#adding-features-to-the-generated-negative-samples",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Adding Features to the generated negative samples",
    "text": "Adding Features to the generated negative samples\nAs you can see from the above output, we only have the item indice and the label. We also need to populate the new negative observations with the same features as the positive ones, i.e.¬†the sequence of previously interacted items.\nSince these negative samples come from the same user, we can use the same features as the positive ones. We just need to use the timestamp of the corresponding positive interaction.\ndef add_features_to_neg_df(pos_df, neg_df, user_col, timestamp_col, feature_cols=[]):\n    \"\"\"\n    Add features from positive samples to negative samples DataFrame.\n    \n    Key insight: Negative samples should have realistic timestamps\n    that align with when the user was actually active.\n    \"\"\"\n    \n    # Create pseudo timestamps for negatives\n    # This timestamp pseudo column is used as join key to the positive samples, ensuring that each negative\n    # maps to one positive sample and get the positive's features.\n    neg_df = neg_df.assign(\n        timestamp_pseudo=lambda df: df.groupby(user_col).cumcount() + 1\n    )\n    \n    # Merge with corresponding positive interaction timestamps\n    neg_df = pd.merge(\n        neg_df,\n        pos_df.assign(\n            timestamp_pseudo=lambda df: df.groupby([user_col])[timestamp_col].rank(\n                method=\"first\"\n            )\n        )[[user_col, timestamp_col, \"timestamp_pseudo\", *feature_cols]],\n        how=\"left\",\n        on=[user_col, \"timestamp_pseudo\"],\n    ).drop(columns=[\"timestamp_pseudo\"])\n    \n    return neg_df"
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#concat-and-re-split-train-test",
    "href": "projects/implement-recsys/c3/index.html#concat-and-re-split-train-test",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Concat and re-split train-test",
    "text": "Concat and re-split train-test\nAfter adding features to the negative samples, we can combine them with the positive samples. Then we re-split the dataset based on the timestamp milestone we used in our original train-test split."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#recap",
    "href": "projects/implement-recsys/c3/index.html#recap",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we tackled the tricky problem of implicit feedback in recommendation systems. Here‚Äôs what we covered:\n\nWhy we need negative samples: Without them, our model would be like someone trying to recognize cats by only seeing cat pictures. We need examples of what users don‚Äôt want to create balanced learning.\nThree approaches to negative sampling: We looked at random sampling (simple but not very challenging), popularity-based sampling (our chosen approach that creates harder training scenarios), and hard negative mining (powerful but complex).\nOur popularity-based implementation: We built a system that samples negative items based on their popularity. This forces our model to learn why a user didn‚Äôt interact with popular items that others liked.\nKeeping things balanced: We generate equal numbers of positive and negative samples for each user (1:1 ratio) to avoid bias in either direction.\nAdding realistic features: We make sure our negative samples have proper timestamps and features that align with when users were actually active. This maintains temporal consistency for sequence modeling.\n\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is in notebooks/002-negative-sample.ipynb and src/negative_sampling.py in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c3/index.html#whats-next",
    "href": "projects/implement-recsys/c3/index.html#whats-next",
    "title": "Implement a RecSys, Chapter 3: Negative Sampling",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nWith our balanced dataset of positive and negative samples, we‚Äôre ready to tackle model evaluation. In Chapter 4, we‚Äôll set up our evaluation framework and experiment tracking with MLflow while implementing a baseline model along the way as an illustration. This will give us the foundation for systematic model development and comparison.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 3: Negative Sampling. dvquys.com. https://dvquys.com/projects/implement-recsys/c3/."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html",
    "href": "projects/implement-recsys/c5/index.html",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "",
    "text": "This is the fifth chapter of the tutorial series: Implement a RecSys."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#introduction",
    "href": "projects/implement-recsys/c5/index.html#introduction",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Introduction",
    "text": "Introduction\nIn Chapter 4, we established our evaluation framework, implemented MLflow experiment tracking, and built a popularity-based baseline model. Since our baseline is simple, it treats all users similarly‚Äîeveryone gets the same popular items regardless of their personal browsing history or preferences.\nThis chapter marks the transition from simple heuristics to a more sophisticated machine learning approach aimed towards personalization. We‚Äôll dive deep into the rationales behind sequence-based recommendation models and explore the design decisions that make them effective.\nThese models excel at understanding the temporal patterns hidden in user behavior. Instead of just knowing that a user liked certain books, our model will learn to recognize meaningful sequences. For example, when someone browses ‚ÄúPython Programming‚Äù followed by ‚ÄúMachine Learning,‚Äù the model understands they might be interested in ‚ÄúDeep Learning with PyTorch‚Äù next.\nThis is going to be a long post. So grab your coffee, and let‚Äôs dive in.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/011-sequence-modeling.ipynb file and the src/sequence/ directory in the project repository.\n\n\n\n\n\n\n\n\nJargon\n\n\n\nThroughout the series I would be using sequence modeling and session-based recommendation interchangeably to refer to the same technique of modeling user‚Äôs behavior based on their sequential interactions."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#why-sequence-modeling-matters-in-recommendations",
    "href": "projects/implement-recsys/c5/index.html#why-sequence-modeling-matters-in-recommendations",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Why Sequence Modeling Matters in Recommendations",
    "text": "Why Sequence Modeling Matters in Recommendations\nTraditional collaborative filtering approaches treat user preferences as static snapshots. They might know that User A liked Items 1, 3, and 7, but they miss the story hidden in the order and timing of these interactions.\nConsider these two users with identical item preferences but different behavioral patterns:\nUser A: Book1 ‚Üí Book2 ‚Üí Book3 ‚Üí Book4 ‚Üí Book5\nUser B: Book5 ‚Üí Book1 ‚Üí Book4 ‚Üí Book2 ‚Üí Book3\nBoth users interacted with the same five books, but their sequences tell very different stories. User A might be following a structured learning path (beginner to advanced), while User B might be jumping between topics based on immediate curiosity. Traditional collaborative filtering would treat these users identically, but sequence models can capture these nuanced patterns.\nThe even-more-compelling part about this our sequential model is that it gives you two very strong arguments to argue with: real-time adaptation and cold start handling.\nThink about what happens in static recommendation systems when new users sign up. They get the same boring popular items everyone else sees. ‚ÄúHere are the top 10 books everyone‚Äôs reading!‚Äù It‚Äôs like walking into a bookstore and having the clerk hand you a list without asking what you‚Äôre interested in. The user has to suffer through generic recommendations until the system has enough data about them. This is the cold start problem, which is, trust me, a real typical ask your Product Manager would come up during your recommendation model pitch.\nOur sequence model flips this on its head. The moment a new user clicks on their first book, the model springs into action. They browse ‚ÄúPython Programming,‚Äù then click on ‚ÄúMachine Learning Basics‚Äù‚Äîthe model immediately understands they‚Äôre on a learning journey. The system starts personalizing from interaction number one, like having a shop assistant who gets better at helping you the longer you browse.\nAnd all of this happens without retraining the model or updating any databases, dealing with one of the biggest problems in recommendation systems: how do you stay relevant when user interests change quickly? Traditional collaborative filtering models need to be retrained to pick up new patterns. Our sequence model adapts immediately. If thriller readers suddenly start buying romance novels (maybe it‚Äôs Valentine‚Äôs Day), the model notices the shift in the very next recommendation request.\nNow that we understand why sequence modeling is crucial, let‚Äôs explore how to actually design it."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#sequence-modeling-approaches",
    "href": "projects/implement-recsys/c5/index.html#sequence-modeling-approaches",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Sequence Modeling Approaches",
    "text": "Sequence Modeling Approaches\nThe central challenge in sequence modeling boils down to one question: how do you take a bunch of user interactions and turn them into something useful for predictions? You have a sequence like ‚ÄúBook A ‚Üí Book B ‚Üí Book C‚Äù and somehow need to compress all that information into a representation that captures what the user is really interested in.\nI like to think of this as a ‚Äúpooling‚Äù problem. You‚Äôre pooling information from multiple items into a single representation. It‚Äôs like trying to summarize a conversation‚Äîyou want to keep the important bits and throw away the noise.\nThe simplest pooling method is averaging. But wait, you might think, average what exactly? We‚Äôre talking about sequences of item IDs that users clicked on. You can‚Äôt just average ‚Äúbook_123‚Äù and ‚Äúbook_456‚Äù like they‚Äôre numbers, right?\nThis is where embeddings become your best friend. Here‚Äôs the key insight: every item in your catalog gets converted into a vector of numbers before any learning happens. Instead of working with raw item IDs, your model works with these dense numerical representations. It‚Äôs the same trick that made Word2Vec so powerful‚Äîremember from Chapter 3 how it could tell you that ‚Äúking‚Äù minus ‚Äúman‚Äù plus ‚Äúwoman‚Äù equals ‚Äúqueen‚Äù1? That magic happens because words become vectors, and vectors can be manipulated mathematically.\nSo when we talk about averaging a sequence, we‚Äôre actually averaging the embedding vectors of the items in that sequence. Book A becomes a 128-dimensional vector, Book B becomes another 128-dimensional vector, and averaging them gives you a single 128-dimensional vector that somehow captures the essence of ‚Äúsomeone who reads both Book A and Book B.‚Äù\nThe beautiful thing about embeddings, just like in ML in general, is that they can start random but learn to be meaningful. During training, the model adjusts these vectors so that similar items end up close together in the embedding space. Books about Python programming cluster together, romance novels form their own neighborhood, and so on.\nNow, averaging is wonderfully simple, and sometimes simplicity wins. I‚Äôve seen myself trying other pooling methods only to discover that good old averaging works just as well. But that doesn‚Äôt mean you shouldn‚Äôt experiment. Some sequences have patterns that averaging destroys‚Äîlike the difference between reading ‚ÄúBeginner Python ‚Üí Advanced Python‚Äù versus ‚ÄúAdvanced Python ‚Üí Beginner Python‚Äù.\nThis is where more sophisticated pooling methods come in. The field of sequence modeling offers several architectural choices, each with its own strengths and trade-offs. The simplest approach uses Recurrent Neural Networks (RNNs), which process sequences step by step, maintaining a hidden state that captures information from previous steps. Think of an RNN as reading a book one page at a time, trying to remember everything important from earlier pages. While this sounds intuitive, vanilla RNNs have a memory problem‚Äîthey forget important details from way back in the sequence, what researchers call the vanishing gradient problem.\nTo fix this memory issue, researchers developed Long Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks. These use clever gating mechanisms to decide what to remember and what to forget. GRUs, in particular, have become the go-to choice for recommendation systems. They‚Äôre simpler than LSTMs but perform just as well‚Äîlike getting 90% of the benefit with 60% of the complexity.\nMore recently, Transformer models have taken the field by storm. Instead of processing sequences step by step, they use self-attention mechanisms to look at all parts of the sequence simultaneously. It‚Äôs like being able to read an entire book at once and instantly connect themes from chapter 1 to chapter 20. Transformers are incredibly powerful for capturing long-range dependencies, but there‚Äôs a catch‚Äîthey can be computationally expensive, especially when you have thousands or millions of items in your catalog.\nNow that we‚Äôve explored different sequence modeling approaches‚Äîfrom simple averaging to sophisticated Transformers‚Äîlet‚Äôs not forget that all architecture decisions should consider the following question: how do we deploy these models in production systems that need to handle millions of items in real-time?"
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#retrieval-vs-ranking",
    "href": "projects/implement-recsys/c5/index.html#retrieval-vs-ranking",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Retrieval vs Ranking",
    "text": "Retrieval vs Ranking\nWhile the main task is still to rank the most relevant and likely-to-be-interacted items on top, in reality it‚Äôs not uncommon that we need to deal with ranking for millions of items. Training a scoring model that takes into account each instance of &lt;user, context, item&gt; and uses it from the start is not feasible because of latency constraints. Calling that model to give fine-grained scores to all millions of items would take forever, and neither our users have that kind of patience nor do we as engineers find that idea sane.\n\nThe Two-Phase Architecture\nTo deal with this problem, we break the whole ranking process into two phases: first we quickly retrieve a shortlist of candidates from millions of items, then we use a fine-grained ranker to give the final ranking on the shortlist.\nThe difference in naming between these two phases already reveals their distinct characteristics. The retrieval phase needs to quickly scan millions of items to find about 1,000 potential candidates in milliseconds. This speed requirement means it cannot use complex computations and often needs to leverage indexing structures like vector databases for fast similarity search. The focus here is on recall‚Äîensuring that relevant items make it into the candidate set, even if the initial scoring isn‚Äôt perfect.\nThe ranking phase operates under very different constraints. With bandwidth and a much more limited scope of hundreds to thousands of items, it can afford to adopt many more signals and complex features into its model. This phase delivers much better fine-grained scores for final ordering, focusing on precision‚Äîgetting the order exactly right among the candidates that have already been deemed potentially relevant.\n\n\n\n\nYan, Ziyou. (Jun 2021). System Design for Recommendations and Search.\n\n\n\n\n\nRetrieval as a Standalone System\nOne important thing to note: most of the time the retrieval phase provides scores together with the candidates, which indicates to some extent how well they match the query. We can already use this signal to rank the candidates and provide recommendations to users.\nIt also allows us to approach building recommendation systems in an incremental manner. In most of my RecSys projects, I start with building and deploying the retrieval model as the first version. The idea is to get both early impact and actual feedback from users interacting with our recommendations. This user feedback becomes a much better source of labeling signals to train our later ranking model on.\nThe benefits are twofold. First, you get a working system deployed quickly, enabling faster time to impact rather than waiting months to build a complex multi-stage system. More importantly, you start collecting actual interaction data from users engaging with your recommendations, rather than relying solely on historical patterns2. This real user feedback becomes invaluable‚Äîit‚Äôs a much richer source of labeling signals for training your later ranking model than any offline evaluation could provide.\nFinally, it allows you to build the serving and monitoring infrastructure in phases, learning how to handle recommendation traffic, monitor model performance, and debug issues at a manageable scale before introducing the additional complexity of a ranking layer.\n\n\nOur Two-Tower Retriever\nIn that spirit, the implementation of our sequential recommendation model in this series is a retrieval-based one. It follows a typical Two Tower architecture, where the query tower embeds information about the user and context‚Äîin our case, the user‚Äôs sequence of interactions‚Äîwhile the candidate tower represents the candidate items.\nThis separation is crucial for efficient serving. The candidate tower can precompute embeddings for all items and store them in a vector index. The query tower only needs to run at request time to generate the user‚Äôs current context embedding.\n\n\n\nTraining Setup\nThe labels for training come from our preparation in previous chapters. We create positive instances from &lt;user, context, item&gt; tuples which have actual interaction records in the past, while negative examples are sampled from the unseen item space for each user-context pair. This creates a binary classification problem where the model learns to distinguish between items a user would interact with versus items they would ignore.\n\n\nServing Architecture\nFor serving, our retrieval system works in two phases. First, in an offline process, we index all candidate item embeddings in a nearest neighbor vector search system. Then, during online serving, we send the &lt;user, context&gt; as input to the query tower to get a query embedding vector, use similarity lookup to search for the nearest candidate neighbors in the index, and return the corresponding items with their similarity scores.\nThis architecture enables low-latency response times even when searching through millions of items, making it practical for recommendation serving.\nAgain, the beauty of this approach is that it‚Äôs both a complete recommendation system on its own and a foundation for more sophisticated ranking models. You can deploy it immediately to start serving personalized recommendations, then later add a ranking layer on top without changing the underlying retrieval infrastructure."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#model-implementation",
    "href": "projects/implement-recsys/c5/index.html#model-implementation",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Model Implementation",
    "text": "Model Implementation\nNow let‚Äôs translate the two-tower architecture into concrete code. But first, let me explain a key design decision that shapes our implementation.\n\nThe Case for Session-Only Models\nTraditional two-tower retrievers include both user embeddings and sequence representations in the query tower. But I‚Äôve chosen to build what I call a ‚ÄúSoleSequenceRetriever‚Äù‚Äîa model that relies entirely on the sequence of interactions, without any user-specific embeddings.\nThis isn‚Äôt just a technical choice; it‚Äôs a strategic one3 that fundamentally changes how the model behaves. By removing user embeddings, we‚Äôre making a bet that the sequence itself contains enough signal to make good recommendations. A user browsing ‚ÄúPython Programming ‚Üí Machine Learning ‚Üí Data Analysis‚Äù tells us more about their immediate intent than knowing they‚Äôre ‚ÄúUser #47832‚Äù with some historical preference profile.\nThis approach solves several practical problems. New users get meaningful recommendations from their very first interaction‚Äîno cold start period where they see generic popular items. Operationally, serving becomes a bit simpler since we don‚Äôt need to manage user embedding lookups or worry about user ID mapping issues.\n\n\nCore Architecture\nWith that context, let‚Äôs look at our implementation. The core is simple and straightforward.\n\n\nsrc/sequence/model.py\n\nclass SoleSequenceRetriever(BaseSequenceRetriever):\n    def __init__(\n        self,\n        num_items: int,\n        embedding_dim: int,\n        pooling_method: str = \"mean\",\n        dropout: float = 0.2,\n        mask_pooling: bool = True,\n    ):\n        self.num_items = num_items\n        self.embedding_dim = embedding_dim\n        self.pooling_method = pooling_method.lower()\n        self.mask_pooling = mask_pooling\n\n        if item_embedding is None:\n            self.item_embedding = nn.Embedding(\n                num_items + 1,  # extra index for unknown/padding\n                embedding_dim,\n                padding_idx=num_items,\n            )\n        else:\n1            self.item_embedding = item_embedding\n\n        if self.pooling_method == \"gru\":\n            self.gru = nn.GRU(embedding_dim, embedding_dim, batch_first=True)\n        elif self.pooling_method == \"mean\":\n            self.gru = None\n        else:\n            raise ValueError(\"Invalid pooling_method. Choose 'gru' or 'mean'.\")\n\n        self.query_fc = nn.Sequential(\n            nn.Linear(embedding_dim, embedding_dim),\n            nn.BatchNorm1d(embedding_dim),\n            nn.Dropout(dropout),\n        )\n\n\n1\n\nWe support pre-trained item embeddings, which can be useful if you have embeddings from other models or external sources.\n\n\nThe architecture reflects our key principles. The item embedding layer converts raw item IDs into dense vectors that can capture semantic relationships. The configurable pooling method (mean or GRU) aggregates the sequence into a single representation. The fully connected layer in the query tower adds learning capacity while batch normalization and dropout help with generalization.\nNotice what‚Äôs not here: any mention of user IDs. The model‚Äôs query tower depends entirely on the sequence of items, making it truly session-based."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#model-training",
    "href": "projects/implement-recsys/c5/index.html#model-training",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Model Training",
    "text": "Model Training\nThe forward pass computes cosine similarity between the query embedding (pooled sequence representation) and candidate embedding, scaled to [0,1] to match our binary labels. This choice of cosine similarity isn‚Äôt arbitrary‚Äîit aligns with how we‚Äôll serve the model using nearest neighbor search in production, i.e.¬†we perform the exact same similarity computation, just against pre-indexed candidate embeddings rather than individual examples.\n\n\nsrc/sequence/model.py\n\nclass SoleSequenceRetriever(BaseSequenceRetriever):\n    # ...\n\n    def get_query_embeddings(self, inputs: Dict[str, torch.Tensor]) -&gt; torch.Tensor:\n1        item_seq = inputs.get(\"item_seq\")\n        if item_seq is None:\n            raise ValueError(\"Missing required input key: 'item_seq'\")\n        item_seq = self.replace_neg_one_with_padding(item_seq)\n        mask = item_seq != self.item_embedding.padding_idx\n        seq_embeds = self.item_embedding(item_seq)\n        # Pool the sequence; the method will decide whether to use the mask based on self.mask_pooling\n        seq_rep = self.pool_sequence(seq_embeds, mask)\n        query_embedding = self.query_fc(seq_rep)\n        return F.normalize(query_embedding, p=2, dim=1)\n\n    def forward(self, inputs):\n        query_embedding = self.get_query_embeddings(inputs)      \n        candidate_embedding = self.get_candidate_embeddings(inputs)  \n        \n        query_embedding = F.normalize(query_embedding, p=2, dim=1)\n        candidate_embedding = F.normalize(candidate_embedding, p=2, dim=1)\n        cos_sim = torch.sum(query_embedding * candidate_embedding, dim=1)\n        \n        return (cos_sim + 1) / 2  # Scale to [0,1] since cosine similarity is in [-1, 1]\n\n\n1\n\nNote how we only need the item_seq from the inputs dict. This handles the cold start problem‚Äînew users immediately get meaningful recommendations based solely on their current session, without needing historical preference data.\n\n\n\nMask Pooling\nOne challenge we need to address: variable sequence lengths. In practice, users have different numbers of interactions‚Äîsome might have browsed 3 books, others 15. To batch these sequences efficiently for training, we need to pad shorter sequences to a fixed length. We do this by filling empty positions with a special padding token (typically -1).\nMasked pooling can help us deal with this issue. Without masking, our pooling operations would include these padding tokens in their calculations, diluting the actual sequence representation. For mean pooling, averaging real embeddings with padding embeddings would give us a less meaningful representation. For GRU pooling, the model might learn spurious patterns from the padding tokens.\nBy implementing masked pooling, we tell the model to ignore these -1 padding tokens during sequence aggregation. The mask ensures that only genuine user interactions contribute to the final sequence representation, preserving the integrity of the learned patterns.\n\n\nsrc/sequence/model.py\n\nclass SoleSequenceRetriever(BaseSequenceRetriever):\n    # ...\n\n    def pool_sequence(self, seq_embeds: torch.Tensor, mask: torch.Tensor = None) -&gt; torch.Tensor:\n        if self.mask_pooling and mask is not None:\n            if self.pooling_method == \"gru\":\n                lengths = mask.sum(dim=1).clamp(min=1)\n                packed_seq = nn.utils.rnn.pack_padded_sequence(\n                    seq_embeds, lengths.cpu(), batch_first=True, enforce_sorted=False\n                )\n                _, hidden_state = self.gru(packed_seq)\n                return hidden_state.squeeze(0)\n            elif self.pooling_method == \"mean\":\n                mask_float = mask.unsqueeze(-1).float()\n                sum_embeds = (seq_embeds * mask_float).sum(dim=1)\n                count = mask_float.sum(dim=1).clamp(min=1)\n                return sum_embeds / count\n\nAs with other ideas, feel free to experiment with using masked pooling or not. It may seem sound but not always lead to noticeable improvements according to my experience.\n\n\nTraining Loop\nThe training function uses binary cross-entropy loss against our positive/negative samples. We use PyTorch Lightning to leverage its built-in training loop, logging capabilities and integration with MLflow instead of implementing ourselves4.\n\n\nsrc/sequence/trainer.py\n\nclass LitSequenceRetriever(L.LightningModule):\n    # ...\n\n    def training_step(self, batch, batch_idx):\n        # Get model's predictions\n        predictions = self.model({\n            \"user_ids\": batch[\"user\"],\n            \"item_seq\": batch[\"item_sequence\"], \n            \"candidate_items\": batch[\"item\"]\n        })\n        \n        # Compare to actual user behavior\n        labels = batch[\"rating\"].float()\n        loss = nn.BCELoss()(predictions, labels)\n        \n        # This loss drives the learning process\n        return loss\n\n\n\nPreparing PyTorch Datasets\nPyTorch models work best with PyTorch datasets. Our UserItemRatingDFDataset class handles the conversion from pandas DataFrames to PyTorch tensors:\n\n\nsrc/dataset.py\n\nclass UserItemRatingDFDataset(Dataset):\n    def __init__(self, df, user_col: str, item_col: str, rating_col: str, timestamp_col: str):\n        self.df = df.assign(\n            **{rating_col: (df[rating_col] / MAX_RATING).astype(np.float32)}  # Normalize rating to [0,1]\n        )\n    \n    def __getitem__(self, idx):\n        return dict(\n            user=torch.as_tensor(self.df[self.user_col].iloc[idx]),\n            item=torch.as_tensor(self.df[self.item_col].iloc[idx]),\n            rating=torch.as_tensor(self.df[self.rating_col].iloc[idx]),\n            item_sequence=torch.tensor(self.df[\"item_sequence\"].iloc[idx], dtype=torch.long),\n        )\n\nThis dataset is then wrapped into a PyTorch DataLoader for batching and shuffling.\ntrain_loader = DataLoader(\n    train_dataset,\n    batch_size=batch_size,\n    shuffle=True,\n    num_workers=2,\n)"
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#integration-with-mlflow-for-experiment-tracking",
    "href": "projects/implement-recsys/c5/index.html#integration-with-mlflow-for-experiment-tracking",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Integration with MLflow for Experiment Tracking",
    "text": "Integration with MLflow for Experiment Tracking\nEvery training run is automatically logged to MLflow through our configuration system:\ncfg = ConfigLoader(\"../cfg/common.yaml\")\ncfg.run.run_name = \"002-sequence-retriever-gru\"\ncfg.run.experiment_name = \"Retrieve - Binary\"\ncfg.init()  # Automatically sets up MLflow logging\nWe customzize our Lightning trainer module to help us automatically log:\n\nTraining metrics: Loss, learning rate, weight norms\nValidation metrics: ROC-AUC, PR-AUC, ranking metrics\nModel artifacts: Best model checkpoints\n\nWhile the model is training, we can observe how it converges and how it performs on the validation set in real-time on MLflow Web UI:"
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#model-comparison",
    "href": "projects/implement-recsys/c5/index.html#model-comparison",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Model Comparison",
    "text": "Model Comparison\nAfter training, we can compare the performance of our model with the popularity baseline:\n\n\n\nThe results validate our sequence-based approach with substantial improvements across all metrics. Let‚Äôs break down what these numbers tell us about our retrieval system‚Äôs effectiveness.\n\nRecall: The Heart of Retrieval Performance\nSince we‚Äôre building a retrieval system, recall is chosen to be our north-star metric. Recall measures what fraction of relevant items we successfully include in our candidate set. In the two-stage retrieval-ranking framework, if our retrieval system misses a relevant item, no amount of sophisticated ranking can fix that‚Äîthe item is gone forever from the user‚Äôs recommendations.\nOur sequence-based model achieves significant improvements in recall across thresholds:\n\nRecall@100: 0.186 vs 0.062 (197% improvement)\nRecall@10: 0.038 vs 0.01 for the popularity baseline (280% improvement)\n\nThese numbers tell two important stories. Assuming we would send hundreds of candidates as the output to the later ranking stage, the recall@100 improvement shows we‚Äôre nearly doubling our ability to capture relevant items in a typical retrieval pass. This is crucial for the downstream ranking stage‚Äîwe‚Äôre giving it much better raw material to work with.\nThe recall@10 improvement indicates positive sign but for a different reason. When we deploy this retrieval system as a standalone recommender (without a ranking stage), users see these top-10 results directly. A 280% improvement means users are nearly three times more likely to find something relevant in their immediate recommendations.\n\n\nRanking Quality Validation\nHowever, when talking about the end-result ranking for users, NDCG tells a more complete story. The significant improvements in NDCG ranking metrics provide additional validation: NDCG@10: 0.018 vs 0.005 (360% improvement). NDCG measures whether we‚Äôre putting the most relevant items at the top of our candidate list. This improvement suggests our retrieval system isn‚Äôt just finding relevant items‚Äîit‚Äôs finding them and scoring them appropriately.\nThis ranking quality matters regardless of whether we add a downstream ranking stage. If we deploy the retrieval system directly, users get better-ordered recommendations. If we add ranking later, we‚Äôre providing the ranking model with better-scored candidates to work with.\n\n\nWhat This Means for Users\nThese metric improvements translate to concrete user experience benefits:\n\nImmediate Impact: Users are 3x more likely to find relevant items in their top recommendations, dramatically reducing the time spent browsing through irrelevant suggestions.\nBetter Cold Start: New users get personalized recommendations from their very first interaction, eliminating the typical cold start period of generic popular items.\nSystem Flexibility: The improved candidate quality gives us options‚Äîwe can deploy this as a complete recommendation system now, or use it as a strong foundation for a more sophisticated ranking stage later.\n\nThe results demonstrate that sequence modeling captures meaningful patterns in user behavior that static approaches miss entirely.\nThat said, while the uplifts are strong, the absolute numbers are still low. This is expected since we haven‚Äôt really optimized the model, so keep in mind that this is just the beginning. But if you ask me what high or good numbers look like, I would say this kind of improvement is already sufficient for us to deploy in production, given that we have no other models and are currently using the popularity baseline."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#model-registry-and-versioning",
    "href": "projects/implement-recsys/c5/index.html#model-registry-and-versioning",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Model Registry and Versioning",
    "text": "Model Registry and Versioning\nAs mentioned earlier, MLflow does not only help us track the training process but also provides a model registry for version management and easier deployment. We can easily log the training output artifacts to MLflow, while including a quality gate to ensure that only models that exceed minimum performance thresholds get registered:\n# Automatic model registration after successful training\nif val_roc_auc &gt; cfg.eval.min_roc_auc:\n    # Register model as new version\n    mlflow.pytorch.log_model(\n        model, \n        cfg.train.retriever.mlf_model_name,\n        registered_model_name=cfg.train.retriever.mlf_model_name\n    )\n    \n    # Tag as champion if performance exceeds threshold\n1    if val_roc_auc &gt; champion_threshold:\n        client.set_registered_model_alias(\n            cfg.train.retriever.mlf_model_name, \n            \"champion\", \n            latest_version\n        )\n\n1\n\nChampion is a special alias for the current best model. It makes it easier for a new model to go online since we just need to tag it as champion without having to change the code or worry about version number.\n\n\nThat‚Äôs it. Finally we are ready to conclude the chapter. If you have made this far, I give you my respect!"
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#recap",
    "href": "projects/implement-recsys/c5/index.html#recap",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Recap",
    "text": "Recap\nIn this chapter, we achieved a major milestone in our recommendation system journey‚Äîtransitioning from simple heuristic-based approaches to personalized machine learning models that capture temporal patterns in user behavior. Here‚Äôs what we accomplished:\nFrom Theory to Practice:\n\nWhy sequence modeling matters: We established that traditional collaborative filtering misses the crucial story hidden in the order and timing of user interactions. By modeling sequences, we can distinguish between users who follow structured learning paths versus those who jump between topics randomly.\nReal-time adaptation and cold start solutions: We demonstrated how sequence-based models solve two critical RecSys challenges. New users get personalized recommendations from their very first interaction, and the model adapts immediately to changing user interests without requiring retraining.\n\nArchitectural Decisions:\n\nTwo-tower retrieval architecture: We chose a retrieval-focused approach over end-to-end ranking, enabling fast candidate selection from millions of items while maintaining millisecond-level response times.\nSession-only modeling: By building a ‚ÄúSoleSequenceRetriever‚Äù that relies entirely on interaction sequences without user embeddings, we prioritized adaptability and simplified serving infrastructure while solving cold start problems elegantly.\n\nTechnical Implementation:\n\nMasked pooling for variable sequences: We addressed the practical challenge of variable-length user sessions by implementing masked pooling that ignores padding tokens during sequence aggregation.\nCosine similarity alignment: Our training objective uses the same cosine similarity computation as production serving, ensuring consistency between offline training and online inference.\nPyTorch Lightning integration: We leveraged Lightning‚Äôs capabilities for training loops, distributed training, and automatic MLflow integration, avoiding the complexity of custom implementations.\n\nValidation and Results:\n\nSubstantial performance improvements: Our sequence-based retriever achieved 197% improvement in Recall@100 and 280% improvement in Recall@10 compared to the popularity baseline, demonstrating that temporal patterns contain meaningful signals for recommendations.\nMLflow model registry: We established automated model versioning with quality gates, ensuring only models exceeding minimum performance thresholds get registered for potential deployment.\n\nThis sequence-based retriever represents a complete, deployable recommendation system that can serve personalized suggestions in real-time. More importantly, it provides a solid foundation for future enhancements‚Äîwhether adding a downstream ranking layer or incorporating additional features like item content or user demographics.\n\n\n\n\n\n\nCode\n\n\n\nAll code for this chapter is available in the notebooks/011-sequence-modeling.ipynb file and the src/sequence/ directory in the project repository."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#whats-next",
    "href": "projects/implement-recsys/c5/index.html#whats-next",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "What‚Äôs Next?",
    "text": "What‚Äôs Next?\nWith our sequence-based retrieval model performing strongly, we have several exciting directions for future development:\nShort-term enhancements could include experimenting with Transformer-based pooling for better long-range dependency modeling or implementing more sophisticated negative sampling strategies like hard negative mining. Most importantly though, as long as request-level latency is still kept in check, we can always experiment with adding more input features to our towers.\nExamples:\n\nIncorporating item content features to improve cold-start item recommendations\nProvide query tower with user profile features, focusing on the information that we may be able to extract even for new users\nHelp model be aware of the timings of the items in the interaction sequence\n\nMedium-term evolution might involve building the ranking layer on top of our retrieval system based on the actual feedback labels from the new deployed recommendation module.\nProduction deployment includes setting up the vector database infrastructure for candidate indexing, implementing real-time serving APIs, and establishing A/B testing frameworks for online evaluation.\nIn Chapter 6, we will continue our journey to build an end-to-end recommendation system by preparing the offline computation and online serving infrastructure, e.g.¬†MLflow, Redis, Qdrant. The focus would shift a bit towards platform/infrastructure, but we only touch upon how we set them up locally so hopefully it should not be too much of a stretch.\nContinue to the next chapter.\n\n If you find this tutorial helpful, please cite this writeup as:\n\nQuy, Dinh. (May 2025). Implement a RecSys, Chapter 5: Session-based Recommendation Model. dvquys.com. https://dvquys.com/projects/implement-recsys/c5/."
  },
  {
    "objectID": "projects/implement-recsys/c5/index.html#footnotes",
    "href": "projects/implement-recsys/c5/index.html#footnotes",
    "title": "Implement a RecSys, Chapter 5: Session-based Recommendation Model",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nIf you don‚Äôt recall anything about king and queen‚Ä¶ Well, yeah, cause I didn‚Äôt say anything about that (LOL). But I would assume if you read any random article about Word2Vec, you would run into this famous analogy.‚Ü©Ô∏é\nThe sequence itself can also serves as a long-term historical signal, like containing the interactions from two months ago. Just that in this case we would want to couple those interactions with their timestamps to help the model differentiate.‚Ü©Ô∏é\nOr‚Ä¶ it‚Äôs not entirely wrong if you think I‚Äôm just a lazy guy who doesn‚Äôt want to deal with the missing of user embedding for new users üòÖ.‚Ü©Ô∏é\nI still remember how frustrating it was trying to implement DDP (Distributed Data Parallel) training loop with pure PyTorch. After figuring out that Lightning does not only handle that elegently but also has a lot of other features that I would have to implement myself, I never looked back.‚Ü©Ô∏é"
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html",
    "href": "projects/reviews-parsing-mlsys/index.html",
    "title": "Let‚Äôs build an ML system",
    "section": "",
    "text": "This project focuses on applying engineering practices to build a Machine Learning System using Kubernetes and deploy on Google Cloud Platform."
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html#architecture",
    "href": "projects/reviews-parsing-mlsys/index.html#architecture",
    "title": "Let‚Äôs build an ML system",
    "section": "Architecture",
    "text": "Architecture"
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html#demo",
    "href": "projects/reviews-parsing-mlsys/index.html#demo",
    "title": "Let‚Äôs build an ML system",
    "section": "Demo",
    "text": "Demo\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2024). Let‚Äôs build an ML system. dvquys.com. https://dvquys.com/projects/reviews-parsing-mlsys/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog",
    "section": "",
    "text": "Implement a RecSys, Chapter 7: Building the API Layer\n\n\nFrom trained models to production-ready APIs with BentoML and FastAPI\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nJun 2, 2025\n\n\n21 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 6: Preparing for Serving\n\n\nBuild the foundation that bridges model training to serving\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nJun 1, 2025\n\n\n17 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 5: Session-based Recommendation Model\n\n\nDesign and build sequence models that capture temporal patterns in user behavior\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 31, 2025\n\n\n28 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 4: Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation\n\n\nEstablishing the evaluation foundation and implementing your baseline\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 30, 2025\n\n\n14 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 3: Negative Sampling\n\n\nWhy your model needs to see what users don‚Äôt want‚Äîand how to generate those examples\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 26, 2025\n\n\n10 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 2: Understanding the Data and Feature Engineering\n\n\nExploring Amazon product dataset and building features for sequence models\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 25, 2025\n\n\n8 min\n\n\n\n\n\n\n\nImplement a RecSys, Chapter 1: Introduction and Project Overview\n\n\nUnderstanding session-based recommendations and setting up your development environment\n\n\n\nrecsys\n\n\nimplement-recsys-series\n\n\n\n\n\n\nMay 24, 2025\n\n\n11 min\n\n\n\n\n\n\n\nBuilding a Conversational Assistant for Restaurant Discovery and Booking\n\n\nLessons from Creating a Chatbot Using RAG, LLMs, and Advanced Retrieval Techniques\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\nLet‚Äôs build an ML system\n\n\nEnd-to-end set up and deploy on Kubernetes\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\n\nA Hierarchical approach with Elasticsearch: Lessons from 22 Months of Iteration\n\n\nA Practical Guide to Designing Query Templates, Boosting Relevance, and Addressing Site-Search Challenges\n\n\n\ntech\n\n\nsearch\n\n\n\n\n\n\nJun 1, 2023\n\n\n57 min\n\n\n\n\n\n\n\nFrom Model to Production: Deploying Your Machine Learning Solution on Google Cloud\n\n\nTransforming a Laptop-Trained ML Model into a Scalable Batch Processing Pipeline using GCP‚Äôs Dataproc\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nFeb 17, 2020\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  }
]