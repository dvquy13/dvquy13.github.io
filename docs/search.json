[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hi, I’m Quy—an Applied Data Scientist. I blog about RecSys, ML, Productivity and other random things.\nHere are some of my side projects regarding the above topics.\nI spend most of my other free time in football and DotA. Recently I started to learn badminton but damn this sport is hard.\nI’m from Hanoi, Vietnam 🇻🇳. Currently living in Singapore 🇸🇬."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "Review Rec Bot\n\n\nDemo building Restaurant Recommendation Chatbot based on Yelp Public Data\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\n\n\n\n\n\nReviews Parsing MLSys\n\n\nSetting up a ML system on K8s\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/review-rec-bot/index.html",
    "href": "projects/review-rec-bot/index.html",
    "title": "Review Rec Bot",
    "section": "",
    "text": "Project Details\n\n\n\n\n\n\n\nDemo\n\n\n\nProblem Statement\nListing/reservation businesses like Yelp offer value to users by providing useful information to make them find out where to go next. Good search and recommendation systems go a long way, but they are still far from delivering the ultimate experience where users can interact naturally with the system for complex queries or have a conversation to drill down their needs.\n\n\nApproach\nBuild a chatbot assistant to assist users in discovering places to go and booking.\nWorkflow:\n\nDownload Yelp reviews data. Sample 5,240 reviews from 100 businesses.\nSet up development environment including experimentation tracking via MLflow, observability with Arize Phoenix\nBuild MVP version using LlamaIndex, Qdrant\nBuild synthetic evaluation datasets with 30 questions for retrieval and response. Manual response dataset are gradually built up and added based on error analysis\nConduct error analysis and look at the model’s output to come up with new iteration ideas. Run a total of 10 experiments to improve the RAG, with notable attempts including: Replacing Llama-8.1B with GPT-4o-mini, Fine-tuning Embedding Model, Hybrid Retrievers, Semantic Chunking, BGE Reranker, Query Expansion\nBuild RAG Agent based on OpenAI API with Query Engine tool and Reservation Service tool. Chatbot UI is built with Chainlit.\n\n\n\nEvaluation\nEvaluation results can be found here.\nTwo proposed key metrics are Retrieval Hit Rate and Response Correctness.\nRetrieval is a critical component in any RAG system. Along with data prep, retrieval sits at the top of the pipeline so any improvements in these fronts is more likely to improve the overall system. Hit rate is chosen as a key metric because since we can employ rerank as a subsequent step, we have room to optimize for the ranking issues. \nFor response, Correctness measures both how relevant the answer is with respect to the query and how correct it is compared to the referenced answer. It’s therefore a better indicator than pure relevance, which is just based on the query and hence easier to get right.\nFor reference, Response Correctness on synthetic dataset has improved +166% from 1.75 / 5.00 from MVP version to 4.67 / 5.00 on the current version. The current Retrieval Hit Rate @ 50 reaches 73%, not directly comparable but at MVP version Retrieval Hit Rate @ 10 was 20%.\nAs next steps, while there is not much room to improve Response Correctness, we ought to increase Retrieval Hit Rate to 90% which should be doable since this dataset only contains a small amount of data.\n\n\nLearnings/Remarks\n\nUsing question-style query leads to 5-20% uplift in retrieval hit rate compared to using keyword search\nBM25 Retriever alone results in 200% increase in retrieval effectiveness including hit rate, average precision, MRR and NDCG\nFine-tuning small embedding model like Snowflake/snowflake-arctic-embed-m-v1.5 yield +80% retrieval effectiveness, especially rankings of the retrieved nodes\nUsing GPT-4o-mini as response synthesizer significantly improve the quality of response in all aspects (especially correctness from 2.6 to 3.8) compared to Llama 3.1-8B-Instruct\nUsing TreeSummarize with custom prompt yields a +10% uplift on response correctness evaluation, from 3.97 to 4.37. Based on eyeballing we also see a way better response that is recommendation-like\n\n\n\nChallenges\n\nChallenge 1: Auto-retrieval not reliable\nWhile theoretically both precision and recall should be greatly improved if we are able to apply the right filters for User questions instead of relying on embedding/keyword matching, my first attempt to apply auto-retrieval with ChromaDB did not yield promising results. There were at least two syntactic issues which broke the agentic workflow. Even after fixing those two the unreliable nature of this approach is still there and I also witnessed a -10% degradation in Retrieval Hit Rate.\nIn the end I forfeited the feature but nevertheless look forward to a way to re-applying this technique.\n\n\nChallenges 2: Indexing pipeline takes too long\nIndexing 70K nodes from 30K reviews for 400 businesses takes more than 6 hours!\n\n\n\nFuture Improvements\n\nGuardrail system inputs and outputs\nExperiment with Contextual Compression and Filters\nFine tune LLM Re-ranker (FlagEmbedding BGE Reranker)\nTry ColBERT as a new retriever (may be add to the list of retrievers)\nTry different loss function in training embeddings\nImprove the diversity by implement custom re-ranker that weight downs the reviews from the already seen biz_id\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Sep 2024). Review Rec Bot. dvquy13.github.io. https://dvquy13.github.io/projects/review-rec-bot/."
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html",
    "href": "posts/site-search-elasticsearch/index.html",
    "title": "A Site-Search Engineer’s Journal Approaching Relevance Challenges in Elasticsearch Query Construction",
    "section": "",
    "text": "This article is originally published in Towards Data Science"
  },
  {
    "objectID": "posts/site-search-elasticsearch/index.html#appendix-1-detailed-breakdown-of-default-tfidf-matching-where-length-of-the-field-value-affect-overall-matching-score",
    "href": "posts/site-search-elasticsearch/index.html#appendix-1-detailed-breakdown-of-default-tfidf-matching-where-length-of-the-field-value-affect-overall-matching-score",
    "title": "A Site-Search Engineer’s Journal Approaching Relevance Challenges in Elasticsearch Query Construction",
    "section": "Appendix 1: Detailed breakdown of default TFIDF matching where length of the field value affect overall matching score",
    "text": "Appendix 1: Detailed breakdown of default TFIDF matching where length of the field value affect overall matching score\n# Result\n{\n  \"hits\": {\n    \"hits\": [\n      {\n        \"_id\": \"003vietnamesepho\",\n        \"_score\": 1.0470967,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 3\n        },\n        \"_explanation\": {\n          \"value\": 1.0470967,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.13353139,\n                  \"description\": \"weight(cuisine:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            },\n            {\n              \"value\": 1.0470967,\n              \"description\": \"sum of:\",\n              \"details\": [\n                # Matching score with \"vietnamese\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:vietnamese in 0) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.52354836,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        {\n                          \"value\": 2.2,\n                          \"description\": \"boost\",\n                          \"details\": []\n                        },\n                        {\n                          \"value\": 0.47000363,\n                          \"description\": \"idf, computed as log(1 + (N - n + 0.5) / (n + 0.5)) from:\",\n                          ...\n                        },\n                        {\n                          \"value\": 0.50632906,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            {\n                              \"value\": 1,\n                              \"description\": \"freq, occurrences of term within document\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 1.2,\n                              \"description\": \"k1, term saturation parameter\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 0.75,\n                              \"description\": \"b, length normalization parameter\",\n                              \"details\": []\n                            },\n                            # Notice the length=2 here is in the denominator,\n                            # which means that the higher the length the less\n                            # the score\n                            {\n                              \"value\": 2,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            {\n                              \"value\": 2.6666667,\n                              \"description\": \"avgdl, average length of field\",\n                              \"details\": []\n                            }\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                # Matching score with \"pho\"\n                {\n                  \"value\": 0.52354836,\n                  \"description\": \"weight(restaurant_name:pho in 0) [PerFieldSimilarity], result of:\",\n                  # Details are exactly like above\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      },\n      {\n        \"_id\": \"002vietnamesephonoodle\",\n        \"_score\": 0.8942772,\n        \"_source\": {\n          \"restaurant_name\": \"Vietnamese Pho Noodle\",\n          \"cuisine\": \"Vietnamese\",\n          \"rating\": 4\n        },\n        \"_explanation\": {\n          \"value\": 0.8942772,\n          \"description\": \"max of:\",\n          \"details\": [\n            {\n              \"value\": 0.13353139,\n              \"description\": \"sum of:\",\n              \"details\": [...]\n            },\n            {\n              \"value\": 0.8942772,\n              \"description\": \"sum of:\",\n              \"details\": [\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:vietnamese in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [\n                    {\n                      \"value\": 0.4471386,\n                      \"description\": \"score(freq=1.0), computed as boost * idf * tf from:\",\n                      \"details\": [\n                        ...,\n                        {\n                          \"value\": 0.4324324,\n                          \"description\": \"tf, computed as freq / (freq + k1 * (1 - b + b * dl / avgdl)) from:\",\n                          \"details\": [\n                            ...,\n                            # Here the length=3 (greater than length=2 of the\n                            # above restaurant)\n                            {\n                              \"value\": 3,\n                              \"description\": \"dl, length of field\",\n                              \"details\": []\n                            },\n                            ...\n                          ]\n                        }\n                      ]\n                    }\n                  ]\n                },\n                {\n                  \"value\": 0.4471386,\n                  \"description\": \"weight(restaurant_name:pho in 1) [PerFieldSimilarity], result of:\",\n                  \"details\": [...]\n                }\n              ]\n            }\n          ]\n        }\n      }\n    ]\n  }\n}\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2023). A Site-Search Engineer’s Journal Approaching Relevance Challenges in Elasticsearch Query Construction. dvquy13.github.io. https://dvquy13.github.io/posts/site-search-elasticsearch/."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html",
    "href": "posts/deploy-ml-gcp/index.html",
    "title": "What I learned about deploying Machine Learning application",
    "section": "",
    "text": "This article is originally published in VinID Engineering\nImagine a company named Rainbow imports boxes of flowers and need to classify them into species. For six months, they have some staff label the boxes manually. Now, they hire you to build a Machine Learning model to do the task.\nWith a small amount of labelled data as input and tons of experience working on Kaggle projects, you quickly develop a 95% accuracy using simple RandomForestClassifier from the popular scikit-learn library. Nice. Stakeholders approve and ask you when you could deploy that model to production.\nHmm, deploy a model from my laptop? …\nIn case you wonder, I hope this tutorial will help you understand one among some common and most simple approaches. The diagram below depicts how we will use Google Cloud Platform to do the job in a batch-processing manner.\nI choose the Iris data set as our input to help you see how our approach works with small-sized problems. All the codes are in this repo."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#about-writing-codes",
    "href": "posts/deploy-ml-gcp/index.html#about-writing-codes",
    "title": "What I learned about deploying Machine Learning application",
    "section": "About writing codes",
    "text": "About writing codes\nInstead of writing a long script to do everything, we break a pipeline into tasks and checkpoint interim data to disk. For example, after doing preprocess on train and test data, we dump both the data outputs and the transformer to Google Cloud Storage. We then load those objects as inputs for the next step.\nThis strategy has several purposes. First, for a long-running task, if a job fails at one of the last steps, we can re-run the pipeline from the nearest checkpoint rather than wasting time and resources restarting the whole pipeline. Second, it allows us to (1) debug more easily, (2) get alert when things break and (3) monitor interim outputs. Lastly, decoupled components can be understood more clearly, and easier to be replaced or extended later."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#about-computing-resources",
    "href": "posts/deploy-ml-gcp/index.html#about-computing-resources",
    "title": "What I learned about deploying Machine Learning application",
    "section": "About computing resources",
    "text": "About computing resources\nNormally for a small input size, we are fine with setting up a single virtual machine on the cloud. However, in some companies with mature cloud practice, the overhead of managing that VM is a type of cost that is difficult to justify. Especially when we have better options. For instance, Cloud Dataproc provides us with virtual machines that only live for the duration of one run, thereby free us from managing the machines. In this post, we explore Dataproc as our main engine for all the computing process."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#create-a-gcp-project-and-enable-necessary-components",
    "href": "posts/deploy-ml-gcp/index.html#create-a-gcp-project-and-enable-necessary-components",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Create a GCP project and enable necessary components",
    "text": "Create a GCP project and enable necessary components\n\n👉 Create a free GCP account with $300 credit by going to console.cloud.google.com. Beware that by following this tutorial, you might incur a cost of about $0.2–$0.5.\n\n\n\n\n\n👉 Click Billing at the left sidebar and initiate a billing account to be able to use the components used in this tutorial\n\n\n\n\n\n👉 Select Library, then search and enable the following API: Cloud Dataproc, Cloud Storage and Cloud Firestore.\n👉 Navigate to the Firestore either by scrolling the sidebar to the left or search from the top menu bar. When you arrive at the below screen, choose SELECT NATIVE MODE, then choose us-east1 as the location."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#environment-setup",
    "href": "posts/deploy-ml-gcp/index.html#environment-setup",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Environment setup",
    "text": "Environment setup\n\nStep 1: Launch terminal window\n\n👉 At the home page of your GCP project, select the command button to the right of your menubar. The CloudShell window then appears as you can see below:\n\n\n\n\n\n👉 Launch Cloud Shell Editor:\n\n\n\n\nIt’s recommended to use Cloud Shell to follow this tutorial. However, if you’re using Linux and want to use terminal on your local machine, make sure you first install the Google Cloud SDK and firebase CLI.\n\n\nStep 2: Clone Github repo\n\n👉 In the Terminal window:\n\ngit clonehttps://github.com/dvquy13/gcp_ml_pipeline.git\ncd gcp_ml_pipeline\n\n👉 Select File then open the file gcp_ml_pipeline/configs/.project_env:\n\n\n\n\n\n👉 Replace the values enclosed by &lt;&gt;. For the GCP_PROJECT, you need to provide the id of your GCP project. For the remaining, feel free to choose some random names for the global variables that identify your resources. The final output looks like this:\n\nGCP_PROJECT='zinc-primer-230105'\nGCS_BUCKET=dvquys-tut-gcp-ml-pipeline\nDATA_LOCATION=us-east1\nBQ_DATASET=tut_iris\nBQ_ORG_TABLE=F_ORIGINAL\nCLUSTER_NAME=iris-pred\n\n👉 Grant execute permission to the folder scripts by running the command: chmod +x -R ./scripts. Then, run ./scripts/00_import_data_to_bigquery.sh. Link to the script.\n\n\n\nStep 3: Create Dataproc cluster and submit jobs\nWe use Makefile to orchestrate our actions. You can find it here: https://github.com/dvquy13/gcp_ml_pipeline/blob/master/Makefile.\n️Now, run the following commands in sequence:\n\nmake create-dataproc-cluster: This command creates a Dataproc cluster. The single-node flag indicates that this is a cluster containing only one machine. n1-standard-1 is the cheapest machine we can rent. To install Python packages, we supply the metadata and initialization-actions params.\nmake build: Package your code, including your source code and other 3rd party libraries that you can not pre-install when creating the cluster (PyYAML for example). To submit a job to the cluster, we will send these codes to those machines via the gcloud dataproc jobs submit pyspark command.\nmake submit-job ENV=dev MODULE=data_import TASK=query_train_pred: Submit job cloning input data for training and predicting. The submit-job make command allows you to use this interface to run on both local and development environments.\nmake submit-job ENV=dev MODULE=feature_engineer TASK=normalize: Prepare features. In this illustrative example, we choose to include only normalization in the pipeline. After learning the normalization parameters from the train data set, we save those configurations for later usage.\nmake submit-job ENV=dev MODULE=model TASK=fit: Train model. Here we build a pipeline consisting of 2 steps, Normalization and Logistic Regression. After that, we persist the fit pipeline.\nmake submit-job ENV=dev MODULE=predict TASK=batch_predict: Batch predict. This job demonstrates the process when you use your learned model to make predictions.\nmake submit-job ENV=dev MODULE=predict TASK=store_predictions: Store predictions. The reason we do not combine this with the above step is two-fold. First, writing to a database often takes time and requires several retries. Second, we write to a document database like Cloud Firestore because when other team uses, they typically retrieve one document per query. However, there are times when we want to inspect the whole batch of predictions (e.g. debugging, count number of documents scored more than 0.9). For this query pattern, we will better off using the persisted outputs from the previous step, stored as parquet files in Cloud Storage.\nmake delete-dataproc-cluster: Delete Dataproc cluster. After the process finishes, delete the cluster so no further cost incurs.\n\n\n\n\nSucceeded Dataproc jobs\nYou can see that your predictions are stored at Cloud Firestore by accessing its web console.\n\n\n\nFirestore populated with predictions\nAlong the way, you will see that the output data of each step is persisted in Cloud Storage. I use parquet rather than CSV as the serialization format because it can embed schema information (therefore you do not have to specify column types when reading) and reduce storage size. For more detail, please refer to this benchmark."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#clean-up",
    "href": "posts/deploy-ml-gcp/index.html#clean-up",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Clean up",
    "text": "Clean up\n\n👉 Finally, when you’re done exploring the results, you can delete all resources by running these commands:\n\n./scripts/01_erase_resources.sh\n./scripts/02_disable_resources.sh\n./scripts/03_delete_project.sh"
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#apache-spark-for-bigger-data",
    "href": "posts/deploy-ml-gcp/index.html#apache-spark-for-bigger-data",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Apache Spark for bigger data",
    "text": "Apache Spark for bigger data\nIn this tutorial, we rent one small machine from Dataproc and use pandas as our preprocessing engine, which perfectly handles the case of data fit into the memory of that machine. However, often data input in real-world situations will be much bigger, therefore require us to use a distributed computing framework for scalability. In that case, you can just switch to using Apache Spark. From version 1.3, Spark introduces its DataFrame API, which greatly bears resemblance to Pandas counterpart. After porting your code from Pandas to Spark, to be able to run jobs across multiple machines, you just need to create a bigger cluster with a master and multiple workers."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#apache-airflow-for-orchestration",
    "href": "posts/deploy-ml-gcp/index.html#apache-airflow-for-orchestration",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Apache Airflow for orchestration",
    "text": "Apache Airflow for orchestration\nMost of the batch job is not ad hoc. If it is, we should not even think about putting effort to standardize the process in the first place. Apache Airflow can play the role of both a scheduler and a monitor. It keeps metadata of each run and can send you alerts when things fail.\n\n\n\n\nExample of Airflow DAG: Source\n\n\n\nAn alternative is Dataproc Workflows. This is a native solution offered by GCP, but I haven’t tried it myself so I will just leave the documentation here."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#local-development",
    "href": "posts/deploy-ml-gcp/index.html#local-development",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Local development",
    "text": "Local development\nBecause rarely our codes work the first time we write them, it’s very important to be able to quickly test without having to go through all the boilerplate steps from setting up variables to requesting cloud resources. My suggestion is that we should set up our local environment asap. We can install Apache Spark 2.4.3+ to act as our runner engine, and MongoDB to be our alternative for Cloud Firestore. Here in the code repo, you can still refer to some line containing what I call the “environment branching logic”, which enables you to switch between running the same code on both local and cloud environments."
  },
  {
    "objectID": "posts/deploy-ml-gcp/index.html#unit-testing",
    "href": "posts/deploy-ml-gcp/index.html#unit-testing",
    "title": "What I learned about deploying Machine Learning application",
    "section": "Unit testing",
    "text": "Unit testing\nMany people have already talked about unit testing, so I won’t go too detailed here. I also don’t do unit testing in this tutorial for the sake of simplicity. However, I strongly encourage you to add testing yourself. Whatever it takes, unit testing forces us to modularize our code and add a layer of alerting. This is very important because things in data science often break in silence."
  },
  {
    "objectID": "projects/reviews-parsing-mlsys/index.html",
    "href": "projects/reviews-parsing-mlsys/index.html",
    "title": "Reviews Parsing MLSys",
    "section": "",
    "text": "This project focuses on applying engineering practices to build a Machine Learning System in the domain of public reviews data.\n\n\n\n\n\n\nProject Details\n\n\n\n\n\n\n\nArchitecture\n\n\n\n\n\nDemo\n\n If you find this article helpful, please cite this writeup as:\n\nQuy, Dinh. (Jun 2024). Reviews Parsing MLSys. dvquy13.github.io. https://dvquy13.github.io/projects/reviews-parsing-mlsys/."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Review Rec Bot\n\n\nDemo building Restaurant Recommendation Chatbot based on Yelp Public Data\n\n\n\ntech\n\n\nllm\n\n\n\n\n\n\nSep 22, 2024\n\n\n4 min\n\n\n\n\n\n\n\nReviews Parsing MLSys\n\n\nSetting up a ML system on K8s\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nJun 21, 2024\n\n\n1 min\n\n\n\n\n\n\n\nA Site-Search Engineer’s Journal Approaching Relevance Challenges in Elasticsearch Query Construction\n\n\nA hierarchical keyword-matching approach\n\n\n\ntech\n\n\nsearch\n\n\n\n\n\n\nJun 1, 2023\n\n\n57 min\n\n\n\n\n\n\n\nWhat I learned about deploying Machine Learning application\n\n\nA tutorial on building custom ML training workflow using Google Cloud Platform\n\n\n\ntech\n\n\nmachine learning\n\n\n\n\n\n\nFeb 17, 2020\n\n\n14 min\n\n\n\n\n\n\nNo matching items"
  }
]