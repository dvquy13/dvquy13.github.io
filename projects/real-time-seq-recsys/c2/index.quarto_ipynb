{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Building Real-time RecSys Chapter 2 - Understanding the Data and Feature Engineering\"\n",
        "subtitle: \"Exploring Amazon product dataset and building features for sequence models\"\n",
        "date: \"2025-05-24\"\n",
        "categories: [recsys]\n",
        "image: \"../static/L2%20-%20Data%20signals.excalidraw.png\"\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    code-annotations: hover\n",
        "# draft: true\n",
        "---\n",
        "\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In [Chapter 1](../c1/index.qmd), we set up our development environment and explored the project architecture. Now it's time to dive into the heart of any recommendation system: **data**. The quality of our recommendations depends entirely on how well we understand, process, and engineer features from our data.\n",
        "\n",
        "This chapter focuses on the data pipeline from raw data to model-ready features. We'll explore the Amazon product dataset, understand user-item interaction sequences, and build the feature engineering pipeline that will power our session-based recommender.\n",
        "\n",
        "::: {.callout-note appearance=\"simple\"}\n",
        "## Code\n",
        "All code for this chapter is available in the `notebooks/000-prep-data.ipynb` and `notebooks/001-features.ipynb` files in the [project repository](https://github.com/dvquy13/recsys-seq-model).\n",
        ":::\n",
        "\n",
        "## Dataset Overview: Amazon Reviews 2023\n",
        "\n",
        "A good dataset for this kind of sequential recommenandation project would typically have the following characteristics:\n",
        "\n",
        "1. **Temporal richness**: Each interaction has a timestamp, enabling sequence modeling\n",
        "2. **Scale**: Millions of interactions across thousands of users and items\n",
        "3. **Real-world patterns**: Authentic user behavior with natural sparsity\n",
        "\n",
        "Among some of the well known public data sources, I choose to use the [Amazon Reviews 2023](https://amazon-reviews-2023.github.io/) dataset from the McAuley Lab, specifically focusing on the \"Books\" category. Apart from possessing the above characteristics, this dataset also contains other potentially useful features regarding both users, items and their interactions like reviews, with a decent amount of observations.\n",
        "\n",
        "::: {.column-page}\n",
        "![](../static/dataset.png)\n",
        ":::\n",
        "\n",
        "::: {.column-page}\n",
        "![](../static/dataset-info.png)\n",
        ":::\n",
        "\n",
        "The main schema of the dataset is as simple as follows:\n",
        "\n",
        "```python\n",
        "# From cfg/common.yaml\n",
        "data:\n",
        "  user_col: \"user_id\"        # <1>\n",
        "  item_col: \"parent_asin\"    # <2>\n",
        "  rating_col: \"rating\"       # <3>\n",
        "  timestamp_col: \"timestamp\" # <4>\n",
        "```\n",
        "1. Unique user identifier\n",
        "2. Product identifier (in our case, the ASIN—Amazon Standard Identification Number)\n",
        "3. User rating (1-5 scale)\n",
        "4. Interaction timestamp\n",
        "\n",
        "::: {.callout-note collapse=\"true\"}\n",
        "## Configuration Management\n",
        "\n",
        "```python\n",
        "from src.cfg import ConfigLoader\n",
        "\n",
        "# Load configuration\n",
        "cfg = ConfigLoader(\"cfg/common.yaml\")\n",
        "```\n",
        "\n",
        "Throughout the project, we will define a central place to store most of the configuration in `cfg/common.yaml` to have notebooks and scripts more easily access their inputs and outputs. Also, as running multiple experiments is a nature of working in ML, this design aims to make it easier to try different sets of configurations.\n",
        ":::\n",
        "\n",
        "## Data Preprocessing Pipeline\n",
        "\n",
        "The data flows through several key preprocessing steps:\n",
        "\n",
        "### 1. Data Sampling and Filtering\n",
        "\n",
        "As ML is mostly about iterative development, the main rationale of sampling and filtering is to have a small but decent enough dataset to experiment different ideas **fast**. The more experiments we run, the more likely we would introduce improvements to our models.\n",
        "\n",
        "What does it mean for a decent RecSys dataset? One of the key criteria is **sparsity**—the ratio of observed interactions to all possible user-item pairs. \n",
        "\n",
        "To understand why sparsity is problematic, consider the interaction matrix where each cell represents a potential user-item interaction:\n",
        "\n",
        "- **Matrix size**: `num_users × num_items` (total possible interactions)\n",
        "- **Actual interactions**: Much smaller number of observed ratings/clicks\n",
        "- **Sparsity**: `1 - (actual_interactions / (num_users × num_items))`\n",
        "\n",
        "The sparsity problem gets **quadratically worse** as datasets grow:\n",
        "\n",
        "```python\n",
        "# Small dataset example\n",
        "users = 1,000, items = 1,000 → possible interactions = 1M\n",
        "actual interactions = 50,000 → sparsity = 95%\n",
        "\n",
        "# Larger dataset \n",
        "users = 10,000, items = 10,000 → possible interactions = 100M  \n",
        "actual interactions = 500,000 → sparsity = 99.5%\n",
        "```\n",
        "\n",
        "Each new user adds an entire **row** of mostly empty interactions, and each new item adds an entire **column** of mostly empty interactions. Since users typically interact with only a tiny fraction of available items, the interaction matrix becomes increasingly sparse as the catalog grows.\n",
        "\n",
        "In RecSys, the interaction distribution is typically long tail—a lot of noisy items or users have just a few interactions. So while randomly sampling may work just fine for many ML use cases, we need to apply it a bit more carefully here. Remember that ultimately we want to have a sample dataset where each user/item has at least X interactions.\n",
        "\n",
        "The tricky part is that a basic random sampling of users and items would create **sparsity cascade**—a domino effect that breaks your dataset.\n",
        "\n",
        "Here's what happens: You start with users and items that look fine on their own. User A has 10 interactions, Item X has 15 interactions. When you remove User B, you also lose all of User B's interactions with Item X. Suddenly Item X only has 8 interactions. Oops, now it's too sparse, so you remove it too. But removing Item X means User A loses some interactions and might become too sparse as well.\n",
        "\n",
        "It's like pulling threads from a sweater—everything starts unraveling.\n",
        "\n",
        "To deal with this problem, we can take an iterative approach where we gradually drop random users from the dataset while keeping an eye on the conditions and our sampling target. The trade-off is that we would no longer have an exact fixed number of users and items in the dataset as we would like, but rather defining minimum acceptable thresholds, like below:\n",
        "\n",
        "```python\n",
        "# From cfg/common.yaml\n",
        "sample:\n",
        "  sample_users: 10000\n",
        "  min_val_records: 5000         # <1>\n",
        "  min_user_interactions: 5\n",
        "  min_item_interactions: 10\n",
        "```\n",
        "1. We need to ensure sufficient validation data to evaluate our models.\n",
        "\n",
        "```python\n",
        "from src.sample import InteractionDataSampler\n",
        "\n",
        "data_sampler = InteractionDataSampler(\n",
        "    user_col=cfg.data.user_col,\n",
        "    item_col=cfg.data.item_col,\n",
        "    sample_users=cfg.sample.sample_users,\n",
        "    min_val_records=cfg.sample.min_val_records,\n",
        "    random_seed=cfg.run.random_seed,\n",
        "    min_item_interactions=cfg.sample.min_item_interactions,\n",
        "    min_user_interactions=cfg.sample.min_user_interactions,\n",
        "    perc_users_removed_each_round=0.1,\n",
        "    debug=False,\n",
        ")\n",
        "```\n"
      ],
      "id": "d45564f8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Randomly removing 2960 users - Round 18 started\n",
        "2025-03-09 15:23:54.942 | INFO     | src.sample:sample:118 - After randomly removing users - round 18: num_users=29,605\n",
        "2025-03-09 15:23:54.943 | INFO     | src.sample:sample:122 - Number of users 29,605 are still greater than expected, keep removing...\n",
        "\n",
        "\n",
        "Randomly removing 2413 users - Round 19 started\n",
        "2025-03-09 15:23:55.594 | INFO     | src.sample:sample:118 - After randomly removing users - round 19: num_users=24,137\n",
        "2025-03-09 15:23:55.594 | INFO     | src.sample:sample:122 - Number of users 24,137 are still greater than expected, keep removing...\n",
        "2025-03-09 15:23:55.622 | INFO     | src.sample:sample:137 - Number of val_df records 4,282 are falling below expected threshold, stop and use `sample_df` as final output...\n",
        "2025-03-09 15:23:55.634 | INFO     | src.sample:sample:146 - len(sample_users)=19,734 len(sample_items)=7,388"
      ],
      "id": "b42b03a7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<details>\n",
        "<summary>InteractionDataSampler implementation</summary>\n",
        "```python\n",
        "from functools import partial\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from loguru import logger\n",
        "\n",
        "\n",
        "class InteractionDataSampler:\n",
        "    \"\"\"\n",
        "    Just randomly get X users will not guarantee that the output dataset would qualify the condition of richness.\n",
        "    Instead we take an iterative approach where we gradually drop random users from the dataset while keeping an eye on the conditions and our sampling target.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        user_col: str = \"user_id\",\n",
        "        item_col: str = \"item_id\",\n",
        "        sample_users: int = 1000,\n",
        "        min_val_records: int = 1000,\n",
        "        random_seed: int = 41,\n",
        "        min_user_interactions: int = 5,\n",
        "        min_item_interactions: int = 10,\n",
        "        buffer_perc: float = 0.2,\n",
        "        perc_users_removed_each_round: float = 0.01,\n",
        "        debug: bool = False,\n",
        "    ):\n",
        "        self.user_col = user_col\n",
        "        self.item_col = item_col\n",
        "        self.sample_users = sample_users\n",
        "        self.min_val_records = min_val_records\n",
        "        self.random_seed = random_seed\n",
        "        self.min_user_interactions = min_user_interactions\n",
        "        self.min_item_interactions = min_item_interactions\n",
        "        self.buffer_perc = buffer_perc\n",
        "        self.perc_users_removed_each_round = perc_users_removed_each_round\n",
        "        self.debug = debug\n",
        "        self.min_val_records = min_val_records\n",
        "\n",
        "    def remove_random_users(self, df, k=10):\n",
        "        users = df[self.user_col].unique()\n",
        "        np.random.seed(self.random_seed)\n",
        "        to_remove_users = np.random.choice(users, size=k, replace=False)\n",
        "        return df.loc[lambda df: ~df[self.user_col].isin(to_remove_users)]\n",
        "\n",
        "    def get_unqualified(self, df, col: str, threshold: int):\n",
        "        unqualified = df.groupby(col).size().loc[lambda s: s < threshold].index\n",
        "        return unqualified\n",
        "\n",
        "    def sample(\n",
        "        self,\n",
        "        train_df: pd.DataFrame,\n",
        "        val_df: pd.DataFrame,\n",
        "        keep_random_removing: bool = True,\n",
        "    ):\n",
        "        get_unqualified_users = partial(\n",
        "            self.get_unqualified,\n",
        "            col=self.user_col,\n",
        "            threshold=self.min_user_interactions,\n",
        "        )\n",
        "        get_unqualified_items = partial(\n",
        "            self.get_unqualified,\n",
        "            col=self.item_col,\n",
        "            threshold=self.min_item_interactions,\n",
        "        )\n",
        "\n",
        "        r = 1\n",
        "\n",
        "        sample_df = train_df.copy()\n",
        "\n",
        "        while keep_random_removing:\n",
        "            keep_removing = True\n",
        "            i = 1\n",
        "\n",
        "            num_users_removed_each_round = int(\n",
        "                self.perc_users_removed_each_round * sample_df[self.user_col].nunique()\n",
        "            )\n",
        "            if r > 1:\n",
        "                print(\n",
        "                    f\"\\n\\nRandomly removing {num_users_removed_each_round} users - Round {r} started\"\n",
        "                )\n",
        "                new_sample_df = self.remove_random_users(\n",
        "                    sample_df, k=num_users_removed_each_round\n",
        "                )\n",
        "            else:\n",
        "                new_sample_df = sample_df.copy()\n",
        "\n",
        "            while keep_removing:\n",
        "                if self.debug:\n",
        "                    logger.info(f\"Sampling round {i} started\")\n",
        "                keep_removing = False\n",
        "                uu = get_unqualified_users(new_sample_df)\n",
        "                if self.debug:\n",
        "                    logger.info(f\"{len(uu)=:,.0f}\")\n",
        "                if len(uu):\n",
        "                    new_sample_df = new_sample_df.loc[\n",
        "                        lambda df: ~df[self.user_col].isin(uu)\n",
        "                    ]\n",
        "                    if self.debug:\n",
        "                        logger.info(f\"After removing uu: {len(new_sample_df)=:,.0f}\")\n",
        "                    assert len(get_unqualified_users(new_sample_df)) == 0\n",
        "                    keep_removing = True\n",
        "                ui = get_unqualified_items(new_sample_df)\n",
        "                if self.debug:\n",
        "                    logger.info(f\"{len(ui)=:,.0f}\")\n",
        "                if len(ui):\n",
        "                    new_sample_df = new_sample_df.loc[\n",
        "                        lambda df: ~df[self.item_col].isin(ui)\n",
        "                    ]\n",
        "                    if self.debug:\n",
        "                        logger.info(f\"After removing ui: {len(new_sample_df)=:,.0f}\")\n",
        "                    assert len(get_unqualified_items(new_sample_df)) == 0\n",
        "                    keep_removing = True\n",
        "                i += 1\n",
        "\n",
        "            sample_users = sample_df[self.user_col].unique()\n",
        "            sample_items = sample_df[self.item_col].unique()\n",
        "            num_users = len(sample_users)\n",
        "            logger.info(f\"After randomly removing users - round {r}: {num_users=:,.0f}\")\n",
        "            if (\n",
        "                num_users > self.sample_users * (1 + self.buffer_perc) or r == 1\n",
        "            ):  # First round always overriding sample_df with new_sample_df to keep all qualified items and users\n",
        "                logger.info(\n",
        "                    f\"Number of users {num_users:,.0f} are still greater than expected, keep removing...\"\n",
        "                )\n",
        "                sample_df = new_sample_df.copy()\n",
        "            else:\n",
        "                logger.info(\n",
        "                    f\"Number of users {num_users:,.0f} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
        "                )\n",
        "                keep_random_removing = False\n",
        "\n",
        "            val_sample_df = val_df.loc[\n",
        "                lambda df: df[self.user_col].isin(sample_users)\n",
        "                & df[self.item_col].isin(sample_items)\n",
        "            ]\n",
        "            if (num_val_records := val_sample_df.shape[0]) < self.min_val_records:\n",
        "                logger.info(\n",
        "                    f\"Number of val_df records {num_val_records:,.0f} are falling below expected threshold, stop and use `sample_df` as final output...\"\n",
        "                )\n",
        "                keep_random_removing = False\n",
        "\n",
        "            r += 1\n",
        "\n",
        "        sample_users = sample_df[self.user_col].unique()\n",
        "        sample_items = sample_df[self.item_col].unique()\n",
        "        logger.info(f\"{len(sample_users)=:,.0f} {len(sample_items)=:,.0f}\")\n",
        "\n",
        "        return sample_df, val_sample_df\n",
        "```\n",
        "</details>\n",
        "\n",
        "In the end, we would not have exact like 10000 users, but the numbers would be close to that.\n",
        "\n",
        "### 2. Temporal Splitting\n",
        "\n",
        "Unlike traditional ML, recommendation systems require **temporal splits** to simulate real-world deployment:\n",
        "\n",
        "```python\n",
        "def create_temporal_splits(df, train_ratio=0.8):\n",
        "    \"\"\"\n",
        "    Create time-aware train/validation splits.\n",
        "    This simulates the real-world scenario where we predict future interactions.\n",
        "    \"\"\"\n",
        "    # Sort by timestamp - critical for temporal integrity\n",
        "    df_sorted = df.sort_values('timestamp')\n",
        "    \n",
        "    # Split based on time, not random sampling\n",
        "    split_idx = int(len(df_sorted) * train_ratio)\n",
        "    \n",
        "    train_data = df_sorted.iloc[:split_idx]\n",
        "    val_data = df_sorted.iloc[split_idx:]\n",
        "    \n",
        "    return train_data, val_data\n",
        "```\n",
        "\n",
        ":::{.column-page}\n",
        "![Temporal data patterns show clear user interaction sequences that our models need to capture](../static/L2%20-%20Data%20signals.excalidraw.png){.lightbox}\n",
        ":::\n",
        "\n",
        "## ID Mapping: From Strings to Indices\n",
        "\n",
        "Deep learning models work with numerical indices, not string IDs. Our `IDMapper` class handles this conversion:\n",
        "\n",
        "```python\n",
        "from src.id_mapper import IDMapper, map_indice\n",
        "\n",
        "# Create and fit the ID mapper\n",
        "idm = IDMapper()\n",
        "idm.fit(user_ids=df['user_id'].unique(), \n",
        "        item_ids=df['parent_asin'].unique())\n",
        "\n",
        "# Transform string IDs to numerical indices\n",
        "df_indexed = map_indice(df, idm, \n",
        "                       user_col='user_id', \n",
        "                       item_col='parent_asin')\n",
        "\n",
        "print(f\"Users: {len(idm.user_to_index)} → indices 0-{len(idm.user_to_index)-1}\")\n",
        "print(f\"Items: {len(idm.item_to_index)} → indices 0-{len(idm.item_to_index)-1}\")\n",
        "```\n",
        "\n",
        "The `IDMapper` provides:\n",
        "- **Deterministic mapping**: Same IDs always get same indices\n",
        "- **Unknown handling**: Graceful handling of new users/items\n",
        "- **Serialization**: Save/load mappings for production deployment\n",
        "\n",
        "```python\n",
        "# Save for later use in model serving\n",
        "idm.save(\"data/idm.json\")\n",
        "```\n",
        "\n",
        "## Sequence Generation: The Heart of Sequential RecSys\n",
        "\n",
        "The key innovation in our approach is generating **item sequences** for each user interaction. This captures the temporal context that traditional collaborative filtering misses.\n",
        "\n",
        "```python\n",
        "from src.sequence.utils import generate_item_sequences\n",
        "\n",
        "def generate_sequences(df, sequence_length=10):\n",
        "    \"\"\"\n",
        "    For each user interaction, create a sequence of their previous items.\n",
        "    This gives our model context about user behavior patterns.\n",
        "    \"\"\"\n",
        "    df_with_sequences = generate_item_sequences(\n",
        "        df=df,\n",
        "        user_col='user_indice',\n",
        "        item_col='item_indice', \n",
        "        timestamp_col='timestamp',\n",
        "        sequence_length=sequence_length,\n",
        "        padding=True,\n",
        "        padding_value=-1  # Special token for \"no previous item\"\n",
        "    )\n",
        "    \n",
        "    return df_with_sequences\n",
        "```\n",
        "\n",
        "### How Sequence Generation Works\n",
        "\n",
        "Let's trace through an example:\n",
        "\n",
        "```python\n",
        "# User interactions over time:\n",
        "# Time 1: User buys \"Python Programming\" (item_indice: 42)\n",
        "# Time 2: User buys \"Machine Learning\" (item_indice: 73) \n",
        "# Time 3: User buys \"Deep Learning\" (item_indice: 91)\n",
        "\n",
        "# Generated sequences:\n",
        "# Row 1: item_sequence = [-1, -1, ..., -1]           # No previous items\n",
        "# Row 2: item_sequence = [-1, -1, ..., 42]          # Previous: Python book\n",
        "# Row 3: item_sequence = [-1, -1, ..., 42, 73]      # Previous: Python, ML books\n",
        "```\n",
        "\n",
        "This gives our model the **context** it needs to understand user preferences and make sequential predictions.\n",
        "\n",
        "### Configuration for Sequences\n",
        "\n",
        "```python\n",
        "# From cfg/common.yaml\n",
        "train:\n",
        "  sequence:\n",
        "    sequence_length: 10  # Keep last 10 items as context\n",
        "```\n",
        "\n",
        "The sequence length is a key hyperparameter:\n",
        "- **Longer sequences**: More context but higher memory usage\n",
        "- **Shorter sequences**: Less context but faster training\n",
        "- **Padding**: Ensures all sequences have the same length for batch processing\n",
        "\n",
        "## Dataset Classes for Training\n",
        "\n",
        "Our `src/dataset.py` provides PyTorch-compatible dataset classes:\n",
        "\n",
        "```python\n",
        "from src.dataset import UserItemBinaryDFDataset\n",
        "\n",
        "# Create dataset for binary feedback (clicked/not clicked)\n",
        "dataset = UserItemBinaryDFDataset(\n",
        "    df=df_with_sequences,\n",
        "    user_col='user_indice',\n",
        "    item_col='item_indice',\n",
        "    rating_col='rating',\n",
        "    timestamp_col='timestamp'\n",
        ")\n",
        "\n",
        "# Each sample contains:\n",
        "sample = dataset[0]\n",
        "print(f\"User: {sample['user']}\")           # User index\n",
        "print(f\"Item: {sample['item']}\")           # Item index  \n",
        "print(f\"Rating: {sample['rating']}\")       # Binary label (0 or 1)\n",
        "print(f\"Sequence: {sample['item_sequence']}\")  # Previous items\n",
        "```\n",
        "\n",
        "### Why Binary Labels?\n",
        "\n",
        "We convert ratings to binary labels because:\n",
        "- **Implicit feedback**: Focus on engagement (rating ≥ 1) vs no engagement\n",
        "- **Training stability**: Binary classification is more stable than rating prediction\n",
        "- **Real-world relevance**: Most recommendation systems optimize for engagement, not rating accuracy\n",
        "\n",
        "## Data Flow Through the Pipeline\n",
        "\n",
        "Let's trace the complete data flow from our configuration:\n",
        "\n",
        "```python\n",
        "# 1. Raw data paths (from cfg/common.yaml)\n",
        "data:\n",
        "  train_fp: \"$ROOT_DIR/data/train.parquet\"           # Processed train split\n",
        "  val_fp: \"$ROOT_DIR/data/val.parquet\"               # Processed validation split\n",
        "  train_features_fp: \"$ROOT_DIR/data/train_features.parquet\"  # With sequences\n",
        "  val_features_fp: \"$ROOT_DIR/data/val_features.parquet\"      # With sequences\n",
        "  idm_fp: \"$ROOT_DIR/data/idm.json\"                  # ID mappings\n",
        "\n",
        "# 2. Processing pipeline (notebooks/001-features.ipynb)\n",
        "raw_data → filtering → temporal_split → id_mapping → sequence_generation → save\n",
        "```\n",
        "\n",
        "### Key Processing Steps\n",
        "\n",
        "1. **Data Loading**: Load preprocessed train/val splits\n",
        "2. **ID Mapping**: Convert string IDs to numerical indices  \n",
        "3. **Sequence Generation**: Create item sequence features\n",
        "4. **Data Validation**: Ensure data quality and consistency\n",
        "5. **Serialization**: Save processed features for model training\n",
        "\n",
        "## Configuration-Driven Development\n",
        "\n",
        "Everything is controlled through our YAML configuration:\n",
        "\n",
        "```yaml\n",
        "# cfg/common.yaml - Data section\n",
        "data:\n",
        "  hf_datasets:\n",
        "    name: \"McAuley-Lab/Amazon-Reviews-2023\"\n",
        "    mcauley_variant: \"Books\"\n",
        "  \n",
        "  # File paths with environment variable substitution\n",
        "  train_fp: \"$ROOT_DIR/data/train.parquet\"\n",
        "  val_fp: \"$ROOT_DIR/data/val.parquet\"\n",
        "  idm_fp: \"$ROOT_DIR/data/idm.json\"\n",
        "  \n",
        "  # Column mappings\n",
        "  user_col: \"user_id\"\n",
        "  item_col: \"parent_asin\" \n",
        "  rating_col: \"rating\"\n",
        "  timestamp_col: \"timestamp\"\n",
        "\n",
        "# Sampling configuration  \n",
        "sample:\n",
        "  sample_users: 10000\n",
        "  min_user_interactions: 5\n",
        "  min_item_interactions: 10\n",
        "\n",
        "# Sequence configuration\n",
        "train:\n",
        "  sequence:\n",
        "    sequence_length: 10\n",
        "```\n",
        "\n",
        "This approach provides:\n",
        "- **Reproducibility**: Same config = same results\n",
        "- **Flexibility**: Easy to experiment with different parameters\n",
        "- **Environment handling**: Automatic path resolution with `$ROOT_DIR`\n",
        "\n",
        "## Data Quality Checks\n",
        "\n",
        "Before moving to model training, we validate our processed data:\n",
        "\n",
        "```python\n",
        "def validate_processed_data(df, idm):\n",
        "    \"\"\"Ensure our processed data is ready for model training.\"\"\"\n",
        "    \n",
        "    print(f\"Data shape: {df.shape}\")\n",
        "    print(f\"Users: {df['user_indice'].nunique()}\")\n",
        "    print(f\"Items: {df['item_indice'].nunique()}\")\n",
        "    \n",
        "    # Check for invalid indices\n",
        "    max_user_idx = len(idm.user_to_index) - 1\n",
        "    max_item_idx = len(idm.item_to_index) - 1\n",
        "    \n",
        "    assert df['user_indice'].max() <= max_user_idx, \"Invalid user indices\"\n",
        "    assert df['item_indice'].max() <= max_item_idx, \"Invalid item indices\"\n",
        "    \n",
        "    # Check sequence validity\n",
        "    if 'item_sequence' in df.columns:\n",
        "        seq_lengths = df['item_sequence'].apply(len)\n",
        "        print(f\"Sequence lengths: min={seq_lengths.min()}, max={seq_lengths.max()}\")\n",
        "    \n",
        "    print(\"✅ Data validation passed!\")\n",
        "```\n",
        "\n",
        "## What We've Accomplished\n",
        "\n",
        "By the end of this chapter, you have:\n",
        "\n",
        "✅ **Understanding of recommendation data characteristics** - Temporal patterns, sparsity, and user behavior  \n",
        "✅ **Processed dataset with proper ID mappings** - Ready for deep learning models  \n",
        "✅ **Feature engineering pipeline for sequences** - The core of session-based recommendations  \n",
        "✅ **Configuration-driven data processing** - Reproducible and flexible pipeline  \n",
        "✅ **Data validation and quality checks** - Ensuring reliability for model training  \n",
        "\n",
        "### Key Files Created\n",
        "\n",
        "- `data/train_features.parquet` - Training data with sequences\n",
        "- `data/val_features.parquet` - Validation data with sequences  \n",
        "- `data/idm.json` - ID mappings for production deployment\n",
        "\n",
        "## What's Next\n",
        "\n",
        "In **Chapter 3**, we'll tackle the critical challenge of **negative sampling**. Since we only observe positive interactions (ratings), we need to carefully generate negative examples for training our models.\n",
        "\n",
        "We'll explore:\n",
        "- Why negative sampling is essential for implicit feedback\n",
        "- Different negative sampling strategies and their trade-offs\n",
        "- Implementation using `src/negative_sampling.py`\n",
        "- Creating balanced datasets ready for model training\n",
        "\n",
        "The quality of negative sampling directly impacts model performance, so let's dive into this crucial component next!\n",
        "\n",
        "---\n",
        "\n",
        "<br>\n",
        "If you find this tutorial helpful, please cite this writeup as:\n",
        "\n",
        "> Quy, Dinh. (May 2025). {{< meta title >}}. {{< var website_url >}}. https://{{< var website_url >}}/projects/real-time-seq-recsys/c2/. "
      ],
      "id": "6220267b"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/dvq/miniconda3/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}