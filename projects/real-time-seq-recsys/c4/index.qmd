---
title: "Building Real-time RecSys Chapter 4 - Offline Evaluation, MLflow Experiment Tracking, and Baseline Implementation"
subtitle: "Establishing the evaluation foundation and implementing your first recommendation model"
date: "2025-05-27"
categories: [recsys, recsys-real-time-series]
image: "../static/evaluation-framework.png"
format:
  html:
    code-fold: false
    code-annotations: hover
# draft: true
---

## Introduction

In [Chapter 3](../c3/index.qmd), we built a robust negative sampling pipeline to create balanced training data. Now we're ready to tackle one of the most critical aspects of any ML project: **evaluation**. Before we dive into complex sequence models, we need to establish a solid foundation for measuring success.

This chapter focuses on three essential components: building a comprehensive evaluation framework, setting up MLflow for experiment tracking, and implementing a popularity-based baseline model. Think of this as laying the groundwork for systematic model development—you'll use these tools throughout the rest of the series.

::: {.callout-note appearance="simple"}
## Code
All code for this chapter is available in the `notebooks/010-baseline-popular.ipynb` file and the `src/eval/` directory in the [project repository](https://github.com/dvquy13/recsys-seq-model).
:::

## Why Evaluation Matters in Recommendation Systems

Recommendation systems are notoriously tricky to evaluate. Unlike classification problems where accuracy is straightforward, recommendations involve ranking, relevance, and user satisfaction—all of which are nuanced concepts.

Consider these challenges:

- **Implicit feedback**: Users don't explicitly rate most items they see
- **Ranking quality**: The order of recommendations matters as much as the items themselves  
- **Personalization**: A good recommendation for one user might be terrible for another
- **Temporal dynamics**: User preferences change over time

To address these challenges, we need a comprehensive evaluation framework that can measure multiple aspects of recommendation quality.

## Online vs Offline Evaluation

Before diving into implementation, let's understand the two main evaluation paradigms:

### Online Evaluation
- **What it is**: Testing recommendations with real users in production
- **Metrics**: Click-through rates, conversion rates, user engagement
- **Pros**: Measures real business impact and user satisfaction
- **Cons**: Expensive, slow, requires significant traffic

### Offline Evaluation  
- **What it is**: Testing recommendations on historical data
- **Metrics**: Precision@K, Recall@K, NDCG, coverage
- **Pros**: Fast, cheap, reproducible, enables rapid iteration
- **Cons**: May not capture real user behavior perfectly

For this tutorial series, we'll focus on **offline evaluation** to enable rapid experimentation. In production, you'd typically use offline evaluation for model development and online evaluation for final validation.

## Evaluation Metrics for Recommendation Systems

Our evaluation framework implements several key metrics:

### Ranking Metrics
- **Precision@K**: What fraction of top-K recommendations are relevant?
- **Recall@K**: What fraction of relevant items appear in top-K recommendations?
- **NDCG@K**: Normalized Discounted Cumulative Gain—rewards relevant items appearing higher in the ranking
- **F-Beta@K**: Harmonic mean of precision and recall with configurable weighting

### Diversity Metrics
- **Personalization**: Measures how different recommendations are across users (higher is better)

### Classification Metrics
- **ROC-AUC**: Area under the ROC curve for binary relevance prediction
- **Precision-Recall curves**: Trade-offs between precision and recall at different thresholds

## The Evaluation Framework Architecture

Our evaluation framework is built around three core modules in `src/eval/`:

```python
from src.eval import (
    create_label_df,           # Create ground truth labels
    merge_recs_with_target,    # Merge recommendations with labels  
    log_ranking_metrics,       # Log ranking-based metrics
    log_classification_metrics # Log classification-based metrics
)
```

### Core Evaluation Utilities

The `src/eval/utils.py` module provides essential functions for evaluation setup:

```python
def create_label_df(df, user_col, item_col, rating_col, timestamp_col):
    """
    Create ground truth labels from validation data.
    Ranks items by rating and timestamp for each user.
    """
    label_cols = [user_col, item_col, rating_col, "rating_rank"]
    label_df = (
        df.sort_values([timestamp_col], ascending=[False])
        .assign(
            rating_rank=lambda df: df.groupby(user_col)[rating_col].rank(
                method="first", ascending=False
            )
        )
        .sort_values(["rating_rank"], ascending=[True])[label_cols]
    )
    return label_df
```

This function creates a ranking of items for each user based on their ratings and interaction timestamps, providing the ground truth for evaluation.

```python
def merge_recs_with_target(recs_df, label_df, k=10, user_col, item_col, rating_col):
    """
    Merge recommendation rankings with ground truth labels.
    Essential for computing ranking metrics.
    """
    return (
        recs_df.pipe(
            lambda df: pd.merge(
                df, label_df[[user_col, item_col, rating_col, "rating_rank"]],
                on=[user_col, item_col], how="outer"
            )
        )
        .assign(
            rating=lambda df: df[rating_col].fillna(0).astype(int),
            rec_ranking=lambda df: df["rec_ranking"].fillna(k + 1).astype(int),
        )
        .sort_values([user_col, "rec_ranking"])
    )
```

This function aligns your model's recommendations with the ground truth, handling cases where recommended items don't appear in the validation set.

### Metric Logging with Evidently

The `src/eval/log_metrics.py` module uses the [Evidently](https://www.evidentlyai.com/) library to compute and log comprehensive metrics:

```python
def log_ranking_metrics(cfg: Config, eval_df):
    """
    Compute and log ranking metrics using Evidently.
    Automatically integrates with MLflow for experiment tracking.
    """
    column_mapping = ColumnMapping(
        recommendations_type="rank",
        target=cfg.data.rating_col,
        prediction="rec_ranking", 
        item_id=cfg.data.item_col,
        user_id=cfg.data.user_col,
    )

    report = Report(
        metrics=[
            NDCGKMetric(k=cfg.eval.top_k_rerank),
            RecallTopKMetric(k=cfg.eval.top_k_retrieve),
            PrecisionTopKMetric(k=cfg.eval.top_k_rerank),
            FBetaTopKMetric(k=cfg.eval.top_k_rerank),
            PersonalizationMetric(k=cfg.eval.top_k_rerank),
        ]
    )
    
    report.run(reference_data=None, current_data=eval_df, column_mapping=column_mapping)
    
    # Automatically log to MLflow if configured
    if cfg.run.log_to_mlflow:
        mlflow.log_artifact(evidently_report_fp)
        # Log individual metrics for easy comparison
        for metric_result in report.as_dict()["metrics"]:
            # ... metric logging logic
    
    return report
```

The beauty of this approach is that Evidently handles the complex metric calculations while our framework automatically logs everything to MLflow for tracking and comparison.

## MLflow Integration: Configuration-Driven Experiment Tracking

One of the standout features of our framework is the seamless MLflow integration through the configuration system. Instead of manually managing MLflow runs, everything is handled through the `ConfigLoader`:

### Configuration-Based Setup

```python
from src.cfg import ConfigLoader

# Load configuration and initialize MLflow
cfg = ConfigLoader("../cfg/common.yaml")
cfg.run.run_name = "001-baseline-popular"
cfg.run.experiment_name = "Retrieve - Binary"
cfg.init()  # This automatically sets up MLflow!
```

The `cfg.init()` method handles all the MLflow setup:

```python
def init(self):
    """Initialize MLflow experiment tracking automatically."""
    if self.run.log_to_mlflow:
        logger.info(f"Setting up MLflow experiment {self.run.experiment_name}...")
        mlflow.set_experiment(self.run.experiment_name)
        mlflow.start_run(run_name=self.run.run_name)
        
        self._mlf_logger = MLFlowLogger(
            experiment_name=self.run.experiment_name,
            run_id=mlflow.active_run().info.run_id,
            tracking_uri=mlflow_uri,
            log_model=True,
        )
    return self
```

### Automatic Parameter Logging

The configuration system automatically logs all parameters to MLflow:

```python
def log_config_to_mlflow(self):
    """Log all configuration parameters to MLflow with dot notation."""
    flat_config = flatten_dict(self.config.model_dump())
    for key, value in flat_config.items():
        mlflow.log_param(key, value)  # e.g., "train.learning_rate": 0.01
```

This means every experiment automatically captures the complete configuration, making results fully reproducible.

## Implementing the Popularity Baseline

Now let's implement our first recommendation model: a popularity-based recommender. While simple, this baseline serves several important purposes:

1. **Sanity check**: Ensures our evaluation pipeline works correctly
2. **Performance benchmark**: Provides a baseline for more complex models to beat
3. **Production fallback**: Often used as a fallback when personalized models fail

### Popularity Score Calculation

The implementation is straightforward—we rank items by their interaction frequency in the training data:

```python
# Calculate popularity scores from training data
popular_items_df = (
    train_df.groupby(cfg.data.item_col, as_index=False)
    .size()
    .assign(
        score=lambda df: df["size"] / df["size"].max(),  # Normalize to [0,1]
        rec_ranking=lambda df: df["score"]
        .rank(method="first", ascending=False)
        .astype(int),
    )
    .sort_values(["rec_ranking"], ascending=[True])
)

# Get top-K popular items for evaluation
top_popular_items_df = popular_items_df.head(cfg.eval.top_k_retrieve)
```

This creates a ranking where the most frequently interacted items get the highest scores.

### Generating Recommendations for All Users

For evaluation, we need to generate recommendations for every user in the validation set:

```python
# Create recommendations: same popular items for every user
unique_users = val_df[cfg.data.user_col].unique()
recommendations_df = pd.concat([
    top_popular_items_df.assign(**{cfg.data.user_col: user})
    for user in unique_users
], ignore_index=True)
```

This gives every user the same set of popular items, which is exactly what a popularity-based recommender should do.

## Complete Evaluation Workflow

Here's how everything comes together in the `010-baseline-popular.ipynb` notebook:

### 1. Setup and Configuration

```python
from src.cfg import ConfigLoader
from src.eval import (
    create_label_df, log_classification_metrics, 
    log_ranking_metrics, merge_recs_with_target,
)

# Initialize configuration with MLflow
cfg = ConfigLoader("../cfg/common.yaml")
cfg.run.run_name = "001-baseline-popular"
cfg.run.experiment_name = "Retrieve - Binary"
cfg.init()  # Automatic MLflow setup
```

### 2. Data Preparation

```python
# Load preprocessed data with negative samples
train_df = pd.read_parquet(cfg.data.train_features_neg_fp)
val_df = pd.read_parquet(cfg.data.val_features_neg_fp)

# Create ground truth labels for evaluation
label_df = create_label_df(
    val_df,
    user_col=cfg.data.user_col,
    item_col=cfg.data.item_col,
    rating_col=cfg.data.rating_col,
    timestamp_col=cfg.data.timestamp_col,
)
```

### 3. Model Implementation and Evaluation

```python
# Calculate popularity scores and generate recommendations
popular_items_df = (
    train_df.groupby(cfg.data.item_col, as_index=False)
    .size()
    .assign(
        score=lambda df: df["size"] / df["size"].max(),
        rec_ranking=lambda df: df["score"].rank(method="first", ascending=False).astype(int),
    )
    .sort_values(["rec_ranking"], ascending=[True])
)

# Generate recommendations for all users
unique_users = val_df[cfg.data.user_col].unique()
recommendations_df = pd.concat([
    popular_items_df.head(cfg.eval.top_k_retrieve).assign(**{cfg.data.user_col: user})
    for user in unique_users
], ignore_index=True)

# Merge recommendations with ground truth
eval_df = merge_recs_with_target(
    recommendations_df, label_df,
    k=cfg.eval.top_k_retrieve,
    user_col=cfg.data.user_col,
    item_col=cfg.data.item_col,
    rating_col=cfg.data.rating_col,
)

# Log ranking metrics
ranking_report = log_ranking_metrics(cfg, eval_df)
```

### 4. Classification Metrics

We also evaluate the model's ability to predict binary relevance:

```python
# Prepare data for classification evaluation
eval_classification_df = pd.merge(
    val_df,
    popular_items_df[[cfg.data.item_col, "score"]],
    on=[cfg.data.item_col],
    how="left",
    validate="m:1",
).assign(label=lambda df: df[cfg.data.rating_col].gt(0).astype(int))

# Log classification metrics
classification_report = log_classification_metrics(
    cfg, eval_classification_df, 
    target_col="label", 
    prediction_col="score"
)
```

### 5. Experiment Cleanup

```python
# Log all configuration parameters and end the MLflow run
if cfg.run.log_to_mlflow:
    cfg.log_config_to_mlflow()
    mlflow.end_run()
```

## Understanding the Results

After running the baseline, you'll see several types of outputs:

### MLflow Experiment Tracking
- **Parameters**: All configuration values automatically logged
- **Metrics**: Ranking and classification metrics with step-wise tracking
- **Artifacts**: Evidently HTML reports for detailed analysis

### Evidently Reports
The framework generates comprehensive HTML reports showing:
- Precision@K and Recall@K curves
- NDCG scores across different K values
- Personalization metrics
- ROC curves and precision-recall curves

### Baseline Performance Expectations
For a popularity-based recommender, expect:
- **Low personalization scores**: Everyone gets the same recommendations
- **Moderate precision@K**: Popular items have broad appeal
- **Variable recall**: Depends on how well popular items match user preferences
- **Decent ROC-AUC**: Popular items are often relevant

## Why This Foundation Matters

This evaluation framework and baseline implementation provide several critical benefits:

### 1. **Reproducible Experiments**
Every experiment is fully tracked with:
- Complete configuration parameters
- Evaluation metrics across multiple dimensions
- Generated artifacts for detailed analysis

### 2. **Systematic Model Development**
The framework enables you to:
- Compare models objectively using consistent metrics
- Track improvements over time
- Identify which changes actually help

### 3. **Production Readiness**
The evaluation patterns you establish here will:
- Scale to more complex models
- Integrate with production monitoring
- Support A/B testing frameworks

### 4. **Debugging and Validation**
A working baseline helps you:
- Validate that your evaluation pipeline works correctly
- Identify data quality issues early
- Establish reasonable performance expectations

## Configuration Management Best Practices

The configuration-driven approach offers several advantages:

### Centralized Configuration
```yaml
# cfg/common.yaml
eval:
  top_k_retrieve: 100
  top_k_rerank: 10
  min_roc_auc: 0.5

run:
  log_to_mlflow: true
  experiment_name: "Retrieve - Binary"
```

### Easy Experimentation
```python
# Try different evaluation settings
cfg.eval.top_k_retrieve = 50
cfg.eval.top_k_rerank = 5

# Or different experiment tracking
cfg.run.experiment_name = "Retrieve - Rating"
cfg.train.label_format = "rating"
```

### Automatic Documentation
Every MLflow run captures the complete configuration, making it easy to reproduce results or understand what changed between experiments.

## What's Next

With your evaluation framework and baseline model in place, you have:

- **A robust evaluation pipeline** that can assess recommendation quality across multiple dimensions
- **MLflow integration** for systematic experiment tracking and comparison
- **A working baseline model** that establishes performance expectations
- **The foundation** for building and evaluating more sophisticated models

In [Chapter 5](../c5/index.qmd), we'll build on this foundation to implement sequence-based recommendation models that can capture temporal patterns in user behavior. The evaluation framework you've built here will be essential for measuring whether these more complex models actually improve upon our simple baseline.

## Recap

This chapter established the critical evaluation infrastructure for your recommendation system:

- **Comprehensive metrics**: Ranking, classification, and diversity metrics using Evidently
- **Seamless MLflow integration**: Configuration-driven experiment tracking
- **Baseline implementation**: Popularity-based recommender as a performance benchmark
- **Complete workflow**: From data preparation to metric logging and experiment cleanup

The evaluation framework you've built is production-ready and will scale with you as you develop more sophisticated models. Most importantly, you now have a systematic way to measure progress and make data-driven decisions about model improvements.

---

<br>
If you find this tutorial helpful, please cite this writeup as:

> Quy, Dinh. (May 2025). {{< meta title >}}. {{< var website_url >}}. https://{{< var website_url >}}/projects/real-time-seq-recsys/c4/. 